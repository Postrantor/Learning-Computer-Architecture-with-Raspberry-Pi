Chapter 2

# Recapping Computing

**\*NOTE: YOU MAY** already know the material in this chapter. Anyone who’s taken any coursework in computing, or played around with computers and programming on their own, has at least a modest grasp of what we present here. This chapter is a broad and very high-level overview of what computers do and what parts of the computer are used to do it. You’ll know within a few pages whether it’s useful for you or not. If it isn’t, feel free to skip directly to* [*Chapter\* _3_](#06*9781119183938-ch03.xhtml)*.\_

Although we created computers to do calculations, computers are not calculators. We’ve had calculators for a very long time. The abacus is known to have been used by the Persians as early as 600 BCE, and it was probably in use earlier than that. The precursor to the slide rule, called “Napier’s Bones”, was invented by John Napier in 1617. The very first mechanical calculator, the Pascaline, was invented by Blaise Pascal in 1642—when he was only 19! Better and more elaborate mechanical calculators were devised over the years until very recently, when digital calculators shoved mechanical and analogue calculators onto history’s high shelf.

Charles Babbage is usually credited with the idea of programmability in calculation. He was too poor and his “analytical engine” too complex for him to construct it in 1837, but his son built and demonstrated a more modest version of the machine in 1888. However, it wasn’t until the 1930s that the ideas underlying modern computing began to be understood fully. Alan Turing laid the theoretical groundwork for fully programmable computers in 1936. In 1941, Konrad Zuse built a programmable electromechanical computer, called the Z3 machine, that understood binary encoding and floating point numbers. Zuse’s machine was later proven to be “Turing complete”—that is, capable of implementing Turing’s principles of general-purpose computing.

Zuse’s Z3 had been created to perform statistical analysis of the German air force’s wing designs. World War II accelerated the development of digital computers on many fronts, driven first by the need to calculate artillery trajectories, and later to handle the complex mathematics used by the developers of the nuclear bomb. By 1944, the Colossus computers at Bletchley Park were in daily service aiding the cryptanalysis of German, Italian and Japanese wartime ciphers.

Not all calculation is done in a single step, as are basic arithmetic operations like addition and multiplication. Some calculation requires iterative operations that run in sequence until some limiting condition is reached. There are calculations so complex that the calculator must inspect its own operations and results as it goes along, to determine whether it has completed its job or must repeat some tasks or take up new ones. This is where programmability comes in, and where a calculator takes the fateful step away from calculation into true computing.

It’s this simple: computers are not calculators. Computers follow recipes.

## The Cook as Computer

In some respects, we’ve been computing since long before we were calculating. Homo sapiens broke away from the rest of the primate pack through the ability to pass along knowledge verbally from one generation to the next. Much of this transmitted knowledge was “how-to” in nature, such as how to shape an axe head from a piece of stone. Following step-by-step instructions is now such a pervasive part of life that, most of the time, we don’t even realize we’re doing it. Watch yourself work the next time you cook anything more complex than a toasted cheese sandwich. You’re not just cooking. You’re computing.

### Ingredients as Data

All recipes begin with a list of ingredients. The list is very specific, in terms of both the ingredients and their quantities: For example, the ingredients for _carré d’agneau dordonnaise_ are:

2 racks of lamb

½ cup shelled walnuts

1 small onion

1 3 oz can of liver pâté

½ cup bread crumbs

2 tablespoons parsley

1 tsp salt

2 tbsp lemon juice

½ tsp finely ground black pepper

The goal in cooking is to combine and process these ingredients to make something that doesn’t already exist in your refrigerator. In computing, there are also ingredients: text, numbers, images, symbols, photos, videos and so on. A computer program can take these ingredients and combine and process them into something new: a PDF document, a web page, an e-book or a PowerPoint presentation.

Recipes are step-by-step instructions for going from the ingredients to _carré d’agneau dordonnaise_. Some recipes may be absurdly simple, but most are very explicit and usually done in a specified order:

1. Remove the bones from both racks. 2. Trim the fat off the meat. 3. Finely chop the walnuts. 4. Grate the onion. 5. Stir the liver pâté until smooth. 6. Beat the walnuts and onion into the pâté. 7. Mix the breadcrumbs and parsley together. 8. Season the stuffing mix with salt, lemon juice and pepper.

…and so on. Granted, you could grate the onion before you chopped the walnuts; in many cases order doesn’t matter. However, it does matter sometimes—you can’t beat the chopped walnuts into the pâté before you’ve chopped the walnuts.

Just like recipes, computer programs are sequences of steps that start at the beginning, do something with the data and then pause or stop after all the steps have been performed. You can see simple programs called scripts running in a terminal window on the Raspberry Pi as they do exactly that: they start, they run and they stop when their job is completed. You can see each step in the “recipe” scroll by as it is performed.

With more complex programs, like word processors, the recipe isn’t as linear and the steps aren’t reported onscreen. A word processor is a little like a cook in a café. At the counter you ask for a lunch special, the cook nods and then disappears into the heart of the kitchen to put your meal together. When it’s done, the cook hands the lunch special over the counter to you through the window and waits for another order. When you’re not typing or selecting commands from the menu, a word processor is like the cook waiting at the counter. When you type a character, the word processor takes the character and integrates it with the current document, then waits for another. Regardless of whether you can see the steps happen, each time you type a character, a whole long list of things happen in order, for example, to display the letter “y” at the end of the word “Raspberry”.

### Basic Actions

In both recipes and computer programs, individual steps may contain lists of other steps. The step of grating the onion, for example, is performed in several, smaller steps: first you have to grab the onion in one hand, then pick up the grater with the other hand, and then rub the onion against the face of the grater while allowing the grated onion to fall into a bowl.

In recipes, these internal steps are not called out every time. Most people who have done some cooking know how to grate an onion, and providing detailed directions for grating an onion is unnecessary. However, you follow steps when you grate an onion, whether the steps are spelled out explicitly in the recipe or not. This can happen only because you, the cook, already knew how to grate an onion.

That’s an important point. Cooks use a large number of specific, named actions to complete a recipe. Expert cooks know them all and they can use them without explanation: peel, grate, mix, fold, zest, chop, dice, sift, skim, simmer, bake and so on. Some of these actions are commoner than others, while some—like acidulate—are used so rarely that recipes typically do spell them out in simpler terms, in this case, “Add vinegar or lemon juice to make the sauce more acidic”.

Computers, like cooks, understand a moderate list of fairly simple actions. These simple actions are combined into larger and more complex actions, which in turn are combined into complete operational programs. The simple, basic steps that a computer understands are called _machine instructions_. Machine instructions can be combined into more complex actions called _subprograms_, _functions_ or _procedures_. Here’s an example of a machine instruction:

```
	MOV PlaceB, PlaceA
```

The `MOV`</code> instruction moves a single piece of data from one place to another place inside the computer. Machine instructions may be combined into functions that do a great deal more. Here, for example, is a function:

```
	capitalize(streetname)
```

The `capitalize()` function does what you probably expect: the name of a street is a short string of text characters, which the previous statement in the program placed in a named data item called `streetname`. The function capitalizes the words within the street name according to standard rules for capitalization. This is how a computer turns the text “garden of the gods road” into “Garden of the Gods Road.” Inside the `capitalize()` function may be dozens or hundreds of machine instructions, just as in a cooking task the instruction to “reduce” involves a fair bit of fussy adding, simmering, stirring and testing.

## The Box That Follows a Plan

That’s about as far as we can take the recipe metaphor, and perhaps a little further than we should. Computers are indeed a little like cooks following recipes. Cooks also improvise, try weird things and sometimes make a mess. Computers don’t improvise unless we tell them to, and when they make a mess it’s because we have made some kind of mistake, not them. A metaphor that is closer to reality is author Ted Nelson’s description of a computer as “a box that follows a plan”. A computer is a box, and inside the box are the plan, the machinery that follows the plan and the data upon which the plan acts.

### Doing and Knowing

One more metaphor and we’ll let it rest: programs are what a computer _does_ and data are what a computer _knows_. (This description is credited to computer author Tom Swan.) The part that “does” is called the _central processing unit_ (CPU). The part that “knows” is called _memory_. This “knowing” is done by encoding numbers, characters and logical states using the binary numeric notation discovered by Gottfried Leibniz in 1679. It wasn’t until 1937 that Claude Shannon systematized the use of binary numbers into the maths and logic that computers use to this day. A _bit_ is a binary digit, an irreducible atom of meaning that expresses either 1 or 0. As we explain a little later, bits are represented in computers by on/off electrical states.

Today, both the CPU and memory are made out of large numbers of transistors etched onto silicon chips. (A transistor is simply an electrical switch made out of exotic metals called semiconductors.) This wasn’t always the case; before silicon chips, computers were built out of individual transistors and even vacuum tubes. (Zuse’s seminal Z3 machine used electromechanical relays.)

Whatever they were made of, early computers followed the general plan shown in Figure 2-1. A central control console monitored several different subsystems, each of which was generally in its own cabinet or cabinets. There was the CPU, a punched tape or magnetic tape storage unit and two different memory units. One of the memory units held a series of machine instructions that comprised a computer program. The other memory unit held the data that the program manipulated. This is sometimes called the Harvard architecture, because the Mark I, a very early electromechanical computer developed at Harvard University in 1944, stored data and instructions separately.

![[FIGURE 2-1:](#05_9781119183938-ch02.xhtml#rc02-fig-0001) A pre-von Neumann computer](./media/images/9781119183938-fg0201.png)

Not only were the data memory and the instruction memory of the Mark I physically separate, but they were also, generally, nothing like one another. Data memory might consist of vacuum tubes, dots on a phosphor screen or even sound pulses traveling through columns of mercury. (You can read more on the evolution of memory in [Chapter 3](#06_9781119183938-ch03.xhtml).) Early instruction memory consisted of rows of mechanical switches and wire jumpers that could be moved from one point on a terminal bar to another. Technicians had to set each individual machine instruction by hand, using switches or jumpers, before the program could be run. (As you might imagine, there weren’t a lot of machine instructions in early programs.)

### Programs Are Data

The protean genius John von Neumann worked in many different fields, from mathematics to fluid dynamics, but computer people remember him for a remarkable insight: that _programs are data_ and should be stored in the same memory system as data, using the same memory address space as data. It took some work to redesign computers to read machine instructions from data memory but once it was done, computing was changed forever. Instructions could be entered through a single panel of switches and stored in data memory, one-by-one. Later they could be written out from memory onto lengths of tape punched with patterns of holes, so that they wouldn’t have to be entered by hand every time they were run.

Von Neumann’s insight simplified computing greatly, and led straight to the explosion of computer power that occurred during the 1950s. Figure 2-2 is a highly simplified schematic of how modern computers operate. The figure shows no particular model or family of computer, and omits many of the more advanced features that we explain in later chapters.

![[FIGURE 2-2:](#05_9781119183938-ch02.xhtml#rc02-fig-0002) A simplified modern computer](./media/images/9781119183938-fg0202.png)

### Memory

In the simplest possible terms, _system memory_ is a long row of storage compartments for data. Each location in the row has a unique numeric _address_. All locations are the same size; in modern computers this is generally the 8-bit byte (see Figure 2-3). However, computers read data from system memory in multi-byte chunks. Thirty-two-bit systems like the Raspberry Pi access memory 32 bits (4 bytes, generally called a word) at a time, and perform most of their internal operations on 32-bit quantities. In larger 64-bit desktops and laptops, system memory is accessed 64-bits (8 bytes) at a time.

> [!NOTE]
> that nearly all modern computers allow operations to be performed on single bytes or 2-byte _halfwords_, though there is sometimes a speed penalty for doing so. However, the “bitness” of a computer is the size of its internal data word and operations, _not_ the size of individual memory locations.

![[FIGURE 2-3:](#05_9781119183938-ch02.xhtml#rc02-fig-0003) Memory locations and their addresses](./media/images/9781119183938-fg0203.png)

Memory addresses are ordered in numeric sequence beginning with 0. There is a little disconnect in having the first memory location at address 0 rather than 1, but think of number lines in mathematics, which start at 0. The maths of memory addresses is much easier when the addresses begin at 0.

The CPU locates its data for reading and writing by using memory addresses. It uses machine instructions to fetch data words from specified addresses in the system memory and place them in its registers for calculation or testing. It uses other machine instructions to write values stored in its registers to the system memory.

As mentioned earlier, computer programs themselves are stored in system memory, as sequences of machine instructions, each of which is (usually) a single data word. The difference between a program file and a data file lies almost entirely in how the CPU interprets the data in the file.

Memory is a very complicated business, and we treat it in depth in [Chapter 3](#06_9781119183938-ch03.xhtml).

### Registers

All CPUs contain a certain limited number of storage locations called _registers_. Registers are right on the silicon of the CPU, and the digital logic that executes machine instructions is not only near them but literally all around them. Each register holds a single value. Some registers have no single job and can be put to many different kinds of work. These _general-purpose registers_ are named or numbered. Other registers have special jobs within the CPU. A few registers fall somewhere in between, in that they have specific jobs to do when certain machine instructions are executed, but in other cases may be used, like general- purpose registers, as a sort of silicon shirt pocket where the CPU can tuck values that will be needed again soon. Writing to registers and reading from them is fast—faster than accessing any other type of memory, especially system memory that lies outside the silicon on some other part of the computer’s main circuit board.

There are many kinds of special-purpose registers. Some of the most common are:

- **Program counter:** A program counter register holds the address of the next machine instruction to be brought in from memory for execution. It “keeps the place” in a computer program. - **Status:** A status register (sometimes called a flags register) holds a value divided into single bits or groups of bits. Each bit or group is updated with the status of something the CPU has just done. When the CPU compares the values in two registers, a single-bit “equal” flag will be set to either 1 (if the values were equal) or 0 (if the values were not equal). This allows an instruction that follows the comparison to know which way the comparison went. - **Stack pointer:** A stack pointer holds an address in memory where a data structure called a last-in-first-out stack is stored. Stacks are fundamental to CPU operation; we describe them in more detail in [Chapter 4](#07_9781119183938-ch04.xhtml) in the section “[Inside the CPU](#07_9781119183938-ch04.xhtml#c04-sec-0009)”. - **Accumulator:** The accumulator is a register that holds the result of arithmetic and logical operations. (It is so named because it was used to accumulate intermediate values during calculations in very early computers.) In modern computers, no single register is the sole location for arithmetic results, and the accumulator’s job has been redistributed to some or all of the general-purpose registers. However, some older machine instructions assume that a single register will hold the results of their operations, which is why the term has survived.

The ARM11 processor at the heart of the original Raspberry Pi has a total of 16 registers available to ordinary programs, of which three have special jobs. An additional two registers act as status registers. We have more to say about this in [Chapter 3](#06_9781119183938-ch03.xhtml).

Registers are “valuable” because they are inside the CPU itself and therefore extremely fast. The more registers a CPU has, the less it must access system memory to store intermediate results. A universal rule in computing is that memory access is slow. A great deal of engineering has been done in recent years to reduce the number of times system memory must be accessed in order to get a given amount of work done.

### The System Bus

One of the fundamental challenges of computing is getting values between system memory and the CPU as quickly as possible. Data values are stored in memory at locations that have specific numeric addresses. To access a value in the memory, the CPU must present the value’s address in the memory to the memory system. The value will then be copied from memory and sent back to the CPU.

There is a pathway between the CPU and memory called the _system bus_. The system bus is a side-by-side group of electrical conductors called _lines_, each of which carries one bit of information. The number of bus lines varies depending on the type of computer and the chips it uses. The system bus carries three things:

- Memory addresses - Data values - Control signals that allow the CPU and system memory to coordinate traffic over the bus

In simple terms, the CPU places the address of a memory location on the bus. It also places one or more signals on the control lines, to tell the memory electronics whether the address is to be read from or written to. The CPU then either places a value on the bus to be written to the specified memory location, or waits for the system memory to place the value at the specified address on the bus to be sent back to the CPU.

Computer programs and program data are stored in different locations in memory but, except for how the CPU interprets them, there is no difference between a data word and a machine instruction. For this reason, the term “data values” embraces both data and instructions. We’ll have more to say about this in the next two chapters.

### Instruction Sets

There are a host of different CPU models in the world. Each has its own way of talking to memory and to other parts of the computer system. What sets the models apart most clearly are the individual operations that the CPU can perform. These are the machine instructions and, taken as a group, they are called an _instruction set_.

An instruction set is specific to a specific family of CPUs. Intel’s CPUs represent one such family; ARM is another. Most individual CPUs understand only a single instruction set. The original Raspberry Pi’s ARM11 processor actually has two instruction sets, though only one of them is actually used by the Raspberry Pi software. (There will be more on this in [Chapter 4](#07_9781119183938-ch04.xhtml).)

The machine instructions in an instruction set are grouped by their general function: instructions that move data from or to memory and between registers; instructions that perform arithmetic calculations; instructions that perform logical operations; instructions that read status bits or set control bits; and so on. Early CPUs might have had as few as a dozen machine instructions. Modern CPUs can have a hundred or more.

Although it’s useful to have a big-picture view of CPU instruction sets, you don’t need to memorize them. Programmers rarely write programs by stringing together machine instructions. (This is done sometimes, but it’s slow, specialized work.) Instead, programmers write lists of action statements that read more like human languages. These lists of action statements are then given to programs that translate them into lists of machine instructions. The translator programs are called _compilers_ or _interpreters_, depending on how they operate. We cover these in much more detail in [Chapter 5](#08_9781119183938-ch05.xhtml).

## Voltages, Numbers and Meaning

It’s common to say that computers don’t really deal with text; they deal with numbers. Strictly speaking, even that isn’t true. Down inside the silicon of the CPU where things happen, computers deal only with electrical voltage levels. The actual operation of computer chips entails a constant storm of electrical activity in which voltage levels change back and forth between two—and only two—values. One level is no voltage at all (0 volts) and the other is a single higher voltage level that may vary from computer to computer. It could be 5V or 3V or 3.6V or (on many mobile computers, as well as the Raspberry Pi) 1.2V or less. It could be some other value entirely, as long as it’s always the same inside any given computer. We use 3V in the following discussion.

Computers do deal with numbers, but those numbers are encoded as voltage levels. By convention we say that a voltage level of 0V means the number 0 and a voltage level of 3V (or whatever level it is in the computer being discussed) means the number 1. Only two voltage levels are used in computer chip circuitry, so computers really only understand the two numeric digits, 0 and 1. That’s all, and it doesn’t sound like much. What can you do with only 0 and 1?

Everything.

### Binary: Counting in 1s and 0s

Humans understand just 10 numeric digits: 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9. Yet with those 10 digits we perform mind-bogglingly complex mathematical operations and express numbers that literally have no maximum value. We can express very large numbers with only a couple of different digits: a good approximation of the number of atoms in the entire observable universe can be stated as 1 followed by eighty 0s. Obviously, it’s not about the number of numeric digits we have; it’s about how we arrange them and (more to the point) the meaning that we assign to them.

The decimal notation that we just call numbers*,* which we learned when we were little, is less about numeric digits than columns. Multidigit numbers are digits arranged in columns, with each column having a value 10 times that of the column to its right. In a decimal number like 72,905, each column has a value and a digit in the column to tell us how many times that value is present in the number as a whole. In 72,905, there are 7 ten-thousands, 2 thousands, 9 hundreds, 0 tens and 5 ones.

This concept is easier to understand as a picture; see Figure 2-4.

![[FIGURE 2-4:](#05_9781119183938-ch02.xhtml#rc02-fig-0004) How decimal numbers are evaluated](./media/images/9781119183938-fg0204.png)

We’re so used to thinking in terms of powers of ten that it seems odd to imagine column values other than powers of ten. However, it doesn’t just work; columnar notation using other column values is essential to understanding computing. So consider what numbers would look like if each column had a value _two_ times the value of the column on its right, rather than ten. Instead of columns of ones, tens, hundreds, thousands and ten thousands, we would have columns of ones, twos, fours, eights, sixteens and so on. How many different digits would such a columnar system need?

Two: 0 and 1. In other words, instead of decimal notation with columnar multiples of ten, we have a _binary_ notation with columnar multiples of two. See Figure 2-5, which dissects the binary number 11010. In 11010, there is 1 sixteen, 1 eight, 0 fours, 1 two and 0 ones. (Commas are not used in binary columnar notation.)

![[FIGURE 2-5:](#05_9781119183938-ch02.xhtml#rc02-fig-0005) How binary numbers are evaluated](./media/images/9781119183938-fg0205.png)

There is an alien look about numbers without the digits 2 to 9, but the numbers are real. To see what the binary number’s value actually is in decimal terms we have to add up the values represented by all the columns: 16 + 8 + 0 + 2 + 0 = 26. The two numbers 11010 and 26 have the same value. They’re expressed in different notation, but the numbers are precisely equal. To recast a (very) old joke: there are only 10 kinds of people in the world: those who understand binary and those who don’t.

The value of column multiples in a system of numeric notation is the base of the system. If the columnar multiple is 10, the system is _base 10_. If the columnar multiple is two, the system is base 2. (The small subscript numbers in the figures specify the number bases of the numbers beside them.) Theoretically, column multiples may be any integer value at all: base 3, base 4, base 8, base 11, base 16, anything. There’s only one problem, which is explained in the next section.

### The Digit Shortage

Our ingrained decimal notation is called base 10, and uses 10 digits. Base 2 uses two digits. Base 8 uses eight digits. Base 16 uses 16 digits—except that there are only ten digits. Zero to 9 is all we have. What about the other six digits? If we had evolved with eight fingers on each hand, there would doubtless be 16 digits, each a single, distinct symbol. Any symbols will do, as long as we agree on what each symbol means. We could use the symbols @, %, \*, &, \# and $. However, there is an ordering problem. These symbols have no universally understood order. Does \* come before &? Only when they’re typed in that order. Confusion would result without an agreed-upon ordering. So let’s use six symbols that _do_ have an agreed-upon order: A, B, C, D, E and F. Counting to 10 in our familiar decimal notation and symbols looks like this:

```
	1, 2, 3, 4, 5, 6, 7, 8, 9, 10.
```

To count to 16 with an expanded digit set, we could say:

```
	1, 2, 3, 4, 5, 6, 7 8, 9, A, B, C, D, E, F, 10.
```

In a scheme like this, the digit A represents decimal 10, B represents decimal 11, C represents decimal 12 and so on. A value is a value, irrespective of base. The differences between number bases is one of notation, not value. Base 16 is called _hexadecimal_ notation, and it is crucial in understanding modern computers.

### Counting and Numbering and 0

Before we go on, it’s worth exploring a famous little weirdness from the computer world. Counting to 10, as we learned as kids, we begin with the digit 1. In computer technology, however, we start counting with the digit 0. When a computer person is counting memory locations, he or she starts at the first memory location and says, “0, 1, 2, 3, 4, 5…”. What’s going on here? It’s actually a misunderstanding. Counting memory locations like this really isn’t counting them. It’s numbering them. And just as a number line from mathematics begins at 0, numbering entities in computer science begins with 0. A person would say, “There are six memory locations, numbered 0 to 5”. A count (here, six) is how many entities are out there. Numbering them gives them both names and an order. The first memory location can be called “location 0”. Having given that first memory location the name “location 0”, it’s clear that the name of the second location is “location 1” and so on.

When memory locations are numbered in this way, counting from 0, the numbers we give them are called _addresses_. The first address in an address space is always 0.

### Hexadecimal as a Shorthand for Binary

Hexadecimal notation is a columnar notation, just as decimal and binary notations are. Each column has a value 16 times the value of the column to its right. The numbers look odd because the 16-digit symbols are a mixture of letters and numbers, but the notation works the same way as decimal and binary. The values of the columns mount up fast: by the fifth column, the value of the column is 65,536.

Figure 2-6 shows this. The hexadecimal number 3C0A9 is equivalent to the decimal number 245,929. Both numbers are equivalent to the binary value 111100000010101001. This is a clue as to why hexadecimal notation is important.

![[FIGURE 2-6:](#05_9781119183938-ch02.xhtml#rc02-fig-0006) How hexadecimal numbers are evaluated](./media/images/9781119183938-fg0206.png)

So why does hexadecimal notation even exist? Computers don’t really use hexadecimal numbers. They use binary numbers, period, encoded as electrical voltage levels. “Hex” (as we say informally) is used by all of us who have trouble interpreting long strings of 1s and 0s. It’s a sort of shorthand, allowing us to express binary numbers in a much more accessible form. 111100000010101001 is the same value as 3C0A9. Which would you prefer to work with?

Figure 2-7 summarizes the use of hexadecimal as shorthand and also binary numbers are represented by a series of different voltage levels on electrical conductors like the system bus. The system bus shown is 16 bits wide. Each line in the system bus might be a copper trace on a circuit board or a microscopic wire inside a chip, with one of either two voltages on each of the copper traces. The digit 1 represents a 3V reading on a bus line. The digit “0” represents a 0V reading on a bus line.

![[FIGURE 2-7:](#05_9781119183938-ch02.xhtml#rc02-fig-0007) Bus lines, voltages, binary bits and hexadecimal numbers](./media/images/9781119183938-fg0207.png)

Each digit in a hexadecimal number can represent values from 0 to 15. It takes four bits to represent values up to 15. This is why each digit in a hexadecimal number represents four binary digits of either 1 or 0.

It’s possible to lose track of which base a given value is written in. The number 11 is a binary number. It’s also a decimal number, and a hexadecimal number as well. The three values are of course different, but the two digits—11—look precisely the same. Different typographical conventions are used to explicitly specify the number base of a given number:

- For binary, the letter b or B is often used after the number; for example, 011010B. - For binary, the prefix 0b is often used, as in 0b011010. - You may also sometimes see the prefix _%_ in front of binary numbers; for example, %011010. - For hexadecimal, use the letter h or H after the number; for example, F2E5H. - The prefixes $ and 0x are also used to designate hexadecimal notation; for example, $F2E5 and 0xF2E5.

In printed material, such as books and documentation, a subscript suffix is sometimes used to indicate the number base, as in F2E5<sub>16</sub>. Subscripts are difficult to do in editors used for programming, so even in printed work, one of the previously mentioned conventions is used.

### Doing Binary and Hexadecimal Arithmetic

Binary and hexadecimal are simply different forms of notation. All the laws of arithmetic still apply. It’s possible to do addition, subtraction, multiplication and division on paper in either binary or hexadecimal. The methods are identical; you simply have to remember things like the fact that, in binary, 1 + 1 = 10. In hex, A + 2 = C and A + C = 16 (just not the 16 you’re used to—16H is 22 decimal). Carries and borrows work the same way irrespective of base. Performing long division on paper in hex is a little surreal, but it can be done.

Yes, it can be done, and it may be good practice, but with a software calculator app on virtually every computer with a graphical shell it may not be the best use of your time. We’re not going to explain how to do manual binary or hex maths here. Instead, we suggest you become familiar with a software calculator capable of number bases other than decimal. On the Raspberry Pi under the Raspbian operating system, the calculator is called Galculator. It’s listed in the start menu in the Accessories group. If you haven’t yet used any operating system (Raspbian is only one of many, as are Windows and OS X), hold that thought; we’ll cover operating systems in the next section.

By default, Galculator works in decimal only, in Basic mode. To use Galculator for calculation in other number bases, first select View and then Scientific mode. The keys for hex digits A–F are greyed out. To change the number base used, select Calculator from the main menu, then Number bases from the pull-down (see Figure 2-8). Click the radio button for the base of your choice. (Galculator also supports octal, which is base 8, but octal is increasingly uncommon and we don’t mention it further here.) For binary, all digits except 0 and 1 are greyed out. For hex, all digits become active.

![[FIGURE 2-8:](#05_9781119183938-ch02.xhtml#rc02-fig-0008) Changing number bases in Galculator](./media/images/9781119183938-fg0208.png)

When you’re in scientific mode with your base of choice selected, Galculator works just as a calculator works in decimal.

---

> [!TIP]

Here’s a tip: to convert a value from one base to another, enter the value in its original base and then select Calculator ⇒ Number bases and click the button for the base to which you want to convert the value. The conversion is done instantly, just by changing the base.

## Operating Systems: The Boss of the Box

There is a great deal of digital machinery baked into the silicon of modern CPUs. They do not, however, run completely by themselves. Factories need managers and if a CPU and its memory system represent a factory, the factory manager is called an _operating system_ (OS). There have been thousands of operating systems throughout computer history, but at the time of writing only a handful have any significant market share: Windows, GNU/ Linux, Android, OS X and iOS. None of these arose in a vacuum. Windows has its roots in IBM’s OS/2, as well as an older “big iron” operating system called VAX VMS. All the others have deep roots in Unix, another big-system OS created by Bell Labs in the late 1960s.

Operating systems are programs, and like all programs they’re ultimately sequences of machine instructions. Unlike word processors and video games, operating systems have special powers that enable them to manage a computer system. Many of these powers depend on special machine instructions that are designed to be used only by operating systems. Operating systems are loaded and run first, through a boot-up process controlled by a computer’s bootloader, which is a special program tasked with getting the operating system from storage into memory and then running it. Once an OS has loaded and configured itself, the computer is “open for business” and the OS can begin management of the machine.

### What an Operating System Does

A high-level definition of an operating system is that it stands between a computer user and the computer hardware, enabling the user to use the computer’s various resources without interfering with other users or with computer operation itself. Its major jobs can be broken down this way:

- **Process management:** The OS launches individual threads of execution for its own needs and the needs of users. It allocates execution time on the CPU among executing threads. If the CPU has multiple cores, it distributes processes among the cores. (More on this later.) - **Memory management:** The OS allocates memory to running processes, in most cases as separate memory spaces that are protected from interference by other processes. Through a technology called virtual memory, the OS allows the computer literally to use more memory than it actually has, by writing the least-used process memory out to disk when more memory is needed. (Much more on this in [Chapter 3](#06_9781119183938-ch03.xhtml).) - **File management:** The OS maintains one or more file systems, which allocate file storage space on disks and other mass-storage devices and manage the reading of data from files and the writing to and deletion of files. - **Peripheral management:** The OS manages access to system peripherals like keyboards, mice, printers, scanners, graphics coprocessors and (in cooperation with file systems) mass storage devices. This is generally done through specialised software interfaces called device drivers, which are written for specific peripherals and may be installed separately, much like user applications. - **Network management:** The OS manages the computer’s access to external networks (like local area networks and the Internet) through a collection of standard methods called networking protocols. The protocols are implemented in one or more pieces of software that, taken together, are called the network stack. - **User account management:** All modern operating systems allow different users to have their own accounts on the computer. An account includes a unique login, a set of security rules called privileges and a private file space protected from manipulation by other users. - **Security:** Scattered throughout an OS are mechanisms to keep running processes from interfering with one another and with the OS itself. Much of OS security is done by defining rules that specify what processes and users can and cannot do. Certain users called administrators or super users have powers that ordinary users do not have, in order to control the way the OS does its work. - **User interface management:** The OS manages user interaction with the computer through software mechanisms called shells. A shell may be as simple as a text command line in a terminal window, or it can be a full-blown windowed graphical environment like those used in Windows, Mac OS X and desktop implementations of Linux, including Raspbian on the Raspberry Pi.

### Saluting the Kernel

The issue of user shells highlights the question of what is and what is not actually a part of the operating system. We’re used to Microsoft Windows, in which the user interface is tightly bound to the operating system as a whole and cannot be changed except in small ways via configuration options. In Linux (including the Raspbian OS) the user interface is an installable module, not much different in nature from a pure application like a word processor. There are textual shells like bash and ksh, and many different graphical shells, including GNOME, KDE, Xfce, Cinnamon and others. These shells can be installed and uninstalled by users with administrator privileges.

Linux has a long history of modular design. Many of its elements may be changed, within certain limitations. At its heart, however, is a monolithic block of code called a kernel. The Linux kernel has full control over the computer’s hardware. It adapts to differences in hardware through _loadable kernel modules_ (LKMs) that extend the kernel with device-specific code. LKMs include things like device drivers and file systems.

### Multiple Cores

Modern CPUs often have more than a single execution core. A _core_ is a separate and almost entirely independent engine that executes machine instructions. (In silicon design circles, core has a broader meaning, as we explain in [Chapter 4](#07_9781119183938-ch04.xhtml).) At the time of writing, CPUs with two, four and six cores are common in the personal computing world, and units with eight cores are beginning to appear. Each core executes processes independently, but all cores share system resources like memory. The operating system controls the use of all cores in a system, just as it controls everything else. The OS typically runs in one core, and parcels processes out to the other core(s) as needed.

The ARM11 CPU in the Raspberry Pi has only one core. Other ARM processors have as many as four. However, the nature of ARM hardware allows chip designers to create custom CPUs, and the latest ARM CPU—Cortex-A15—supports arbitrary numbers of cores in clusters of four if designers want them.

We’ll have more to say about how ARM CPUs and ARM-based single-chip systems are created in [Chapter 3](#06_9781119183938-ch03.xhtml).
