Chapter 7

# Wired and Wireless Ethernet

**FOR A LONG** time, there were so few computers in the world that the benefits of connecting them to one another simply didn’t occur to anyone. In the mainframe era, “data sharing” consisted of printing out reports on huge piles of paper and sending them to whoever needed the data. Early use of data communications was not for networking but for remote access to user timesharing terminals, card/tape readers and printers. (For more on this, see [Chapter 6](#09_9781119183938-ch06.xhtml).) It wasn’t about connecting computers to computers but rather about connecting computers to their peripherals. As currently understood, _networking_ is the practice of transferring data files and commands between otherwise independent computers.

Only after the cost of computers came down due to the introduction of minicomputers did universities and research organisations have a critical mass of “in-house” computers to interconnect, circa 1965. After that, networking technology advanced quickly. The initial focus was on connecting computers at a distance, in separate buildings or even separate research campuses, in what came to be called a _wide-area network_ (WAN). Lawrence Roberts and Thomas Marill did the experimental work on wide-area network hardware at Massachusetts Institute of Technology’s Lincoln Labs that led directly to the seminal research network Advanced Research Projects Agency Network (ARPANET) by 1969. Robert Kahn and Vint Cerf created TCP/IP (Transmission Control Protocol/Internet Protocol) that was perfected on ARPANET by 1983 and later became the foundation of the modern Internet.

In 1971, ALOHAnet was successfully deployed by the University of Hawaii, as a means of linking the university’s computers spread across several islands via radio signals. It was one of the first packet-based networks, and certainly the first wireless network. ALOHAnet introduced the concept of uncoordinated access to a shared medium (in this case, a block of radio spectrum) with support for collision detection, back-off and retransmission; it was one of the inspirations for Ethernet, which was in development about that time and shares these features, which we’ll discuss during the rest of this chapter.

_Local-area networks_ (LANs) came a little later, once multiple computers were deployed in physical proximity within a single building. One of the first operational LANs was the Cambridge Ring, implemented at Cambridge University in 1974 but never commercialised. Xerox Corporation developed what became Ethernet circa 1970 to 1975, published the spec in 1976 and presented Ethernet as a standard in 1980, in cooperation with Digital Equipment Corporation and Intel. In 1983, Ethernet became Institute of Electrical and Electronics Engineers (IEEE) standard 802.3, to tremendous industry enthusiasm. IBM introduced its token ring network architecture in 1985 as a competitor to Ethernet, but the architecture’s proprietary nature prevented it from becoming a wide success.

Hundreds of network technologies have appeared and vanished since the 1970s. Some were truly minimal: the XModem and Kermit software packages were widely used for transferring files between two microcomputers in the late 1970s and early 1980s, using serial ports. This mechanism required a special serial crossover cable that was often called a _null modem_. The crossover cable connected the serial transmit line of one computer to the serial receive line of the other, allowing direct communication without passing through other communications gear. Computer bulletin-board systems (CBBS) allowed multiple computers to connect to a remote computer via phone lines and modems, allowing text messaging and file transfers. By the late 1990s, the Internet was the dominant WAN, and Ethernet was the dominant LAN.

## The OSI Reference Model for Networking

Networking can be a complicated business, largely because its job is to bridge a great many different technologies spread across computer categories from hand-held devices to desktop computers to servers. Making sense of it requires a roadmap. Fortunately, we’ve had such a roadmap since the mid-1980s: the _Open System Interconnection_ (OSI) reference model, which became an International Organization for Standardization (ISO) standard in 1984.

---

> [!NOTE]

The abbreviation ISO is not an acronym but an adaptation of a classic Greek term _isos_, which is equal.

The OSI model is not a specification in the same sense that the IEEE 802.3 Ethernet document is a specification. It’s a way of creating a “big picture” view of the many smaller ideas falling within the larger idea of networking. It’s an educational tool, and also a way to help engineers and programmers stay on the same page when discussing networking technologies. The basic idea is to separate computer networking into conceptual layers, from networking applications at the top (think email clients and web browsers) to copper and fibre-optic cables, radio waves, and their associated electronics at the bottom. The journey of data across a network connection begins at the top, moves downward through the model’s layers to the physical link at the bottom, across the physical link to another computer, and then up through the layers to the top. Figure 7-1 illustrates the OSI reference model.

![[FIGURE 7-1:](#10_9781119183938-ch07.xhtml#rc07-fig-0001) The OSI reference model for networking](./media/images/9781119183938-fg0701.png)

---

> [!NOTE]

Because the layers of networking machinery are depicted one atop the other in schematic diagrams of the OSI model, they are often referred to as a _network stack_.

We’re going to go through the OSI model layer by layer so that you can get a sense for the big picture of networking. This chapter is primarily about wired and wireless Ethernet (both of which are used a great deal with the Raspberry Pi) and so our focus later in the chapter will be on the bottom four layers (called the _transport set_), which encompass Ethernet and two crucial protocols: Transmission Control Protocol (TCP) and Internet Protocol (IP).One way to think of it is that the transport set is about moving data, whereas the top three layers, called the _application set_, are about processing data via networked applications.

Central to the OSI model is the idea of _abstraction_. Each layer conceptually communicates directly with the corresponding layer (its _peer)_ on the other layer of the link, without depending on the exact details of the levels below it; these details are said to be _abstracted away_. So a web browser (in the application layer on your computer) can communicate with a web server (in the application layer on another computer) without caring in detail how the underlying TCP/IP stack provides a reliable channel between the two machines, or whether the physical medium supporting the communication is an Ethernet cable, a Wi-Fi link, a fibre-optic backbone or some combination of the three.

The OSI model has its limits. Not all networking systems map neatly onto its layers, and some networking systems (particularly the Internet suite of protocols) have their own layered reference models that predate the OSI model and span some of its several layers. It is, however, an excellent way to confront the complexities of networking the first time you’re introduced to them.

### The Application Layer

The journey across a network begins when you, the user, launch a network-aware program. That’s what the application layer is about: creating or selecting data for transfer. The computer you’re using is called a _host_, as is the computer on the other end of the line. The program you’re using to communicate across the network is called a _client_. The program at the other end is likely to be a _server_, which is a program that exists purely to send data across a network in response to a request from a client, without human interaction. A server can be thought of as a sort of data robot: your client sends commands or data to the server and the server in turn sends commands and data to your client.

The application layer provides the “human face” of network client programs like email, chat, Usenet, web browsers, FTP, Telnet, and so on. Once the application layer has worked out the commands and data to be sent over the network, and the address of the destination host, these are passed down the stack to the next layer.

### The Presentation Layer

The name of the presentation layer is a little misleading. It has nothing to do with displaying data. It’s really about data conversion, and about how data will be “presented” to the host on the other end of the connection. As we explained in [Chapter 6](#09_9781119183938-ch06.xhtml), there have been numerous character encoding standards, but the three most important are the American Standard for Code Information Interchange (ASCII, used on almost everything today), Unicode (for character sets larger than 256 characters) and Extended Binary Coded Decimal Interchange Code (EBCDIC), which is used only on older “big iron” IBM mainframes. The presentation layer is where encoding differences like that are ironed out. Two other tasks often handled in the presentation layer are encryption and data compression, both of which are optional but these days quite common.

The presentation layer may translate outgoing data into a specified standard network encoding for transmission; the peer will translate incoming data from the standard encoding into that host’s preferred encoding before passing it up to the application layer. It may add _headers_ to outgoing data before passing it on to the next layer, indicating what encryption or compression has been applied; these are used by the peer to undo the encryption or compression. Headers may be seen as nested envelopes, on each of which is written information relevant to an entity at a particular layer of the stack. Most ISO model layers add one or more headers to the data block passed down from the layer above them. Later on, as the data block passes up the stack on the destination host, the headers are removed in order and interpreted by the peer of the layer that applied them.

This process, called _data encapsulation_, is shown in Figure 7-2. A Protocol Data Unit (PDU) is a chunk of data handled by a particular layer of the OSI model. For the transport layer, this is called a _segment_. For the network layer, this is called a _packet_. For the data link layer, it’s called a _frame_. (> > [!NOTE]
that many people use the terms “packet” and “frame” interchangeably.)

![[FIGURE 7-2:](#10_9781119183938-ch07.xhtml#rc07-fig-0002) OSI model data encapsulation](./media/images/9781119183938-fg0702.png)

> [!NOTE]
> that the IP packet and Ethernet frame PDUs are more complex than shown in [Figure 7-2](#10_9781119183938-ch07.xhtml#c07-fig-0002) and have been abbreviated to simplify the diagram. The transport layer segment is shown as a User Datagram Protocol (UDP) PDU, also for simplicity’s sake. As you will see a little later, the transport layer also supports the much more complex TCP PDU. Either a UDP or TCP segment may be processed at the transport layer.

You’ll find it useful to refer to [Figures 7-1](#10_9781119183938-ch07.xhtml#c07-fig-0001) and [7-2](#10_9781119183938-ch07.xhtml#c07-fig-0002) during the detailed discussion of the various OSI layers that follow.

### The Session Layer

With data in hand from the presentation layer, the session layer opens the actual communication session with the other host. The session layer determines if the other host can in fact be reached. It also determines whether the connection between the two hosts is full duplex or half duplex. _Full duplex_ means that data can pass in both directions simultaneously. _Half duplex_ means that only one end can transmit at a time, while the other end listens for data and waits for the line to “turn around”.

Some network applications can request multiple simultaneous connections to the other host. A web browser, for example, may need an HTML file, a CSS file and perhaps other content files of various sorts in order to render a single web page. The session layer establishes these additional connections and keeps track of what data is moving over which connection. The session layer also provides the highest level of error response, and may attempt to re-establish failed connections automatically.

The session layer is the lowest layer in the application set. Many network applications map to all three layers in the application set, in that a single program (like a web browser or an email client) handles data selection/creation, data presentation and session management. When all requested sessions have been established, the application’s work is done. The data is handed down to the transport set, where the focus is less on the data than on getting the data where it needs to go.

### The Transport Layer

On the transmit side, the transport layer’s primary tasks areto take the data handed down from the session layer by one or more processes, optionally divide it into _segments_ that are small enough to handle conveniently (a process called _segmentation_), and queue segments from these processes for transmission over the network (a process called _multiplexing_). At the receive side the transport layer reassembles segments and routes data to the appropriate receiving process.

Transport layer protocols may be categorised as either connection-oriented or connectionless. _Connection-oriented protocols_ provide a reliable, ordered stream of data between two processes, and so must generally provide a mechanism on the receive side to reorder segments that arrive out of order (which can occur if the underlying network routes segments via multiple routes with different latencies) and to detect and request retransmission of segments that have been dropped or corrupted by the underlying network. They may also provide flow-control facilities, which prevent the transmitter from sending data faster than it can be processed by the receiver. _Connectionless protocols_ are generally much simpler, delegating the handling of errors and out-of-order data upward to the application set; often they provide little more than a multiplexing function.

In the modern Internet, the transport layer is implemented by TCP and UDP. TCP is connection-oriented: it divides the incoming stream of data from a process into segments, and attaches to each segment a header containing a sequence number (which is used to reorder segments at the receive end and detect missing segments) and a checksum (which is used to detect corrupted segments). Flow control is provided by using a sliding window scheme: the segment header contains a window field, which allows each end of the connection to specify how much data it can accept. Multiplexing is provided by means of source and destination port fields in the header, which (along with the source address) are used by the receiver to identify the destination process and stream for each incoming segment.

UDP is a much simpler connectionless protocol. Its header contains only the source and destination port fields required for multiplexing, along with a length field and a checksum; corrupted segments are silently discarded in UDP rather than being retransmitted. UDP is commonly used in applications like Voice over Internet Protocol (VoIP) in which occasional dropped segments can be tolerated but latency must be kept to a minimum.

### The Network Layer

The network layer is primarily concerned with _routing_; that is, determining what path the data will take while it travels to the other host. Although the OSI model diagram in [Figure 7-1](#10_9781119183938-ch07.xhtml#c07-fig-0001) suggests that the data travels directly from the sending host to the receiving host, in WANs (including the Internet) this is not always the case. A network _path_ often includes one or more intermediate “stops” at computers along the way. These _intermediate nodes_ don’t generally unpack or attempt to interpret the data; they simply look at the destination address on each packet and send the packets on their way. The specialised hardware devices that perform this forwarding are called _routers_. Routers contain tables of network addresses and connections called a _routing table_, and can work out the route to the destination host address using the host address and the routing table. Routers are covered in more detail later in this chapter in the “[Routers and the Internet](#10_9781119183938-ch07.xhtml#c07-sec-0019)” section.

In the context of the Internet, the network layer is where the IP does most of its work. IP takes segments passed down from the transport layer and builds them into packets with additional information needed for IP processing (refer to [Figure 7-2](#10_9781119183938-ch07.xhtml#c07-fig-0002)). The IP packet is complex, and its header format is shown in Figure 7-3. Although we can’t explain each of the header fields in detail in this book, here’s a quick summary:

- **Version:** The IP version number—for example, 4 for IPv4 and 6 for IPv6. - **IP header length:** The length of the header in 32-bit words, including options and padding. - **Type of service:** Encodes “quality-of-service” (QoS) values for IP packets. Some packets require special treatment to ensure the quality of the larger data stream. Video, for example, requires that packets be sent in order and with minimum delay (latency) for the highest quality when delivered for display. - **Total packet length:** Specifies the length of the packet in bytes. This length cannot be longer than 65,535 bytes and includes the segment passed down from the transport layer. - **Identification:** A 16-bit value given to every packet belonging to a specific message. It allows the destination host to reassemble a message from packets received out of order or mixed with packets belonging to other messages. - **Flags:** Contains three single-bit control flags that control the splitting of large packets into smaller packets. The first flag is reserved and not used. - **Fragment offset:** Part of a mechanism to identify the order of packets received out of order. - **Time to live (TTL):** Specifies the maximum number of “hops” the packet is allowed to take along its route from the source host to the destination host. Time to live is decremented by one at each hop, and when the value goes to zero the packet is assumed to have “got lost” and is discarded. (> > [!NOTE]
  that “TTL” as used here has nothing to do with transistor-transistor logic chips.) - **Protocol:** Contains an 8-bit code specifying the protocol (generally TCP or UDP) that generated the segment handed down from the transport layer. - **Header checksum:** Part of a mechanism that detects corrupted packet headers. This checksum does not include payload data. - **Source IP address:** The 32-bit IP address (that is, its location on the Internet) of the host that generated the packet. We’ll explain IP addresses in more detail in the section “[Names vs. Addresses](#10_9781119183938-ch07.xhtml#c07-sec-0020)” later in this chapter. - **Destination IP address:** The 32-bit IP address of the host to which the packet was sent. - **Options:** A variable-length field that may contain one or more optional subfields used for security, testing and debugging. - **Data:** The payload embedded in the packet. This is generally a segment passed down from the transport layer.

![[FIGURE 7-3:](#10_9781119183938-ch07.xhtml#rc07-fig-0003) The Internet Protocol (IP) Version 4 (IPv4) header format](./media/images/9781119183938-fg0703.png)

When necessary, the IP can split a segment too large to fit in a single packet into multiple packets. IP doesn’t attempt to keep packets in order or detect errors, both of which are handled by layers above the network layer. Its job is to get packets to the next stop along the route.

### The Data Link Layer

The Internet is not a single network. It’s a network of networks with defined, routable connections between them. Networks can be nested within larger networks to any reasonable level, but at some point there is a _local network_ in which all computers may connect directly to one another without the involvement of a router. The data link layer manages the flow of data over these direct connections, reorganising data coming from higher layers yet again, this time into _frames_ that are of a size and format that the hardware implementing the direct connection can handle.There are many different technologies used in connecting computers on a local network. In the case of technologies that involve communication over a shared medium, a primary function of the data link layer is to arbitrate access to this medium, via a _media access control_ (MAC) scheme. This may involve either centralised coordination or decentralised collision detection and avoidance. As discussed later in this chapter in the section “[Collision Detection and Avoidance](#10_9781119183938-ch07.xhtml#c07-sec-0013)”, modern Ethernet technologies (including Wi-Fi) take the latter approach.

The data link layer may also provide local flow control, which ensures that frames are not sent so quickly that the destination host’s buffers fill up, and reliable delivery, whereby successfully received frames are acknowledged by the receiver and unacknowledged frames are retained by the transmitter and retransmitted as necessary. Ethernet does not provide either of these services; in the case of protocols that do, it is common to regard the data link layer as comprising an upper _logical link control_ sublayer, where these services reside, and a lower MAC sublayer.

### The Physical Layer

The physical layer is where the network connection literally “gets physical”: the frames handed down from the data link layer are received as strings of bits, which are converted to signals in the physical _medium_. This medium is any physical process onto which data may be encoded: electrical pulses on a cable, modulated microwaves, modulated light, whatever.

Most of the physical layer’s operation occurs inside electronic circuitry in a computer’s Network Interface Controller (NIC), and varies widely between standards. The transmitter generally adds a _preamble_ and delimiter bits that indicate the beginning and end of the data, and transforms each bit or group of bits in turn into a _symbol_ to transmit over the medium. The receiver uses the preamble and delimiters to detect incoming data, and decodes the symbols to recover the original bits. In choosing what symbols to transmit over the medium we must consider the need for the receiver to recover a clock from the incoming symbol stream; encoding schemes such as Manchester coding and 4B/5B (there’s more on encoding a little later in this chapter in the “[Ethernet Encoding Systems](#10_9781119183938-ch07.xhtml#c07-sec-0014)” section) guarantee that transitions will occur with a certain minimum frequency, regardless of the input data.

Ethernet spans the data link and physical layers.The Ethernet protocols operate in the data link layer, with a standard interface to any of several Ethernet-specific physical layers, which we’ll discuss in more detail shortly. Wi-Fi is analogous to Ethernet with wireless media, in that it too spans the data link and physical layers of the OSI model, with several variations of the medium access (MAC) mechanism and physical layers. Much of the difference between Ethernet and Wi-Fi physical layers are differences of _modulation_; that is, mechanisms for imposing information on radio-frequency energy. For Ethernet, this radio-frequency energy is conducted through cabling of some sort. For Wi-Fi, the radio-frequency energy is transferred over free space using antennas.

## Ethernet

Like so many other things, Ethernet came out of the Xerox PARC labs in Palo Alto, California. Robert Metcalfe and David Boggs first circulated the idea within PARC in May 1973, and by November of that year the technology went online. The Ethernet concept was an outgrowth of PARC’s research into personal computing, and was intended to link PARC’s forward-looking Alto experimental workstations together at a speed of 3 megabits per second (Mbit/s). Metcalfe coined the term “[Ethernet](#10_9781119183938-ch07.xhtml#c07-sec-0010)” as an allusion to the Victorian idea of the _luminiferous aether_, which was a mysterious (and later shown to be non-existent) medium through which light and radio waves passed. Ethernet was introduced as a commercial product in 1980, and in 1983 was standardised as IEEE 802.3.

### Thicknet and Thinnet

The earliest Ethernet implementations used a fairly stiff 10mm diameter coaxial cable. A workstation or other networked device could be connected only at certain points on the cable. The cable, in fact, bore markings every 2.5 metres to indicate where so-called “vampire taps” could be clamped onto it. The interval was calculated to minimise interference from radio-frequency reflections inside the cable. The thickness and stiffness of the cable prompted the nickname “Thicknet”, even after the formal IEEE designation 10BASE5 was given to the system. A few years later, a variation using thinner coaxial cable was introduced. The cable was only 6mm in diameter, less expensive, and a great deal more flexible. Taps could be placed at any point on the cable. The system came to be called “Thinnet” and bore the designation 10BASE2.

The IEEE nomenclature is still used, and it’s worth a short description here. The “10” indicates the maximum speed of data sent across the cable, in megabits. The 10-megabit value was not the design speed of the interface but the highest speed that the cable-based infrastructure could deliver. Early Ethernet implementations operated at less than half that speed. The “BASE” indicates _baseband_ transmission. In baseband transmission, the digital signal on the physical medium is a pattern of actual bits encoded as transitions from 0 volts to some arbitrary line voltage. This is in contrast to _broadband_ transmission (think cable TV), which imposes a signal on a radio-frequency carrier wave using various modulation schemes. In both modes of transmission, data travels at frequencies high enough to be considered RF. The number at the end of the designation (here, 5 or 2) indicates the maximum length of a network segment, in hundreds of metres. In 10BASE2 the “2” is an exaggeration; in practice, the segment length maxes out at 185 metres.

### The Basic Ethernet Idea

Ethernet has evolved a great deal since its introduction in 1980. To explain its modern form, we have to begin with its original mechanism as implemented in Thicknet and Thinnet. Both forms use coaxial cable to connect some limited number of computers. All computers on the network are _peers_; that is, none have special hardware or software that is not present in all of the others. Any computer on the network can send or receive Ethernet packets to any other computer on the network.

Ethernet originated the idea of a MAC address, and every device attached to the cable (which may include printers and other special-purpose devices like file servers) has a unique 48-bit numeric address, generally expressed as six groups of two hexadecimal digits. Any device with a MAC address, whatever its nature, is called a _node_. The MAC address is in fact more of an ID code than a true address. Unlike IP addresses (which are covered a little later in the “[Names vs. Addresses](#10_9781119183938-ch07.xhtml#c07-sec-0020)” section) a MAC address says nothing about _where_ its device is located on the network and is used only to tell nodes apart. As 48 bits can identify 281 _trillion_ different devices, we won’t run out of MAC addresses any time soon. That said, a few duplicates are known to have been issued by mistake, and with some equipment, including the Raspberry Pi, it’s possible to change the MAC address to mimic another device.

When the network is quiet, all nodes are “listening”; that is, their NICs are ready to receive data from the cable. At any time, a node may place a packet on the cable. On baseband technologies like Ethernet, that simply means that the packet’s bits are imposed on the cable as serial changes in voltage levels, one after the other. Each NIC accumulates bits from the cable in a buffer until the complete packet is present. They then strip off the preamble and delimiters and examine the destination MAC address present in the Ethernet frame. If the destination MAC address matches the NIC’s MAC address, the frame is retained. Otherwise, the frame is ignored. See Figure 7-4.

![[FIGURE 7-4:](#10_9781119183938-ch07.xhtml#rc07-fig-0004) How Ethernet works](./media/images/9781119183938-fg0704.png)

### Collision Detection and Avoidance

There’s a certain elegance in the original Ethernet idea: _Here’s a packet; if it’s yours, keep it_. However, there was a downside to the early Ethernet’s simplicity: collisions. An Ethernet network has no central controller. Any node may place a packet on the network at any time. The nodes are aware when another node is transmitting, and they wait for the current packet to be sent (plus a short additional time period) before beginning their own transmissions. However, when the network is quiet, nothing prevents two or more nodes from beginning a transmission at the same time. This results in a _packet collision_, which generally means that all packets in the attempt are lost.

Collisions in shared-medium Ethernet are detected in an interesting way: when two pulses from two nodes enter the cable at the same moment, the pulses “add” electrically, and the signal voltage on the cable is higher than during normal network traffic. The NICs monitor the signal voltage while transmitting, and a higher-than-normal voltage indicates a collision.

When any transmitting node detects a collision, it ceases to send the current packet and begins sending out a _jam signal_, which is a bit pattern that disrupts the error-detection bits at the end of the frame. Other nodes on the segment will see the packet as damaged and drop it. When the network becomes quiet, those nodes that had collided wait a random period of time called a _backoff period_ (typically only a few microseconds) before attempting to transmit again. The random backoff periods are different for both nodes, making it less likely that the colliding nodes will collide again when they attempt to retransmit their jammed packets.

The backoff period isn’t just a random delay value drawn from a fixed distribution. An algorithm called _truncated binary exponential backoff_ is used to vary the distribution of the backoff period based on collision frequency. An initial collision triggers a random backoff period of either 0 or 1 slot (where a _slot_ is the time normally taken to transmit 512 bits) before attempting retransmission. If packets collide again, a random period of between 0 and 3 slots is used; with each collision the maximum period doubles, until after ten collisions the period is between 0 and 1023 slots in length. The maximum period is then held constant at 1023 slots for a further six collisions, after which the station attempting to transmit stops trying and discards the packet. The overall effect is to slow down network activity during congested periods, “spacing out” retransmitted packets so that the network doesn’t simply grind to a halt in a storm of packet collisions.

This protocol is called CSMA/CD, which stands for Carrier Sense Multiple Access with Collision Detection. “Carrier sense” is a bit of a misnomer here. Base band systems like Ethernet have no carrier, which is technically a radio frequency wave on which signals are imposed via modulation. In this case, it only means that nodes on the network have a way to determine when other stations are transmitting.

A network segment containing nodes that may transmit packets that collide is called a _collision domain_. On early Ethernet systems, the entire network was a single collision domain, which meant that throughput degraded as more nodes were added to the network and collisions became more frequent. We’ll return to collision domains a little later, in connection with Ethernet bridges and switches.

### Ethernet Encoding Systems

Down at the physical level of the OSI model, Ethernet NICs encode the data to be transmitted by imposing a series of voltages on the network medium. The many variations of Ethernet each use a different encoding scheme; here we will briefly describe the schemes used by the 10Mbit standards (10BASE5, 10BASE2 and 10BASE-T), and the dominant 100Mbit (100BASE-TX) and 1Gbit (1000BASE-T) standards.

The electrical design of Ethernet requires us to choose encodings that have a very small DC component (that is, a long-term average voltage of close to zero), regardless of the data being transmitted. Signals from the NIC are inductively coupled onto the shared medium via transformers, which act as high-pass filters; if a DC component were present then this filtering would distort the signal, making it hard for a receiver to accurately recover the transmitted data. An encoding should also be _self-clocking_, possessing sufficiently frequent level transitions to allow the receiver to infer a clock with which to sample the signal. There are obvious parallels here with the encodings used to store data on magnetic media, which is described in [Chapter 6](#09_9781119183938-ch06.xhtml).

10BASE5 and 10BASE2 (along with 10BASE-T; see the “[10BASE-T and Twisted Pair Cabling](#10_9781119183938-ch07.xhtml#c07-sec-0016)” section later in this chapter) encode bits via _Manchester encoding_, shown in Figure 7-5. Each data bit is encoded in one clock cycle, with a transition at the centre of the cycle encoding the bit: a transition from negative to positive is considered a 1-bit, and a transition from positive to negative is considered a 0-bit. If necessary, an extra transition is inserted at the start of a cycle, to put the line into the correct state to encode the bit. The arrows in the figure show you which transitions encode data and which directions the transitions take.

![[FIGURE 7-5:](#10_9781119183938-ch07.xhtml#rc07-fig-0005) Manchester encoding](./media/images/9781119183938-fg0705.png)

Manchester encoding trivially meets our requirements for being self-clocking (as every bit has at least one transition) and having 0 DC component (as half of each bit period is spent at each voltage level). These properties come at a price, however: the extra transitions introduced by the encoding increase the bandwidth of the signal to around 20MHz. To go beyond 10 Mbps with affordable cabling, it was necessary to devise more efficient encoding schemes.

One such scheme, used by 100BASE-TX Fast Ethernet,is _4B/5B_, so named because it encodes each four data bits into five bits for transmission. The 5-bit encoded group is called a _symbol_. The encoding is performed using a simple static dictionary, shown in Table 7-1, in which each unique 4-bit group translates to a unique 5-bit symbol. The code used in 4B/5B was designed to provide at least a single level transition for every four bits of data. This ensures that the transmitted bitstream is self-clocking, even in the presence of long strings of 0- or 1-bits.

<figure> <figcaption>

[Table 7-1](#10_9781119183938-ch07.xhtml#rc07-tbl-0001) 4B/5B Encoding

</figcaption>

**Data word** **4B/5B code word** --------------- --------------------- 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1

</figure>

Given a data rate of 100Mbit/s, applying 4B/5B coding results in a line rate of 125Mbit/s. Rather than transmitting the encoded bits directly, however, 100BASE-TX applies a second encoding technique, borrowed from an earlier standard called FDDI (Fiber Distributed Data Interface, used in fibre-optics connections). This second encoding technical is MLT-3 (multi-level transmit using 3 levels). Given three voltages -V, 0 and +V, MLT-3 encodes a 0-bit by continuing to transmit the current voltage, and a 1-bit by moving to the next voltage in the sequence (0, +V, 0, -V). The maximum fundamental frequency of the resulting signal is 31.25MHz, as it takes a minimum of four bit periods to cycle through the sequence, allowing us to use cost-effective Category 5 cabling, which is covered in more detail shortly. MLT-3 encoding is shown in Figure 7-6.

![[FIGURE 7-6:](#10_9781119183938-ch07.xhtml#rc07-fig-0006) MLT-3 encoding](./media/images/9781119183938-fg0706.png)

While the combination of 4B/5B and MLT-3 coding satisfies the self-clocking requirement, it does not ensure zero DC balance. A partial solution is provided by applying a reversible _scrambling_ procedure to the 4B/5B-encoded bitstream. This involves applying the XOR logical operation between the bitstream and a pseudorandom bit sequence and ensures that, in almost all cases, the MLT-3 output spends 25% of its time in the -V state and 25% in the +V state. Given that the scrambler sequence is known and fixed, it is of course possible to carefully construct a bitstream that cancels out the scrambling, resulting in significant DC bias. Such _killer packets_ may be assumed to be rare in practice; nonetheless, most NICs contain circuitry that will detect and compensate for DC offset if it occurs.

100BASE-TX makes use of two pairs of conductors, one carrying data in each direction. Figure 7-7 shows the 100BASE-TX encoding and decoding scheme in its entirety.

![[FIGURE 7-7:](#10_9781119183938-ch07.xhtml#rc07-fig-0007) 100Base-TX encoding and decoding](./media/images/9781119183938-fg0707.png)

The 1000BASE-T standard provides data rates of 1 gigabit per second while maintaining the same symbol rate as 100BASE-TX (125Msymbols/s). It accomplishes this by using four pairs of conductors (versus one for 100BASE-TX), and by using a denser 5-level amplitude modulation (versus three levels for 100BASE-TX). There are 5^4 = 625 possible symbols, so the theoretical raw bit rate is 125Msymbols/s \* log2(625) = 1160 Mbps. The “spare” coding capacity is used to implement a low-density forward error correction scheme known as _trellis coding,_ the exact details of which are beyond the scope of this book. This approach effectively compensates for the increase in raw error rate caused by the denser amplitude modulation.

In contrast with 100BASE-TX, which implements full-duplex communication using a dedicated pair of conductors for each direction, 1000BASE-T supports simultaneous bidirectional transmission over the same set of conductors. To accomplish this, each receiver subtracts the (known) output of the local transmitter from the voltage observed on the line, leaving only the incoming signal (if any).

### PAM-5 Encoding

After a data stream has been encoded, it must be coupled to the Ethernet medium. This is generally done through small transformers in the NIC. The encoded data stream is a series of digital pulses that exist at one or the other of two voltage levels. When a digital signal is encoded between two voltage levels in this way, it’s called _binary signalling_. Typically, a positive voltage level represents a 1-bit and a negative voltage represents a 0-bit.

Higher data rates, like those of Gigabit Ethernet, require denser encoding. The system widely used today is five-level _pulse amplitude modulation_, abbreviated to PAM-5. PAM-5 encodes 2 bits per pulse by varying the signal voltage over five levels rather than only two. Two of the five levels are positive voltages, two are negative voltages and the fifth is 0 voltage. When information is encoded as a varying voltage level in a signal, it’s called _amplitude modulation_. PAM-5 varies the amplitude of the pulses that make up the encoded data stream, hence the name pulse amplitude modulation. Schemes like this are called _multi-level signalling_, because more than two voltage levels are used to encode data.

We’ve drawn a PAM-5 data stream in Figure 7-8, which is a graph of pulse amplitude over time. The grey bars are pulses, and the wide black line is the amplitude for the stream of pulses. Each pulse is considered a symbol, because one pulse encodes two bits. The 0V level does not encode any particular value, and its primary purpose is to allow the receiver to extract a clock signal from the data, and to facilitate error correction using a technology called _forward error correction_ (FEC). The details of FEC as used in Gigabit Ethernet are beyond the scope of this book. As a broad overview, though, additional bits are added to a data stream, which allows the receiver to identify and correct a limited number of errors without reversing the line (hence “forward”) to request retransmission of data. Forward error correction has much in common with error-correcting code (ECC) memory, which is covered in [Chapter 3](#06_9781119183938-ch03.xhtml).

![[FIGURE 7-8:](#10_9781119183938-ch07.xhtml#rc07-fig-0008) PAM-5 encoding](./media/images/9781119183938-fg0708.png)

The actual graph of a PAM-5 waveform is not as “clean” as the graph in [Figure 7-8](#10_9781119183938-ch07.xhtml#c07-fig-0008), especially at gigabit speeds. Noise gets into the waveform, and extracting symbols from the waveform is a serious challenge to the electronics on the receiver side.

### 10BASE-T and Twisted-Pair Cabling

The early Ethernet implementations based on coaxial cable had a number of problems, collisions being the least of it. 10BASE2, in particular, was vulnerable to mechanical connection issues. Cables had a coaxial connector on each end, and at each node, two cables came together in a coaxial “T” connector; these connectors were inexpensive, and even a comparatively gentle tug on a cable was sometimes enough to interrupt the continuity of the bus. Once the bus had been split in two in this way, radio-frequency signal reflections from the break point would prevent communication even between hosts on the same half. In addition, there were at least twice as many vulnerable connections as there were nodes on the network.

In the late 1980s a new variation of Ethernet appeared: 10BASE-T. The “T” stood for _twisted pair_, which is a way of wrapping two thin (usually 24-gauge) copper wires around one another to reduce interference from external noises sources. To transmit a bit on a coaxial cable, the transmitting NIC applies a voltage to the centre conductor of the cable, with the outer cable shield acting as the ground return path; this is referred to as _single-ended_ signalling. To do the same on a twisted-pair cable, the transmitting NIC applies two different voltages to the two conductors: zeros and ones are represented by positive and negative _differences_, rather than by absolute voltages, so this is referred to as _differential_ signalling. The receiving NIC can extract the encoded data using a differential amplifier, which converts the voltage difference between its inputs into a single output.

Differential signals travelling over tightly twisted pairs of wires have low electromagnetic emissions (because emissions from one wire will be very nearly cancelled by emissions from the other) and good immunity to interference (because interference will create almost the same voltage change on both wires, without affecting the difference between them). Figure 7-9 illustrates the way differential transmission schemes using balanced lines operate.

![[FIGURE 7-9:](#10_9781119183938-ch07.xhtml#rc07-fig-0009) Balanced transmission lines for data](./media/images/9781119183938-fg0709.png)

A 10BASE-T cable consists of four twisted pairs in one jacket, terminated in 8-conductor modular plugs. A cable of that construction that has been tested to a transmission speed of 100MHz is considered _Category 5_ (informally “cat 5”), as defined in the ANSI/EIA-568 cabling standard. Category 5 cables can be used for other sorts of signals, including both audio and video, but Ethernet is now the primary use for Category 5 and its plug-compatible but faster successors, Category 5E and Category 6.

Why four twisted pairs in one jacket? As described earlier, the dominant Gigabit Ethernet technology, 1000BASE-T, achieves its greater throughput in part by splitting a single data stream into four parallel bidirectional streams, each one with its own twisted pair in a Category 5, 5E, or 6 cable. Slower Ethernet technologies may not use all four pairs, and some, like 10BASE-T, use one unidirectional pair for transmit and one for receive.

### From Bus Topology to Star Topology

Category 5 cabling was not the only change in going from 10BASE2 to 10BASE-T. 10BASE-T networks are a whole different _shape_. The way that nodes are connected in a network is called the network _topology_. 10BASE5 and 10BASE2 networks used _bus topology_, in which all nodes are simply daisy chained along a single stretch of coaxial cable. By contrast, 10BASE-T networks use _star topology_, in which all nodes connect to a central _network hub_. See Figure 7-10 for a comparison of the two topologies.

![[FIGURE 7-10:](#10_9781119183938-ch07.xhtml#rc07-fig-0010) Bus topology versus star topology](./media/images/9781119183938-fg0710.png)

The central hub was necessary because 10BASE-T networks use separate differential pairs for transmit and receive. This allowed _full duplex_ operation, by which a node may send and receive data simultaneously. However, it also required that the transmit wires of each node be connected to the receive wires of every other node. This was done at the hub. Early Ethernet hubs were just passive connectors, in which the appropriate wires from each node were connected to the appropriate wires from all the other nodes. Later hubs reduced _crosstalk_ (signal interference between nearby wires due to inductive and capacitive coupling) and _shot noise_ (static from motors, relays and similar electrical equipment) by adding digital amplifiers between each leg of the hub and the central connections. This led to fewer damaged packets and improved overall network throughput. Such active hubs were originally called _repeater hubs_ or _repeaters_ (because the amplifiers took a weak or noisy signal and retransmitted it as a stronger and cleaner signal) but today are known simply as network hubs or Ethernet hubs. Purely passive hubs are no longer used.

In truth, hubs aren’t quite as simple as amplifiers that strengthen and “clean up” packets. A hub used as a link between two network segments isolates the segments from one another with respect to cabling disruptions like bad coaxial connectors.

As good as 10BASE-T cabling and hubs were, the system still connected all nodes to all other nodes as a single collision domain. This meant that packet collisions continued to be an issue with hubs, and collision detection schemes had to be present to manage them.

> [!NOTE]
> that hubs are layer 1 devices, and at the physical layer what a hub does is mostly amplify and thus “clean up” the signals passing through the hub. To do more than that, you need more than a hub.

### Switched Ethernet

An elegant solution to Ethernet packet collision overhead didn’t appear until 1990. The firm Kalpana (later acquired by Cisco) invented a _switching hub_ for Ethernet networks. Switching hubs occupy the same position in star topology networks as conventional hubs do, in that all nodes on a star network are connected to one port on the switching hub at the network’s centre. However, in contrast to conventional hubs, switching hubs operate at layer 2 on the OSI reference model, having some awareness of the data that is being passed through them.Today, such devices are simply called _network switches_, and Ethernet networks could not be as fast or reliable as they are without them.

Network switch technology grew out of an earlier concept called a _network bridge_. The first network bridges were two-port devices that allowed two separate network segments to communicate. The bridge is necessary if the two segments use different technologies (say, 10BASE2 and 10BASE-T) or the same technology operating at different speeds, or if the size of the entire network exceeds the maximum allowable segment size for the technology in question (185m in the case of 10BASE2). A network bridge receives and buffers packets from one segment, and retransmits them on the other when that segment’s medium is quiet; in doing so it prevents the two bridged network segments from becoming a single collision domain. Collisions do not propagate through bridges, allowing each of the two bridged network segments to have more nodes than a single segment could handle.

Very simply put, a network switch creates a momentary dedicated connection between two and only two network nodes. When one node wishes to send a packet to another node connected to a switch, the switch creates the connection for just long enough to allow the packet to pass between them. Because during that brief moment the nodes are on what amounts to an isolated, two-node network, collisions are impossible, and no time or bandwidth is spent on collision detection and retransmission. Other nodes on the network do not see packets passed between the two nodes connected through the switch. Network switches for home use have four, five, or perhaps eight ports. Switches used in corporate environments may have hundreds. Figure 7-11 shows a network switch.

---

> [!NOTE]

We need to be very clear that [Figure 7-11](#10_9781119183938-ch07.xhtml#c07-fig-0011) is a metaphor: network switches are fully electronic, and there are no mechanical switch contacts inside them. Modern switches are capable of sustaining multiple simultaneous connections between many different pairs of nodes, so that more than a single packet can pass through the switch’s _crossbar_ (the matrix of electrical switching logic that connects ports to one another) at any given time.

![[FIGURE 7-11:](#10_9781119183938-ch07.xhtml#rc07-fig-0011) How network switches work](./media/images/9781119183938-fg0711.png)

To do their job, network switches must contain considerably more intelligence than hubs. A switch maintains a table of MAC addresses for all the nodes connected to its ports; using this table, it can instantly associate the MAC address of an incoming packet with an outgoing port and thus make a temporary connection between two hosts. Building and maintaining the table is done in two ways:

- By broadcasting a packet to the reserved MAC address `FF:FF:FF:FF:FF:FF`</code>, the switch can request that all nodes reachable through the switch’s ports respond with their MAC addresses. Some computers also broadcast their own MAC addresses to the network when they are powered up or rebooted. - By listening to both the sending and receiving addresses in all packets that it handles, a switch can verify the MAC addresses reachable on any of its ports.

The simplest possible switch would buffer incoming packets in memory until it receives an entire packet and verifies that it is complete and not corrupt. Only then does it begin forwarding the packet to the destination host. This is called _store-and-forward_ switching. To improve throughput, a technology called _cut-through_ switching was developed. In cut-through switching, the switch inspects an incoming packet only until it has the complete destination address, at which time (if no other transmission to the destination is in progress) it immediately begins forwarding the packet to the destination host. Without buffering overhead, this gets the packet to the destination in the shortest possible time. However, cut-through switching does not verify that packets are complete, and will forward incomplete or damaged packets. The destination host will detect the damaged packet and discard it. If this happens often enough, the throughput benefits of cut-through switching will be lost.

Using switches and hubs isn’t an either/or situation. They can be freely mixed in Ethernet networks, as shown in Figure 7-12. In the figure, four nodes are connected directly to the Ethernet switch. A hub connecting three additional nodes is also connected to the switch. The key issue in using hubs is that collisions again become possible on the leg of the network connected by a hub. The highlighting in [Figure 7-12](#10_9781119183938-ch07.xhtml#c07-fig-0012) shows the four-node collision domain within the network. The switch can create a dedicated connection between node 003 and the hub, but the hub connects nodes 004, 005 and 006 in a way such that the switch cannot reach any of the three individually. If nodes 004 and 006 begin transmitting a packet simultaneously, the packets will collide, and all the usual collision overhead will apply.

![[FIGURE 7-12:](#10_9781119183938-ch07.xhtml#rc07-fig-0012) Mixing switches and hubs](./media/images/9781119183938-fg0712.png)

The situation illustrated in [Figure 7-12](#10_9781119183938-ch07.xhtml#c07-fig-0012) comes up often in wireless networking because wireless access points (APs) are conceptually closer to hubs than to switches.

## Routers and the Internet

One way to think about LANs versus WANs is that LANs are networks of computers, and WANs are networks of networks. It wasn’t that way in the very beginning, when WANs primarily connected large, lonely computers at one company or university with large, lonely computers at other companies and universities. Today, of course, no organisation ever has just one computer, and this is often the case for individuals at home too. No matter how simple or inexpensive, every computer, smartphone or tablet has a network port of some kind, be it wired, wireless or both. With LANs everywhere, the next step is to allow one LAN to network with other LANs. This is what the Internet was designed to do. And although Internet mechanisms go well beyond the stated topic of this chapter (Ethernet) the Internet protocol suite is very much involved in even the smallest local-area network—a network of one device—that connects to the Internet.

### Names vs. Addresses

On a LAN, a node is identified by its MAC address. MAC addresses are (ideally) unique, so theoretically a node should be able to contact another node on the other side of the world by placing the MAC address of the faraway node in a packet. This doesn’t work for an obvious reason: a MAC address contains no information about where its node actually _is_. As a metaphor, think of people at a meeting around a conference table. Everyone can see everyone else around the table, and when anyone talks, everyone can hear. That’s how LANs work. At the same time, other people are sitting around other conference tables in other buildings, talking in the same local way. How can you get two such meetings to talk to one another? If both conference tables have speaker-phones, one table’s phone can call the other, and the two conference tables will be in communication.

A phone number isn’t just an ID code. A phone number consists of several parts in most nations. In the U.S., this is a country code, an area code, an exchange and a subscriber number. Each level contains information about the physical location of the phone and each level narrows the location down further. For example, a phone might be in the U.S. (North America code +1), in the Colorado Springs metro area (area code 719), in an exchange (674) and at some four-digit subscriber number within that exchange.

The Internet uses a system very much like this. As we’ve mentioned briefly earlier in the chapter, the collection of rules and techniques that enable packet-based communication over the Internet is called the _Internet Protocol_ (IP). Within the Internet protocol is an addressing scheme based on a type of numeric address called the _IP address_. The IP is intimately connected with a higher-level protocol called _Transmission Control Protocol_ (TCP). If you refer to [Figure 7-1](#10_9781119183938-ch07.xhtml#c07-fig-0001), you see that TCP is immediately above IP on the OSI reference model.

The IP is focused on addressing and routing _packets_; the TCP is focused on establishing and maintaining _connections_ between computers so that packets may be transferred. TCP is the Internet’s delivery mechanism: it makes sure that packets actually get where they’re going, and that the order of a stream of packets is preserved as it travels from computer to computer. IP and TCP work together and are rarely used separately. This is why most of the time you’ll see them referred to as TCP/IP.

### IP Addresses and TCP Ports

An IP address has two parts. One is the address of a network, and the other is the address of a particular node (in Internet jargon, a _host_) present on that network. Unlike a MAC address, which is more of a name or an ID code, an IP address really is an address, and allows a network appliance called a _router_ to locate a network and a host based on that address.

By convention, IP addresses are usually written out this way, as a so-called _dotted quad_:

```
	264.136.8.101`
```

Each group of numbers separated by periods is called an _octet_, which in computer science means an 8-bit quantity. If you’re sharp you may have noticed that 264 is not expressible in 8 bits. That was deliberate. In writing this chapter, we don’t want to use someone’s actual IP address, and the custom in books and papers is to create imaginary addresses for examples by using a value greater than 255 for the first octet.

In an IP address, one or more of the higher-order octets contains the network address, and one or more of the lower-order octets contains the host address. In Figure 7-13, on the right is a LAN with four hosts. Between the LAN and the outside world is a router. In this case, the three higher-order octets contain the address of the network. The lowest octet contains the address of a particular host. Given an address like 264.136.8.101, a host anywhere in the world can create a TCP connection with the top computer in [Figure 7-13](#10_9781119183938-ch07.xhtml#c07-fig-0013).

![[FIGURE 7-13:](#10_9781119183938-ch07.xhtml#rc07-fig-0013) The two parts of an IP address](./media/images/9781119183938-fg0713.png)

Larger networks with more than 255 hosts divide the IP address differently, with more octets devoted to the host portion of the address, and fewer to the network portion of the address. The split between the network portion and the host portion of an IP address is specified by a four-octet bit pattern called a _subnetwork mask_. (“Subnetwork” is often shortened to “subnet”.) The mask is used to separate the two portions of the address for further processing. Where networks are nested one inside another, a separate subnetwork mask (informally called a “subnet mask”) is used for each of the separate networks.

Internet routing is complex, and the details of how routers work internally is beyond the scope of this book. As mentioned earlier in this chapter, they use an internal routing table to look up network addresses to discover how to reach them. An entry in the routing table provides information allowing the router to choose a route by which the destination network may be reached. A given router cannot necessarily access any arbitrary network with one single connection. It may take several sequential connections (called _hops_) to reach a given destination. At the end of each hop there’s another router, and that router forwards the packet to the next router along the route. Eventually the packet arrives at the destination network, and that network’s router forwards the packet to the individual host to which the packet was addressed.

Routers come in many sizes, from a home network router that can fit in the palm of your hand to a cabinet the size of a refrigerator that weighs hundreds of pounds. The routing table in a large router may have hundreds of thousands of entries. The routing table on a home router typically has only one, which contains the address of the home’s ISP’s router. Any packet originating on a home router has only one possible path—through its ISP—to the rest of the Internet. So the home router forwards all packets to the ISP’s much larger and more powerful router, which then selects the next hop on the route.

The TCP protocol creates connections between two hosts (one or both of which may be servers) using IP addresses. These connections, however, are not simply between computers or other network devices. Connections are actually between two software applications running on those computers. (Refer to [Figure 7-1](#10_9781119183938-ch07.xhtml#c07-fig-0001), and the application set of the OSI reference model.) A web browser on your tablet or computer connects to a web server on a remote host. An email client on your tablet or computer connects to an email server on the remote host. This final piece of routing is accomplished using _port numbers_, which are 16-bit values that (as we saw earlier) are present in every IP packet and allow the network stack on a host to identify which application should receive each packet. We refer to the act of splitting a single stream of incoming packets from the network into multiple streams of packets on the basis of destination port numbers as _demultiplexing_.

When a client application wants to establish a TCP connection to a server application, it begins by assigning an arbitrary unused local port number to uniquely identify its end of the connection. It then sends a connection request, specifying a destination port number; this is not an arbitrary number but instead is generally one of several _well-known port numbers_, which are associated with a higher-level protocol. HTTP is associated with port 80, email with ports 25 (sending via SMTP) and 110 (receiving via POP), SSL (Secure Sockets Layer) with port 443, FTP (File Transfer Protocol) with port 21, and so on. A server application must “listen” to a port for TCP to make a connection to that port; if there is nobody listening on port 80, for example, there is no web server in operation at the remote host. When a connection is accepted, an arbitrary unused port number is assigned to the server end of the connection; further communication happens via this port number, freeing up the well-known port to accept further incoming connections.

Routers can block connections that use specific port numbers as a security measure, to prevent remote connections to unauthorised servers. For example, a common way to combat email spam is for hosting services to configure their routers to block port 25, which is assigned to the Simple Mail Transfer Protocol (SMTP). Certain software is difficult to block by port number because the protocols allow the use of any open port via “port discovery”, which basically means that the two hosts attempt a connection on a range of ports until they find one that works. (BitTorrent is a good example of such an adaptable protocol.)

Ports are also important in a router-based function called Network Address Translation (NAT), which is covered a little later in the “[Network Address Translation](#10_9781119183938-ch07.xhtml#c07-sec-0023)” section.

### Local IP Addresses and DHCP

When the architects of the original Internet defined the suite of Internet protocols that include the IP addressing system, they never imagined that the general public would one day be connecting to the Internet by the billions. They also didn’t envision that devices as mundane as telephones, TV sets and even refrigerators would someday want their own IP addresses as well. This has led to a serious problem: there only 4.3 billion possible 32-bit IP addresses, which isn’t nearly enough to give one to every person (or refrigerator) on Earth.

Several things are being done to deal with this shortage of IP addresses. The high road is to create a whole new addressing scheme with larger addresses, which is being done in the IPv6 project. (The current 32-bit IP addressing system is called IPv4.) The IPv6 address space is 128 bits wide. This allows it to support up to 2<sup>128</sup> different addresses. That number works out at 3.4 × 10<sup>38</sup>, which dwarfs the total number of stars, planets, moons and asteroids in the observable universe.

At this writing, only about 10% of Internet traffic uses IPv6 addresses.The expectation is that IPv6 will eventually dominate the Internet. In the meantime, the shortage has been ameliorated to some extent by the use of _local IP addresses_. The Internet Assigned Numbers Authority (IANA) has set aside four blocks of IP addresses as local, meaning that they cannot be routed and are basically invisible except within their own local networks. This makes them sound useless, but in fact local IP addresses make it possible to use the TCP/IP-based Internet services within a LAN, where there is no router between any two hosts. Because local IP addresses cannot be seen outside of their local network, there’s no danger in reusing them. Hundreds of millions of people can use the address `192.168.1.100` at the same time. The following are the four blocks of local IP addresses:

```
	10.0.0.0– 10.255.255.255`

	169.254.0.1 – 169.254.255.254`

	172.16.0.0 – 172.31.255.255`

	192.168.0.0 – 192.268.255.255`
```

Local IP addresses can’t be seen past a router, but in nearly all home networks, the router performs an important service: it distributes local IP addresses to the nodes in its own network. A piece of software called a _Dynamic Host Configuration Protocol_ (DHCP) server runs in the router, and when a node comes online and asks for network configuration, the DHCP server scans the addresses in its local IP address table and sends down a local address that isn’t already being used. A number of other configuration options (including the subnet mask) are sent down at the same time with the local IP address, but they’re beyond the scope of this chapter.

The node making the request gets a _lease_ on the IP address for a limited period of time, often 24 hours. When the lease expires, the address goes back into the free address pool. If a node is still on the network when its IP address lease expires, it simply requests that the lease be renewed. A reasonable expiration period on DHCP leases (24 hours or more) allows nodes to be powered down overnight without losing their leases. The next time a node is powered up, it will still have the same IP address.

DHCP isn’t used only for distributing local IP addresses to LANs. Internet service providers (ISPs) run DHCP servers as well, and when a home router connects to its ISP, the ISP’s DHCP server sends down configuration information to the home router, including a _global_ IP address. This address is how your LAN is known to other networks across the Internet.

An IP address distributed by a DHCP server, whether local or global, is called a _dynamic IP address_. Dynamic IP addresses are used in situations where the address may change without disrupting network operation. Server software that can be accessed from the Internet requires an IP address that doesn’t change. Such an address is a _static IP address_. Internet hosting services that allow you to run your own servers on the Internet are allocated blocks of static IP addresses. When you establish an account with a hosting service, you are provided with a static IP address for your server. That one static IP address is how people and other servers on the Internet can find your server.

It’s possible to manually assign static local IP addresses to nodes on a LAN. Such addresses are not leased and don’t expire. They are useful for nodes like network printers that are accessed by other network nodes via their IP address. If a network printer’s IP address changes, some nodes on the network may not be able to access it. Most network printers include instructions and sometimes software allowing a static local IP address to be assigned to the printer.

The local IP addresses that begin with 169.254 have a special use: all Windows versions from Windows 2000 onwards implement a service called _Automatic Private IP Addressing_ (APIPA), which provides a local IP address from the 169.254 block any time a DHCP server is not available to provide a local address. A Windows node with an APIPA address can communicate with any other node on its local network segment that has an APIPA address. This allows small numbers of computers to connect through a switch without requiring a router. The more general term for a system that automatically provides local network addresses and other configuration parameters is _zero-configuration networking_. A similar system called Avahi exists for Linux, but Raspbian does not include it by default and it must be installed manually if it is desired. Zero-configuration networking is primarily useful for small networks that have no connection to the Internet and thus no router or DHCP server to handle local segment configuration.

### Network Address Translation

Local IP addresses are invisible to other networks beyond the local router. How, then, can TCP/IP allow nodes with local IP addresses to connect to the Internet? The answer is another software service running on the router: _Network Address Translation_ (NAT). Simply put, NAT translates a non-routable local IP address into a global, routable IP address. In addition, it provides, almost as a by-product, fairly strong protection against unwanted connections from outside the local network. Figure 7-14 is a sketch of a possible home network setup: four computers, a router, and a switch. In many, or even most, cases these days, the router and the switch are combined into a single physical appliance. (They’re broken out here for conceptual clarity.) Each of the network’s four computers has a local, non-routable IP address. NAT is running inside the router. NAT keeps these local IP addresses in a table that it maintains for its own use.

![[FIGURE 7-14:](#10_9781119183938-ch07.xhtml#rc07-fig-0014) How Network Address Translation (NAT) works](./media/images/9781119183938-fg0714.png)

As we explained earlier, the network as a whole has a single public, routable IP address that is the only address for any network node that can be seen by the outside world. This address resides in the network’s router, and for home networks it is generally provided by the ISP’s DHCP server. Local IP addresses are not routable, and to create connections between individual computers on a local network segment and hosts on the other side of the router, the router creates “extended” IP addresses by combining the local IP address assigned to a device on the local network with a TCP port number. Which port number is used isn’t important, as long as it isn’t already used by anything else on that particular network. (There are more than 65,000 different port numbers, so finding a free one on even a modest-sized network is rarely a problem.) NAT stores extended IP addresses for its local nodes in an internal table that acts as a sort of “internal phone book” for devices on the local network segment. This table is not accessible from the Internet. Only NAT can read it or change it. On Linux systems, this process is called _IP masquerading_. In a sense, the router is assigning port numbers as ID codes to the computers on its local network.

When one of the computers inside the network wants to connect (for example) to a web server, NAT takes the web page request and places an extended IP address consisting of the router’s IP address plus the requesting computer’s port number into the request. When the web server establishes a connection, it uses this extended IP address, and not the internal, local IP address of the computer to which it connects. The connection is thus established with the router, _not_ the computer—and the router decides what material delivered from the web server can reach the computer. The web server has no knowledge of the requesting computer beyond its port number, and the port number alone does not allow a connection to a local IP address. Because the address that servers outside the local network must use is created by NAT, the connection must be initiated by NAT, in cooperation with one of the local network nodes. This prevents unsolicited connections from outside the local network.

NAT complicates matters when a user of a computer on a local network wants to run a publicly available server on the computer. Because outside users must be allowed to make a connection to the server, a way for connections initiated outside the network must be provided by the router. This is done through _port forwarding_, in which an outside request for a server connection is forwarded to a local IP address for the computer on which the server is running. NAT ensures that connections are made only to the server, and not to any other software on the computer running the server.

## Wi-Fi

One of the beauties of the OSI reference model is that it encourages engineers to “layer” their designs for networking hardware and software, with well-defined interfaces between adjacent layers. A non-obvious benefit of this layering is that a layer can be “swapped out” for a different layer without completely disrupting the operation of the stack from the perspective of the application layer.

Much of this layer-swapping has occurred at the bottom, at the data link layer and especially the physical layer. 10BASE2 and 10BASE-T provide two different physical media for the transport of network packets: one is a half-duplex system using coaxial cable, and the other is a full-duplex system using twisted-pair conductors. Both implement Ethernet networking, and from the perspective of the higher layers implementing TCP/IP and network applications, there’s no difference.

In the mid-1980s, researchers began to explore the notion of creating Ethernet-like data link and physical layers without wires at all, using radio waves or infrared light. The U.S. Federal Communications Commission, which governs the use of radio communication in the U.S., had opened a number of frequency bands to unlicensed use in 1985. In 1987, NCR created a wireless technology to link its cashier station products. It worked very well, and the firm developed the technology into a commercial product line called WaveLAN, which was placed on the market in 1988. A similar but incompatible system was developed by Canadian firm Telesystems SLW at about the same time, and was eventually spun off as Aironet. Hoping to see its technology incorporated into the IEEE 802 LAN standard, NCR contributed the design to the 802 standards committee in 1990. The IEEE proposed a new standard for wireless Ethernet and called it 802.11. The standard was published in 1997. This original 802.11 spec embraced a number of existing modulation technologies, bit rates and MAC schemes, making it more of a menu than a standard. (For example, it included a physical layer spec for modulated infrared light that never saw broad adoption.) There were so many choices that even products completely compliant with the standard could be incompatible with other compliant products.

Most wireless networking products that adhere to the 802.11 standard are referred to using the name _Wi-Fi_, from an early play on the term “hi-fi” for audio technology. “[Wi-Fi](#10_9781119183938-ch07.xhtml#c07-sec-0024)” is a trademark owned by a trade group called the Wi-Fi Alliance, and rights to use the term on products are not granted until the products are tested for compliance with the pertinent sections of the IEEE 802.11 standard.

### Standards within Standards

Quite apart from compatibility issues, early 802.11 products billed as “wireless Ethernet” offered bit rates of only 1 Mbps or 2 Mbps. This was far slower than 10 Mbps technologies like 10BASE2 and 10BASE-T, and the 100 Mbps and 1000 Mbps technologies that followed them. In the years after 1997, the IEEE 802.11 committee began work on several addenda to the 802.11 standard, defining new wireless technologies focused on improving throughput:

- **802.11a:** Operates on the 5GHz band, with a nominal bit rate of 54 Mbps, and a practical throughput of about half that, using TCP. The spec was finalised in 2000. - **802.11b:** Operates on the 2.4GHz band, with a nominal bit rate of 11 Mbps, and a practical TCP throughput of about 6 Mbps. The spec was finalised in 1999. - **802.11g:** Operates on the 2.4GHz band, but uses several technologies originally developed for the 802.11a standard to allow bit rates of up to 54 Mbps. As with 802.11a, practical TCP throughput is less than half that, at about 22 Mbps. The spec was finalised in late 2003. - **802.11n:** Operates on either the 2.4GHz band or the 5GHz band. It achieves much higher throughput than earlier technologies by using twice the channel bandwidth (40MHz) when possible, and multiple antennas with a technology called multiple-input, multiple-output (MIMO). Maximum bit rate can theoretically be as high as 600 Mbps, but the practical bit rate and TCP throughput depend heavily on local channel congestion and rarely top 100 Mbps. The spec was finalised in 2009. - **802.11ac:** Operates only on the 5GHz band. Its technology is an evolutionary extension of 802.11n, and achieves throughput close to 1000BASE-T (gigabit Ethernet) by using additional antennas and “bonding” adjacent 40MHz channels into 80 or 160MHz channels, where local spectrum use allows. The spec was approved at the beginning of 2014, and commercial products began appearing in large numbers later that year.

Although the IEEE formally withdraws addenda like these once they have been folded into the larger 802.11 spec and ratified, terms like *Wireless-B*and _Wireless-G_ continue to be used to differentiate products that do not support all available technologies. In practice, nearly all commercial products at this writing support standards b, g, and n on 2.4GHz, with some lines supporting 802.11a on 5GHz as well.

Many other addenda to 802.11 have been published and ratified since 1997, generally providing refinements to the primary spec in areas like mobile device roaming, quality of service, bridging networks and security.

### Facing the Real World

Going wireless complicates networking in a number of ways. Wired Ethernet keeps its signal inside a cable of some sort and, beyond certain physical limitations (especially the radius at which an Ethernet cable may be bent), where the cable can go, the signal goes. Wi-Fi uses microwaves travelling freely through the air, in and among buildings and other structures. The problems related to the microwave physical medium fall into several categories:

- Attenuation (reduction of signal strength) due to distance through free space, and the presence of walls and water-rich exterior factors like broadleaf trees, rain or snow - Microwave shadows cast by large metallic objects such as aluminium sides, filing cabinets, refrigerators and industrial equipment - Multipath interference caused by signals taking paths of different lengths from the transmit antenna to the receive antenna, and interfering with one another constructively or destructively at the receive antenna - Channel congestion, which is interference from Wi-Fi signals on the same or adjacent channels - Interference from other technologies using the same frequencies as Wi-Fi, including Bluetooth gear, cordless phones, medical devices, sensor networks and, on some frequencies, amateur radio transceivers - The hidden node problem, in which not all terminals participating in a network can see each other, causing difficulties for MAC

Even along unobstructed paths, microwaves transmitted from an omnidirectional antenna are attenuated by distance according to the inverse square law. When Wi-Fi hardware is used in fixed _point-to-point service_, as in links between buildings, directional antennas may be used to focus microwave energy along the path between a link’s two endpoints. This allows communication across gaps that would be impossible with omnidirectional antennas at the same power level.

Microwaves are electromagnetic radiation, and may be reflected as they travel from transmitter to receiver. Wi-Fi signals bounce off walls, floors, ceilings and large objects, especially objects made of metal. This causes multiple wavefronts to arrive at the receiver along paths of different lengths, and thus at (very) slightly different times. If two or more wavefronts arrive precisely “in phase” they can theoretically boost signal strength at the receive antenna. However, in virtually all cases, the many wavefronts interfere with one another in unpredictable ways, causing _fading_. Even worse, multipath fading effects can be strongly frequency-dependent, causing not just fading but distortion of wideband signals. Figure 7-15 illustrates multipath interference.

![[FIGURE 7-15:](#10_9781119183938-ch07.xhtml#rc07-fig-0015) Multipath interference](./media/images/9781119183938-fg0715.png)

In most Wi-Fi gear going back to the original Wireless-B, access points and wireless routers incorporate two antennas to deal with multipath interference. The ideal distance between them is one wavelength, which at 2.4GHz is 12.5cm, or just under five inches. The Wi-Fi receiver continuously samples signals on both antennas, and chooses the stronger of the two. This is called _diversity reception_. Having the antennas one wavelength apart optimises the chances that a usable signal is present on one antenna when the other antenna is subject to multipath interference.

Channel congestion is a consequence of a small number of distinct channels and of the way microwave spectrum space is allocated to those channels. Wi-Fi channels on 2.4GHz are not laid out nose-to-tail across the band, but overlap, as shown in Figure 7-16. Only three non-overlapping channels exist: channels 1, 6 and 11.

---

> [!NOTE]

Channels 1, 6 and 11 do not overlap with one another, but _do_ overlap with the channels to either side of them. A strong signal of some sort on channel 5 may make channel 6 unusable, for example.

![[FIGURE 7-16:](#10_9781119183938-ch07.xhtml#rc07-fig-0016) Wi-Fi frequency allocation on the 2.4GHz ISM band](./media/images/9781119183938-fg0716.png)

In crowded urban areas, interference from Wi-Fi gear on adjacent channels makes selecting a channel for use difficult. There are Wi-Fi survey apps available for mobile devices like tablets and smartphones that sample Wi-Fi signals and plot out their distribution across the 2.4GHz band on graphs. Once a survey app determines where neighbouring Wi-Fi gear is operating on the band, it becomes possible to choose the quietest channel currently available.

Exactly which channels are available is dependent on national radio frequency regulations. In the U.S., only the first 11 channels may be used. Additional channels 12 and 13 are available in many other countries, including the UK. Channel 14 is available only in Japan. France allows only channels 10-13, and Spain only channels 10-11. Channel allocation on the 5GHz band is complex and difficult to summarise. The band is larger and the individual channels wider, so high bit-rate technologies like Wireless-N work best at 5GHz.

The 2.4GHz band does not belong to Wi-Fi alone. Its formal name is the industrial, scientific, and medical (ISM) band, and many different classes of devices use it. Interference from such devices is not only possible but likely. The most familiar is the short-range Bluetooth wireless technology. Inexpensive cordless phones use 2.4GHz and are a common source of interference, as are microwave ovens, which may emit enough stray microwave energy to cause frequent frame retransmissions and a visibly slower link. If interference from industrial or medical equipment occurs, Wi-Fi users have no recourse but to relocate their gear to a different channel.

### Wi-Fi Equipment in Use

The simplest way to think of a wireless network is to replace a conventional wired Ethernet hub with a Wi-Fi appliance called a _wireless access point_ (AP.) The network shown in [Figure 7-12](#10_9781119183938-ch07.xhtml#c07-fig-0012) then becomes something very much like Figure 7-17. A Wi-Fi AP _is_ an Ethernet hub, using the Wi-Fi data link and physical layers rather than the data link and physical layer for twisted pair network technologies like 10BASE-T, 100BASE-TX or 1000BASE-T. Nodes (often called _stations_ in technical literature) that connect wirelessly use a type of NIC called a _wireless client adapter_. The term “client” here alludes to a sort of client/server relationship with the access point, which “serves” an Ethernet connection to one or more wireless clients. A wireless client adapter can be an add-on device (as it generally is in desktop computers) or an integral part of a mobile device like a laptop, tablet or smartphone.

![[FIGURE 7-17:](#10_9781119183938-ch07.xhtml#rc07-fig-0017) A simple wireless network](./media/images/9781119183938-fg0717.png)

All nodes connecting through the access point become part of a single collision domain, because there is no physical mechanism to support Ethernet switching through a wireless access point. Furthermore, like an early 10BASE5 or 10BASE2 network, Wi-Fi networks are half-duplex, with data travelling in only one direction at a time.

Access points have a number of jobs in a typical Wi-Fi network:

- **Broadcasting its presence:** There is a type of 802.11 management frame called a _beacon frame_, which is broadcast periodically to let stations know that the network is there under a particular name and is available for connection. - **Station authentication and encryption:** This happens through Wi-Fi security protocols like EAP, WEP, WPA and WPA-2. Although it is possible to authenticate a station without encrypting subsequent traffic, authentication and encryption are generally handled together. The exceptions are some public hotspots in restaurants and coffee shops, which simply leave the AP open for everyone. This is a security risk, as others in the location can monitor network traffic using “sniffing” utilities. - **Forwarding frames between stations:** All frames travelling between stations associated to an access point travel via the access point, even if the two stations are within range of each other. The access point receives the frame from the sender, and repeats it to the receiver. - **Bridging to the wired portions of the network:** Because an AP connects a hubbed subnetwork to a switched network, it must perform the function of a network bridge. - **Media access control (MAC):** The access point may provide centralised control of media access, explicitly notifying stations when they are free to transmit. Few products implement this _point coordination function_ (PCF), relying instead on a distributed approach, described in the section “[Wi-Fi Distributed Media Access](#10_9781119183938-ch07.xhtml#c07-sec-0029).”

Early in the Wi-Fi era, wireless access points were separate units, intended to be added onto an existing wired network with its own router/switch appliance. Since the mid-2000s, the router/switch and wireless access point have generally been combined into a single appliance that incorporates a network router governing an Internet connection, a wired Ethernet switch with several Category 5 connectors and a wireless access point. This combination appliance is called a _wireless router_. In early days wireless access points and wireless routers had external, “steerable” antennas. Today most wireless devices (whether wireless routers or mobile clients) have antennas hidden inside the device case.

### Infrastructure Networks vs. Ad Hoc Networks

The technical term for the sort of network shown in [Figure 7-14](#10_9781119183938-ch07.xhtml#c07-fig-0014) is _infrastructure network_. The term “infrastructure” is used because such a network is planned and constructed in a particular way, like a highway system.The access point and the stations associated with it form a _Basic Service Set_ (BSS); this is given a distinctive human-readable name, called the _Service Set_ _Identifier_ (SSID), which wireless stations use to locate and connect to the infrastructure network when they come online. Stations may connect to or disconnect from the AP, but the overall shape of the network does not change. In modern infrastructure networks, there is almost always a router associated with one or more access points, providing a connection to larger wired networks or the global Internet.

From its inception, the 802.11 standard defined another, very different sort of network: the _ad hoc wireless network_. In an ad hoc network, wireless stations connect to one another, without the intermediation of an access point, forming an _Independent Basic Service Set_ (IBSS). This requires that the stations place their Wi-Fi client adapters in ad hoc mode instead of infrastructure mode. “Ad hoc” here indicates that the network is unplanned and assembled when necessary, but it vanishes when the stations disconnect. (Think of a network of laptops convened on a conference table to share documents pertinent to a meeting.) Any station in an ad hoc network may communicate with any other station in the network, just as would be possible in an infrastructure network, but in this case frames travel directly from the sender to the receiver rather than via an access point. See Figure 7-18.

![[FIGURE 7-18:](#10_9781119183938-ch07.xhtml#rc07-fig-0018) An ad hoc wireless network](./media/images/9781119183938-fg0718.png)

Ad hoc networks have some advantages over infrastructure networks. For short-lived networks, they avoid the cost and effort of providing an access point. Also, peak throughput between two stations in a quiet network is doubled, as each frame is transmitted once (from sender to receiver) rather than twice (from sender to access point and from access point to receiver). However, they also suffer from some significant disadvantages:

- All 802.11 wireless networks require that each station maintains an accurate current time, which is used for power management (allowing a station to “go to sleep” for a period when idle) and MAC. In an infrastructure network, each beacon frame transmitted by the access point contains a time, which the other stations in its BSS use to synchronise their clocks. In an ad hoc network this timing synchronisation function (TSF) must instead be implemented in a distributed fashion. Each station periodically attempts to transmit a beacon frame containing its current time; on receiving a beacon frame, a station updates its current time if the time indicated in the beacon frame is later than the current time. This scheme has been shown to scale poorly with the number of stations in the network; beyond a certain limit, contention causes beacons to be lost, and some stations (generally those with the fastest clocks) may become desynchronised from the rest of the network.
- As stations in an ad hoc network must be within radio range of one another to communicate, the maximum separation between stations is roughly half that of an infrastructure network, where a well-placed access point can relay frames between stations on opposite sides of its coverage area. Furthermore, it is often possible to site an access point in an elevated position with good sight lines, extending its footprint.

  Not all operating systems support ad hoc mode adequately, or at all, even though the client adapter hardware may be fully Wi-Fi compliant.

### Wi-Fi Distributed Media Access

As noted earlier, very few products implement the centralised PCF scheme for MAC. In the absence of PCF, stations are still able to regulate their access to the medium using the _distributed coordination function_ (DCF). While the DCF has some similarities to the CSMA/CD approach used in wired Ethernet, a key difference is that it is not generally feasible for a station to sense the wireless medium while transmitting, as the relatively strong local transmitted signal will tend to drown out the relatively weak signals from other stations. This precludes conventional collision detection.

In the absence of reliable collision detection, Wi-Fi networks instead employ _Carrier Sense Multiple Access/Collision Avoidance_ (CSMA/CA), which aims to _avoid_ packet collisions rather than detect them, which works this way: as in CSMA/CD, a station first listens to the channel to detect the signal of another station transmitting. This is called _physical carrier sensing_, because it involves the station actually detecting a signal on the medium. If such a signal is heard, the station wishing to transmit waits for a calculated period of time before listening again. It listens until the channel is clear, and then transmits the packet in its entirety. There is no “after the fact” collision detection, and no jam signal. (A jam signal is impossible because the Wi-Fi radio medium is half-duplex, and stations cannot listen for a jam signal while they are transmitting.)

There are a number of subtleties to the implementation of DCF:

- Rather than transmitting immediately when the medium becomes idle, a station must first wait for a fixed period known as the _distributed inter-frame space_ (DIFS); if the medium remains idle during the DIFS, the station then waits for a further random backoff period before transmitting. The DIFS allows higher-priority traffic, such as PCF frames or acknowledgment frames, preferential access to the medium. As in the case of wired Ethernet, the backoff period reduces the likelihood of two stations that are waiting for the medium to begin transmitting simultaneously, resulting in a collision.
- The backoff period is chosen randomly to lie within a contention window. Too small a window increases the likelihood that two stations will choose the same backoff value; too large a window degrades efficiency as the medium tends to spend more time idle. The solution is to use a dynamic window, which varies based on how much contention is encountered. Initially a station’s window is set to a fixed minimum value; each unsuccessful transmission doubles the size of the window, up to a fixed maximum value, whereas a successful transmission resets it to the minimum value.
- Because dropped frames are far more frequent in wireless networks than wired ones, 802.11 implements a MAC-level acknowledgement and retransmission protocol. When a station successfully receives a frame, it waits for the _short inter-frame space_ (SIFS, where SIFS is less than DIFS, ensuring priority) and then sends an acknowledgement (ACK) frame. If a transmitting station fails to receive an ACK, it can conclude that a collision, or other interfering event, has occurred, and should then retransmit the frame.

  Physically sensing the medium consumes power. To mitigate this, 802.11 implements a virtual carrier sensing mechanism: each frame contains a duration field, which allows the transmitter to indicate how long it (and any associated ACK frame) will occupy the medium. When a station receives a packet, even a packet intended for another station, it copies the duration field into a local timer known as the _network allocation vector_ (NAV), and postpones any transmissions until the timer has expired. Stations generally put their radio hardware into a low-power state while waiting.

### Carrier Sense and the Hidden Node Problem

Both infrastructure and ad hoc Wi-Fi networks have a similar problem: unless every station participating in the network can “hear” every other station, problems arise. The most important of these is the breakdown of the physical and virtual carrier sensing mechanisms described earlier in the section “[Wi-Fi Distributed Media Access](#10_9781119183938-ch07.xhtml#c07-sec-0029).” Such a breakdown leads to an increase in the packet collision rate. See Figure 7-19.

![[FIGURE 7-19:](#10_9781119183938-ch07.xhtml#rc07-fig-0019) The hidden node problem](./media/images/9781119183938-fg0719.png)

In [Figure 7-19](#10_9781119183938-ch07.xhtml#c07-fig-0019), wireless stations Ted and Alice are both connected to Arda, an AP physically about halfway between them. Ted and Alice are far enough apart so that their radios do not have the range to detect one another’s signals. Physical distance has “hidden” Ted from Alice and vice versa. This is called the _hidden node problem_. When two Wi-Fi nodes are hidden from one another, they cannot avoid transmitting colliding packets because they cannot monitor the channel for one another’s transmissions.

To address the hidden node problem, the 802.11 standard defines a virtual carrier sensing mechanism called _request to send/clear to send_ (RTS/CTS). Instead of simply listening for a clear channel, a transmitting station first performs a _handshake_ with the intended receiving station by sending it an RTS frame and waiting for a CTS frame in response. Only after this handshake is complete is data transmitted. This greatly mitigates the hidden node problem: in the example, although Alice cannot receive Ted’s RTS frame, she does receive Arda’s CTS response and is able to update her NAV value, which in turn causes her to postpone any future transmissions until Ted has finished.

Here’s how it works, broken out into steps (see Figure 7-20):

1. When a station wants to send a packet, it first checks to see that the channel is quiet and waits for the DIFS before sending out an RTS frame. The duration field of the RTS frame is set to the total time required to complete the CTS, data transmission and ACK. 2. All stations that hear the RTS frame copy its duration field to their NAV timers. 3. Assuming that the destination station hears the RTS, it waits for the SIFS and replies with a CTS frame. The duration field of the CTS frame is set to the total time required to complete the data transmission and ACK (a slightly smaller value than in the RTS frame). 4. Some stations may not have heard the original RTS frame due to the hidden-node problem. If those stations hear the CTS frame, they copy its duration field into their NAV timers. 5. After the transmitting station receives CTS, it waits for a SIFS interval and begins sending the data frame proper. 6. When the receiving station has successfully received the data packet, it waits for another SIFS interval and sends an ACK frame back to the sending station. 7. By the time the ACK frame has been sent, all the NAV timers associated with the transaction will have timed out. All stations then wait for a DIFS interval before checking the channel for idleness and beginning the process again.

![[FIGURE 7-20:](#10_9781119183938-ch07.xhtml#rc07-fig-0020) How the DCF coordinates a data packet transfer](./media/images/9781119183938-fg0720.png)

Of course, it’s still possible for two stations that are hidden from each other to send overlapping RTS frames, which will then collide and be dropped. The key benefit of the RTS/CTS protocol is that it reduces the period of vulnerability during which a hidden-node collision can occur from the comparatively long data transmission time to the comparatively short time required to transmit the RTS frame. Because the RTS/CTS handshake introduces a substantial overhead, and because its benefits are most pronounced for longer frames, it is common to apply a size threshold below which frames are transmitted without handshaking.Handshaking is often disabled completely for small networks, especially those with stations at fixed positions.

### Fragmentation

Because longer frames have a proportionally higher chance of encountering interference and collisions than shorter frames, Wi-Fi networks provide a configurable option called a _fragmentation threshold_, which specifies the maximum size of frame that may be transmitted in one piece. A frame that is larger than the fragmentation threshold is broken into a numbered series of fragments, which are individually acknowledged and may be individually retransmitted if the acknowledgment does not arrive.

Fragments are transmitted on the medium separated by the SIFS, and so will not be interrupted by other DCF-coordinated traffic. The duration field of each transmitted fragmentspecifies the time required to transmit all the remaining fragments, rather than just the current one. When used with RTS/CTS handshaking, the duration field of the RTS frame specifies the total time required to transmit the CTS and all the fragments, so the medium is reserved for the entire duration of the fragmented transmission.

### Amplitude Modulation, Phase Modulation and QAM

Before we discuss the operation of the various 802.11 physical layers, it is helpful to review a few basic radio concepts.

All radio technologies transmit information over the air by changing or _modulating_ one or more properties of a carrier wave in response to that information. Figure 7-21 illustrates two analogue modulation schemes that you have doubtless encountered in everyday life: amplitude modulation (AM) and frequency modulation (FM). The former holds the frequency of the carrier constant and varies its amplitude; the latter holds the amplitude constant and varies the carrier’s frequency about a central value.

![[FIGURE 7-21:](#10_9781119183938-ch07.xhtml#rc07-fig-0021) AM and FM analogue modulation](./media/images/9781119183938-fg0721.png)

In contrast with analogue modulation, whose goal is to encode a continuous and continually varying signal, digital modulation schemes transmit a discrete series of symbols (such as, in the simplest case, bits). In the rest of this discussion we are concerned with digital rather than analogue modulation.

Figure 7-22 illustrates four digital modulation schemes for transmitting binary data. The first two are digital equivalents of our familiar analogue schemes: _binary amplitude-shift keying_ (BASK)—sometimes also referred to as on-off keying (OOK)—transmits a binary 0 by emitting nothing and a binary 1 by emitting the carrier wave; _binary frequency-shift keying_ (BFSK) transmits 0s and 1s by changing the carrier frequency between two defined values. _Binary phase-shift keying_ (BPSK) transmits a binary 0 by emitting the carrier wave and a binary 1 by emitting the carrier wave phase-shifted by 180° (that is, inverted). In practice, _differential BPSK_ (DBPSK) is often used in place of BPSK; this eliminates the requirement for a fixed phase reference, encoding a binary 0 by continuing to transmit the carrier with its current phase and a binary 1 by shifting the current phase by 180°.

![[FIGURE 7-22:](#10_9781119183938-ch07.xhtml#rc07-fig-0022) BASK (OOK), BFSK, BPSK and DBPSK modulation](./media/images/9781119183938-fg0722.png)

The extension from binary to _m_-ary symbols (symbols that can take _m_ values) is straightforward. For mASK we permit _m_ possible amplitudes for the carrier, rather than simply on and off; for mFSK we permit _m_ possible frequencies; and for mPSK we allow phase shifts that are finer than just 180". If we double _m_ from two to four we can transmit twice as much data over the same channel, as each symbol can represent two bits; doubling _m_ again gives a further 50% increase in capacity, as each symbol can now represent three bits. Ultimately our ability to keep increasing _m_ is limited by noise, which makes it hard for the receiver to accurately discriminate between increasingly finely spaced amplitude levels or frequency or phase shifts. This is in accordance with the Shannon-Hartley theorem, which informally states that the information-carrying capacity of a channel decreases with the signal-to-noise ratio of the channel.

We can, of course, choose to combine amplitude and phase modulation or keying. The resulting modulation scheme is referred to as _quadrature amplitude modulation_ (QAM), because modulating amplitude and phase together is equivalent to modulating the amplitude of two carriers that are 90° degrees out of phase (in quadrature) with each other, and summing the result. A digital QAM scheme is characterised by the set of discrete (phase, amplitude) values used. These are often represented as a _constellation_ (that is, a specific arrangement of values) in the complex plane, as shown in Figure 7-23. In the figure, distance from the origin corresponds to the amplitude, and angular position corresponds to the phase shift. In 16QAM, there are 16 different possible combinations of amplitude and angular position, allowing 16 bits to be encoded by a single pair of phase and amplitude values. QAM systems have to be carefully designed to keep noise immunity high. An engineer will generally attempt to maximise the Euclidean (straight line) distance between any two points in the constellation, maximising the chance that a receiver will be able to identify the intended point in the presence of noise.

![[FIGURE 7-23:](#10_9781119183938-ch07.xhtml#rc07-fig-0023) Example constellations](./media/images/9781119183938-fg0723.png)

### Spread-Spectrum Techniques

The 2.4GHz and 5GHz ISM frequency bands used by Wi-Fi represent a particularly challenging environment in which to transmit data. Standards intended for use in these bands must offer some degree of resilience in the face of interference from a variety of sources:

- Other communication technologies that use the band (such as Bluetooth and ZigBee) - Non-communication devices such as microwave ovens - Clients attached to other Wi-Fi networks with overlapping channel assignments, which do not participate in this network’s collision-avoidance regime - Time-delayed reflections of signals (multipath interference; refer to [Figure 7-15](#10_9781119183938-ch07.xhtml#c07-fig-0015))

Transmitters in the ISM bands are also subject to regulatory limits on the total amount of power that they may radiate in a given frequency window (spectral power density).

The Wi-Fi family of standards use a variety of _spread-spectrum_ techniques to address these challenges. As the name suggests, these spread a signal across a wider bandwidth than would otherwise be the case, offering improved resilience to interference (and in particular to narrowband interference that occupies only a small part of the frequency band) and reduced spectral power density. Three distinct techniques have been used:

- **Frequency-hopping spread spectrum (FHSS):** This approach is used only by the original 802.11 standard, at data rates of 1 to 2 Mbps. It uses 2- or 4-level FSK, and “hops” the frequency of the carrier wave to another point in the channel every 400ms, in a sequence that is known to both transmitter and receiver. - **Direct-sequence spread spectrum (DSSS):** This combines the stream of data bits with a faster stream of _chips_. In the case of 802.11b operating at 1 or 2 Mbps, a repeating 11-chip Barker code is combined with each bit. The closely related complementary-code keying scheme is used at higher data rates. - **Orthogonal frequency-division multiplexing (OFDM):** Data is split into many streams, each of which is modulated at a comparatively low rate onto one of many subcarriers spaced across the band. Since 802.11g, all standards have used OFDM, relying on wider bands, denser modulations and spatial diversity (which is described in more detail later in the next section) to deliver higher data rates in low-noise environments.

### Wi-Fi Modulation and Coding in Detail

It’s time to take a look at the DSSS and OFDM modulation schemes used by 802.11b and 802.11g in more detail. Understanding the modulation schemes thoroughly is not necessary to use Wi-Fi, but it is necessary to comprehend the challenges of wireless networking, as compared to conventional wired Ethernet.

At a data rate of 1 Mbps, the incoming bits are multiplied by a spreading sequence (in this case the 11-digit Barker code) running at a chipping rate of 11 Mbps; each bit in the source stream now corresponds to 11 bits in the spread stream. The spread stream is used to DBPSK-modulate a carrier wave. To achieve a doubling of throughput to 2 Mbps, DQPSK modulation replaces DBPSK. Figure 7-24 shows these two configurations. The 11-digit Barker code:

```
    +1 -1 +1 +1 -1 +1 +1 +1 -1 -1 -1
```

is used as the spreading sequence. It has extremely low autocorrelation: if you multiply the sequence by a shifted version of itself and sum the products, then for any shift that is not a multiple of 11 you get a maximum sum of between -1 and +1, whereas for a shift that is a multiple of 11, the products clearly sum to +11. For a shift of two, the products would be the following, which yields a sum of -1:

![[FIGURE 7-24:](#10_9781119183938-ch07.xhtml#rc07-fig-0024) Spread-spectrum transmission at 1 Mbps and 2 Mbps using 11-digit Barker code](./media/images/9781119183938-fg0724.png)

```
    <figure>
      ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----   +1   -1   +1   +1   -1   +1   +1   +1   -1   -1   -1   ×    ×    ×    ×    ×    ×    ×    ×    ×    ×    ×   +1   +1   -1   +1   +1   +1   -1   -1   -1   +1   -1   =    =    =    =    =    =    =    =    =    =    =   +1   -1   -1   +1   -1   +1   -1   -1   +1   -1   +1   ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----
    </figure>
```

The receiver demodulates the incoming signal, and multiplies the resulting spread stream by the spreading sequence to recover the original data, as shown in Figure 7-25; before doing so, it must synchronise its spreading sequence with that of the transmitter, a task that is simplified by the Barker code’s low autocorrelation. Multiplying by the spreading sequence in the receiver suppresses both inter-symbol interference due to multipath effects (because of the Barker code’s low autocorrelation) and other noise (because it broadens the noise spectrum, allowing it to be rejected by the integrating action of the receiver).

![[FIGURE 7-25:](#10_9781119183938-ch07.xhtml#rc07-fig-0025) The Direct-Sequence Spread-Spectrum (DSSS) process](./media/images/9781119183938-fg0725.png)

To achieve data rates of 5.5 Mbps and 11 Mbps with the same channel bandwidth, we require an approach with greater spectral efficiency. Complementary codes are sets of codes that, like the Barker code, have low autocorrelation (between a code and a shifted version of itself, as before) and cross-correlation (between codes in the set). Unlike the Barker code, however, the codes used here are polyphase codes: rather than being real numbers drawn from the set {-1, 1}, code values are complex numbers drawn from the set {-1, 1, -j, j}. When used as spreading sequences, they have the same advantages as the Barker code in terms of synchronisation and interference rejection, but as there is more than one code in the set, we are now able to convey additional information through our choice of the code used to spread a given symbol.

To transmit at 11 Mbps, we group the incoming bits into 8-bit bytes. Six bits are used to select one of 64 8-bit complementary codes, and the remaining two bits are used to phase-modulate the entire code. The chipping rate remains 11 Mbps, and as eight bits are transmitted for every eight chips, the data throughput is also 11 Mbps. When transmitting at 5.5 Mbps, the number of codes is reduced to four.

The 802.11g standard supports data rates of up to 54 Mbps on the 2.4GHz band. To achieve this, it adopts an OFDM modulation scheme first used (on the 5GHz band) by 802.11a. Each 20MHz channel is divided into 52 subchannels; four channels are reserved for pilot signals, and data is modulated onto the remaining 48 subcarriers using 64-QAM (for 52 Mbps and 48 Mbps modes), 16-QAM (for 36 Mbps and 24 Mbps), QPSK (for 18 Mbps and 12 Mbps) or BPSK (for 9 Mbps and 6 Mbps). A symbol is transmitted every 4μs, so the raw throughput in 64QAM mode is given by

> 48 channels × 250,000 symbols/s × 6 bits/symbol = 72 Mbps

The difference between the raw throughput of 72 Mbps and the actual throughput of 54 Mbps is accounted for by the use of a FEC code with a code rate of 3/4 (that is, four bits are transmitted for every three bits of data). Each 802.11g data rate uses forward error correction, with code rates of 1/2 (for 24 Mbps, 12 Mbps and 6 Mbps), 2/3 (for 48 Mbps) or 3/4 (for 54 Mbps, 36 Mbps, 18 Mbps and 9 Mbps).

The OFDM scheme provides resilience both to narrowband interference and to frequency-selective fading; the FEC code allows the receiver to reconstruct a certain amount of missing data from one or more corrupted subcarriers. The relatively slow modulation rate permits the insertion of a guard interval between each symbol, reducing inter-symbol interference.

### How Wi-Fi Connections Happen

Connecting a Wi-Fi device to a wireless access point is not as simple as it might seem at first. There may be multiple APs and multiple client adapters visible in the same physical location. APs and clients may be scattered across several channels. Not all clients may be authorised to connect to certain APs. At the highest level, resolving such issues is a three-step process:

1. Client adapters need to determine which APs are available on what channels. This process is called _scanning_. 2. The APs need to be able to determine which clients are theirs, and vice versa. This process is called _authentication_. 3. After authentication, the authenticated client may connect to the AP that authenticated it. This process is called _association_.

Scanning may be active or passive. In _passive scanning_, an AP is configured to periodically broadcast a frame containing its SSID. Client adapters listen for these broadcast frames across all channels and build a list. If they have connected to an AP before, they will choose that AP. If they don’t see an SSID that they’ve connected to before, they will attempt to connect to the AP with the strongest signal. How the connection happens and how the user gets involved are implementation dependent. Most modern Wi-Fi software has a _Connect Automatically?_ dialog that appears on first connection and requires user confirmation before automatic connections can happen in the future. The user of the client adapter’s computer may also be given a chance to choose an AP from the list that the client gathers from broadcast SSID frames.

In _active scanning_, a client adapter sends out a _probe request frame_ to all APs within range. The probe request frame may contain the SSID of a preferred AP, in essence asking, “Are you there, `blackwave`?” If the `blackwave` AP is out there, it issues a probe response to the client. The probe request frame may alternatively contain a null (empty) SSID field, which amounts to asking, “Who’s out there?” In that case, any AP within range may send a probe response back to the client, which in most cases will choose the AP with the strongest signal.

Active scanning with a null SSID is done because in some wireless networks, the APs are configured _not_ to broadcast their SSIDs. An active scan is thus the only way for a client to determine what APs are within range. In most home networks and “coffee shop” Wi-Fi providers, the APs broadcast their SSIDs, and passive scanning is sufficient.

After a client adapter identifies the AP that it wants to connect to, the authentication process determines whether the connection is authorised. There are two types of authentication: open and shared-key. _Open authentication_ does not depend on passwords. The client sends an authentication request frame to the AP. This frame includes the client’s MAC address. An AP may be configured to exclude certain client MAC addresses, or only permit certain client MAC addresses. Depending on how the AP is configured, it either grants or refuses the client’s authentication request. If it refuses, the conversation is over, and the connection does not happen. If the AP grants the request, the process moves on to association.

Authentication by MAC address is done less and less often, because clients transmit their MAC addresses as _cleartext_ (that is, without encryption) and an attacker can compile a list of valid MAC addresses with software that simply monitors the channel. Because many client adapters allow users to change their MAC addresses to arbitrary values, the attacker could then “spoof” a legitimate MAC address and connect to the network.

_Shared-key authentication_ uses one of several protocols that involve encryption. The most common protocol for small networks today is called WPA-2, which has been mandatory on new-build Wi-Fi gear since 2006. (WPA-2 is covered in more detail in the next section.) Large corporate networks and those with strong security requirements use a separate authentication server (often one called RADIUS) that implements an IEEE authentication standard called 802.1X. Small networks handle shared-key authentication directly between AP and client. A conversation occurs between AP and client, in which the AP and client require one another to complete a cryptographic challenge. If both AP and client possess the same shared key, the challenge can be completed successfully and authentication is granted. Thereafter, all communication between AP and client is encrypted.

The final connection step is association. After the AP and client adapter have authenticated one another, the client sends the AP an _association request frame_. If granted, the association process goes to completion, after which the client may obtain network configuration parameters and an IP address through the network’s DHCP server. The AP may still refuse association for other reasons; for example, if the number of clients associated with it has already reached a pre-set maximum value. In most cases, however, the association request is granted, and the client connects.

### Wi-Fi Security

There is some inherent security in wired networks: without physical access to a network jack or network equipment, connecting to the network is impossible. Wi-Fi signals can pass through walls and do not limit connections to physical jacks, so security becomes a matter of great importance. The original 802.11 standard specified a simple encryption mechanism called _wired equivalent privacy_ (WEP). A WEP key is a string of hexadecimal digits and not a conventional password. Some Wi-Fi hardware incorporated key generators to convert a human-readable password or passphrase to a hexadecimal WEP key.

In 2001, security researchers found a flaw in WEP’s encryption algorithm that allowed a wireless access point protected by WEP to be cracked after as little as 10 minutes of examining encrypted packets passing over the network. Once the nature of the flaw became generally known, WEP became useless. In 2004, the 802.11 committee ratified addendum 802.11i, which became known as _Wi-Fi Protected Access version 2_, or WPA-2. WPA-2 replaced a short-lived interim solution called WPA, which was not as strong; like WEP, it’s no longer used. WPA-2 uses a 256-bit encryption protocol called the _Advanced Encryption Standard_ (AES). AES is a _block cipher_ that encrypts and decrypts data a block at a time. Older Wi-Fi protocols such as WEP and WPA used _stream ciphers_, which deal with single characters at a time and are much more vulnerable to attack.

WPA-2 allows for ASCII keyphrases up to 63 characters in length, and if the keyphrase consists of random characters, 20 to 30 characters is generally sufficient for home networks.

> [!NOTE]
> that attackers do not simply transmit passwords to a wireless router one after the other until they find one that works. Instead, a utility called a _packet sniffer_ captures encrypted packets off the air and saves them to disk as files. Then, an _offline brute-force attack_ can be attempted. In this type of attack, dictionaries of ordinary words and commonly used passwords are tried against the encrypted packets stored on an attacker’s computer, using a fast application that can attempt tens of thousands of passwords per second. If the attacker is willing to let the software keep trying for weeks or months, a weak password or a concatenation of common dictionary words could be vulnerable. There is some comfort in the “low-hanging fruit” effect: because some people use short or otherwise weak passwords, attackers are less likely to spend months of time on a brute-force attack against a strong password. That assumes that you are not a corporate or military site storing important information. Few attackers will waste that much time breaking your password just to steal your MP3s.

Not all parts of WPA-2 are as secure as the main encryption algorithm. In 2011, a critical flaw was discovered in a WPA-2 accessory technology called Wi-Fi Protected Setup (WPS) that runs in the firmware of wireless routers and allows easy password distribution for small networks. It was found that the WPS protocol “leaks” portions of a PIN code, and allows a brute-force attack to succeed in as little as two hours. WPS is now considered compromised and security professionals recommend that it be disabled in devices that include it.

On the client side, WPA-2 is implemented as a piece of software called a _supplicant_, which runs on the computer that wants to connect to the network and not in the client adapter itself. For Linux distributions (including Raspbian) the supplicant software is called `wpa_supplicant`, and its configuration file `wpa_supplicant.conf` is located in the folder `/etc/wpa_supplicant`. The supplicant “asks” its chosen AP for authentication, and then engages in the WPA-2 protocol with the AP. Some supplicant implementations include a graphical user interface (GUI) for management, whereas others are command-line based and read keyphrases and other information from an editable configuration file.

### Wi-Fi on the Raspberry Pi

Most models of the Raspberry Pi have a wired Ethernet port that is standard and will work without any tweaking on Linux distributions like Raspbian. (The older Model A boards and the Raspberry Pi Zero do not have Ethernet ports.) If you connect your Raspberry Pi board via cable to your router’s Ethernet port with a running DHCP server, Raspbian requests DHCP configuration, which includes a local IP address. After DHCP has configured Raspbian’s networking parameters, the board should be able to communicate with other nodes on your local network as well as the Internet at large, using its IP address.

Raspbian (and most Unix-derived operating systems) includes a command-line utility called `ifconfig`, which allows you to display the configuration of your wired Ethernet port. (There is a better configuration utility for Wi-Fi, which we’ll get to shortly.) Simply open a terminal window and execute this command:

```
ifconfig eth0
```

Here, `eth0` is the default name of the Raspberry Pi’s wired Ethernet port. The utility displays the current status of the port, including its MAC address and IP address. If you’re not using the wired Ethernet port on your Raspberry Pi, it’s a good idea to disable it, especially if you intend to use a Wi-Fi adapter. You disable `eth0` with `ifconfig`:

```
sudo ifconfig eth0 down
```

> [!NOTE]
> that changing parameters (as distinct from merely displaying them) requires the use of admin privileges, via `sudo`. To enable the port again, enter this command:

```
sudo ifconfig eth0 up
```

Unless you have a Raspberry Pi 3 (which has both Wi-Fi and Bluetooth right on the circuit board), you’ll have to obtain a USB Wi-Fi client adapter. Make sure that your board is running the latest image of Raspbian, which includes most available Wi-Fi drivers and tools. There are extremely compact Wi-Fi client adapters that can be plugged into one of the two on-board USB ports or into a powered USB hub. You can find a list of other tested and known-compatible Wi-Fi client units at [`http://elinux.org/RPi_USB_Wi-Fi_Adapters`](http://elinux.org/RPi_USB_Wi-Fi_Adapters).

Make sure that your board is running from a robust power source like a powered hub. No matter how compact, a Wi-Fi adapter includes a microwave radio transmitter, and it needs a certain amount of current to do its job. Adding such an adapter to a board that’s close to overloading its power supply is almost guaranteed to make it fail. When choosing a power supply for a Raspberry Pi installation, always err on the side of more current rather than less. Most of the common problems getting a Raspberry Pi system to work stem from inadequate current from the power supply.

The WPA-2 supplicant that comes preinstalled with Raspbian has a GUI, and if Raspbian has a driver for your client adapter, connecting to your access point can be done entirely using the GUI. Follow these steps:

1. Run the supplicant software by launching Wi-Fi Config from the Raspbian desktop. The `wpa_gui` main window is shown in Figure 7-26.
2. Click the Scan button. The supplicant scans for available APs and displays a list in a new window. See Figure 7-27.
3. Assuming that one of the listed APs is your own, double-click its line in the `Scan` window. The `NetworkConfig` window opens (see Figure 7-28). If you don’t see your AP listed, your Raspberry Pi may be too far away, or there may be some configuration conflict.
4. Enter your AP’s shared key in the PSK field.
5. Click Add. Assuming you entered the shared key correctly, the supplicant connects to your AP. At that point, the Status tab of the `wpa_gui` main window shows `Completed (Station)` in the Status field.
6. Test your new connection by launching Midori and accessing any web page. You can use the `wpa_gui` application for ongoing configuration as well, say if you install a new wireless access point or change your SSID or shared key. For simple status display, another Linux command-line utility provides more information. After you have your Wi-Fi connection established and configured, open a terminal window and enter the following command:

   `iwconfig ![[FIGURE 7-26:](#10_9781119183938-ch07.xhtml#rc07-fig-0026) The ` wpa_gui` main window](./media/images/9781119183938-fg0726.png)

![[FIGURE 7-27:](#10_9781119183938-ch07.xhtml#rc07-fig-0027) The `Scan` window](./media/images/9781119183938-fg0727.png)

![[FIGURE 7-28:](#10_9781119183938-ch07.xhtml#rc07-fig-0028) The `Network Config` window](./media/images/9781119183938-fg0728.png)

The utility displays an eight-line text summary including the AP’s SSID, the wireless technology (a/b/g/n), the AP’s MAC address, the current bit rate, indicators for signal level, link quality and cumulative counts for various errors.

### Even More Networking

Networking is a field that is both broad and deep, and there’s a great deal to learn beyond what we can show in one chapter. Here are some useful topics for independent research:

- **Samba:** A software package that allows Linux operating systems like Raspbian to transfer files with Windows or other non-Linux operating systems. Samba is free and may be installed without charge from the Raspberry Pi repositories. - **Ethernet bridges:** Special Ethernet appliances that forward Ethernet frames from one physical medium to another. This is often from Category 5 cabling to Wi-Fi or vice versa, but there are bridges that can implement Ethernet over residential power wiring, and allow a Category 5 connection on both ends. (As a category, this is called _Powerline Networking_, and it’s often used for bringing a network connection to locations in a building where Wi-Fi cannot reach.) With special software, a Raspberry Pi board can be configured to bridge between wired Ethernet and Wi-Fi. - **Power over Ethernet (PoE):** A technology that uses special adapters to send a modest amount of current through unused twisted pairs in a Category 5 Ethernet cable, or over the signalling conductors if there are no unused pairs available. Because the POE voltage is the same on both pairs of the twisted pair carrying data, the NICs ignore the voltage, which does not interfere with data. Implemented correctly, PoE can allow an Ethernet bridge or even an entire Raspberry Pi computer to be located on a mast or some other location where conventional power isn’t available.

There are also a great many devices like cameras and sensors that can be attached to a computer network through a Category 5 Ethernet cable. More and more everyday household devices are joining the “Internet of Things” and may be controlled via Wi-Fi from computers of all sorts. Learning Ethernet and TCP/IP thoroughly will allow you to extend your reach anywhere that Ethernet cables—or Wi-Fi microwaves—can reach.
