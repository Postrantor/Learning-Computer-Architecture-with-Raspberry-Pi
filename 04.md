Chapter 3

# Electronic Memory

**COMPUTING AS WE** know it today is a wild dance between the central processing unit (CPU) and memory. Instructions in memory are fetched, and the CPU executes them. In executing instructions, the CPU reads data from memory, changes it and then writes it back. Data and instructions that are used a lot are pulled in closer, via cache. Data and instructions that aren’t needed for the time being are swapped out of virtual memory onto disk.

To understand this dance you need an understanding of both the CPU and memory. Which, then, to study first? In most cases, the CPU is considered the star of the show and always begins the parade. That’s a mistake. There are multitudes of CPU designs out there, all of them different and all stuffed to the gills with tricks to make their own parts of the dance move more quickly. Memory, on the other hand, is a simpler and less diverse technology. Its moves in the dance are fairly simple: store data from the CPU and hand it back when requested, as quickly as possible. To a great extent, memory dictates the speed at which the dance proceeds. The designs of our CPUs are heavily influenced by the speed limitations of system memory.

That being the case, it makes sense to study memory first. If you understand memory technology thoroughly, you’re halfway to understanding anything else in a modern computer system.

## There Was Memory Before There Were Computers

For a long time, computers were really special-purpose haywire calculators. What passed for programs were lashed up by hand with switches and jumper wires representing 1s and 0s. Then John von Neumann and others proposed that programs be stored as digital patterns on the machine, right in with the data that the programs were written to process. The first generation of these stored-program computers used single-bit storage circuits (colloquially called _flip-flops_) constructed from vacuum tubes to store programs and data. Imagine for a moment storing a 1 or a 0 in something the size of your fist! Apart from being enormous, hot and power-hungry, vacuum tube data storage was _volatile_—when the computer was powered down, the electronic states of the vacuum tubes vanished as the tubes went dark.

To keep programs and data permanently, vacuum-tube data was written to strips of paper tape or cardboard Hollerith punch cards. (Hollerith cards were used in mechanical tabulation of census data. They predate digital computers by 50 years.) The machines to read tape or cards into a computer were electromechanical and very slow. Sending intermediate results out to electromechanical paper storage was even slower and wasted most of the speed that electronic computing offered. A better way to record code on data than punching holes in pulped trees was desperately needed.

## Rotating Magnetic Memory

In those early, crazy days of computing, many things were tried. Mercury-based delay-line memory units stored bits as mechanical pulses—sound waves, basically—travelling through linear columns of mercury in sealed tubes. Like modern dynamic computer memory, delay-line memory had to be refreshed every time a bit (encoded as a pulse) arrived at the far end of the tube. Strings of pulses representing code and data marched endlessly through the mercury, read and written by quartz piezoelectric crystals as needed. Mercury memory systems were huge, hot, heavy and full of toxic heavy metal. They were also very touchy to adjust and keep in operation.

Another early memory storage scheme encoded bits as dots of light on the surface of a cathode-ray tube (CRT) with long-persistence phosphor, much like the tubes used in early radar displays. The dots, once written, would linger in the phosphor for a few seconds and could be read by a plate placed against the face of the tube. As with delay-line memory, CRT memory had to be refreshed periodically. Nonetheless, each of the tubes could store 1,024 bits in a fraction of the space required by delay-line storage. Known as Williams tubes, these were used as memory in the famous IBM 701 commercial computers, introduced in 1952. They were the first widely used _random-access memory_ (RAM)—so-called because bits could be accessed at any time from anywhere on the face of the tube. The term _RAM_ is still used today, even though we’ve mostly forgotten that there was ever any other kind of computer memory. The preferred term is read/write memory, but terms like _RAM_, _SRAM_, _DRAM_ and _SDRAM_ are so universally used that we use them in this book.

Both of these memory technologies, like vacuum-tube memory, were volatile. A memory technology that would retain its data even when the computer was powered down would make many things easier, and new things would be possible. Encoding information as tiny regions of magnetic alignment on a moving magnetic surface dates back to the early 1930s. The Germans invented magnetic sound recording, which wrote sound waveforms to spools of plastic tape coated with iron-oxide powder. By 1950, this technology had been adapted to store digital data instead of audio waveforms, and it was incorporated in the legendary UNIVAC machine to replace paper tape and Hollerith cards.

Magnetic tape was a faster storage medium than paper tape and cards, and it had the advantage of being rewritable. After a hole was punched in paper tape, the hole was permanent. However, magnetic pulses on tape could be written and erased again and again. Unfortunately, tape was still too slow to be used as computer system memory.

The solution was again invented by the Germans: a metal drum the size of a small wastepaper basket, coated with iron oxide powder, spun on its axis as quickly as the motor and bearing technology of the time would allow. Tiny magnetic sensor heads were attached to the drum’s housing, with each head aligned over a separate narrow “stripe” on the drum’s surface. The magnetic heads could write bits to a track by passing electrical pulses through the heads. A pulse aligned the magnetic poles of oxide particles on the drum surface, creating a tiny magnetised region. This magnetic region would induce a tiny current in the same head when it passed beneath the head. A bit was encoded as a 1 or a 0 by the presence or absence of magnetic alignment in a small region of oxide.

In a way similar to delay-line memory, bits written onto tracks circled endlessly beneath the read/write heads as the drum rotated. The bits could only be read or written sequentially. If a value written onto a drum track was needed by the computer, the computer had to wait until that value came around again in order to read it. This slowed access down, but the drums were being spun very quickly. Access was thus faster than any earlier memory technology except for electronic flip-flops inside the CPU itself.

Programmers learned how to finesse the sequential delays inherent in drum memory by synchronising their programs to the rotation of the drum. The programs knew when a particular sequence of values would appear under the heads, and did other things during the latency period. This sounds foolish today but in 1953 it was a mainstream technique and made drum memory the fastest computer memory technology available.

One final advance in rotating magnetic memory foreshadowed modern hard-drive technology: _fixed-head magnetic memory_, which consisted of a magnetic disk with concentric tracks, each track aligned with its own stationary magnetic read/write head. Disks could be spun much faster than drums, so although a drum could hold more code and data, a disk could provide access more quickly. Apart from the shape of the storage medium, magnetic disk memory and drum memory were the same. Magnetic disk storage units of this sort were used as fast “swap memory” for virtual memory systems until the early 1970s, when moving-head magnetic disk units replaced them.

## Magnetic Core Memory

Moving parts can be bad news, and parts moving very quickly can be very bad news. Rotating magnetic memory was loud and prone to vibration. Worse, if a drum or bearing failed, the device would generally destroy itself beyond repair. So the world was ready for fast computer memory without moving parts. In 1955 it arrived. Unlike earlier memory technologies, magnetic core memory is still used in certain “legacy” (that is, ancient) computers and a small number of industrial process controllers.

Magnetic core memory systems use tiny _toroidal_ (ring-shaped) magnetic beads called _cores._ The cores are made of an exotic iron oxide with high _remanance_ (the ability to retain a magnetic field over time) and low _coercitivity_ (the energy required to change the magnetic field). One core is capable of storing 1 bit. The state of any given bit is represented not by the presence or absence of a magnetic field but by its orientation. A core’s magnetic field can exist in two different orientations, which by convention are called clockwise and counterclockwise. The state of a bit is changed by “flipping” its core’s magnetic field from clockwise to counterclockwise, or vice versa.

The toroidal cores are woven into a rectangular matrix of very fine wire supported by a sheet of circuit board material. Each assembly is called a _plane_. Four wires pass through the centre hole of every core (see Figure 3-1):

- An _x_ wire, which provides one dimension to select a core from a plane - A _y_ wire, which provides the second dimension to select a core from a plane - A sense wire, which allows the system to read the magnetic state of a core - An inhibit wire, which allows the system to set the state of a core

![[FIGURE 3-1:](#06_9781119183938-ch03.xhtml#rc03-fig-0001) The structure of a core memory plane](./media/images/9781119183938-fg0301.png)

In [Figure 3-1](#06_9781119183938-ch03.xhtml#c03-fig-0001), the cores are shown edge-on. By sending carefully controlled electric currents through the four wires in various combinations, the magnetic field orientation in selected cores may be sensed or changed. Cores may be selected singly and at random as the computer requires. Like the earlier Williams tubes, magnetic core memory is random-access memory. It’s also non-volatile, and the cores retain their magnetic fields (and thus their data) when the computer is powered down.

### How Core Memory Works

Electrical conductors generate magnetic fields when current passes through them. The strength of this magnetic field is proportional to the strength of the current. If a wire running through the centre hole of a core generates a sufficiently strong magnetic field, the magnetic field in the core aligns itself with the direction of the current flowing through the wire.

The _x_ and _y_ wires are used to select one core from the grid of cores in a plane, just as _x_ and _y_ values select one point in a Cartesian plane from geometry. A current is passed through the _x_ and _y_ wires that both pass through the core to be selected. Each of the two wires carries enough current to generate half of the magnetic field required to flip the core. Thus, the core through which both wires pass is given enough of a magnetic pulse to change its orientation. The direction of the current passing through the _x_ and _y_ wires determines the orientation. Passing the current one way imposes a 0-state on the core. Passing the current the other way imposes a 1-state on the core.

This sounds simpler than it is. The problem is that the computer must read a core before writing to it. And reading the core involves an attempt to write to it. The process of reading a core is easier to follow as a list of steps:

1. The computer attempts to force the state of the selected core to a 0-state by sending current of the appropriate direction to the _x_ and _y_ wires that intersect at the core of interest. 2. If the selected core was already at the 0-state, nothing happens. 3. If the selected core was originally in the 1-state, the core state changes to 0. The state change induces a small current in the sense wire. The presence of a current on the sense wire tells the computer that the bit had originally been a 1-bit.

The computer now knows whether the core was set to 1 or to 0. Alas, reading the state of a core is like holding a match to your sweater to see if it’s made of a flammable fabric. If the sweater catches fire, the material is flammable—and now there’s a big hole in your sweater. By reading a core’s state, the core is forced to 0. This kind of operation is called a _destructive read_. To retain the value that the core had originally expressed requires that the state read must be written back to the core.

Here’s how writing to a core is done:

1. The computer attempts to read the core’s state. This forces the core to the 0-state. Whatever state had been present before is discarded by the circuitry. 2. To write a 1-bit, current of the proper direction is sent through the _x_ and _y_ wires that intersect at the core. The core’s state changes to 1. 3. To write a 0-bit, the same current is sent through the same _x_ and _y_ wires. However, this time, an identical current is sent through the inhibit wire. This creates a magnetic field that bucks (cancels) the field created by the _x_ and _y_ wires. The inhibit wire prevents (inhibits) the change to a 1-bit. Because the bit was originally a 0 bit, the 0-state remains unchanged.

It sounds a little crazy today, but it does work: to read a bit from a core, you must read it and then write it back. To write a bit to a core, you must first clear the core to 0 by reading it and then either write (1) or inhibit a write (0) by using the inhibit wire.

### Memory Access Time

We’ve gone on about the internals of core memory at some length to make a point: electronic memory is governed by physics that may be a lot more subtle and complex than you expect. At some level, even digital devices operate by analogue physics. This complexity governs the all-important factor of memory access time. Reading memory takes time. Writing to memory takes time. From a height, progress towards increasing the speed of computers is the struggle to make memory fast enough not to slow the CPU to a crawl.

Core memory was the fastest sort of memory in existence when it was introduced, and it swept drum and fixed-head disk memory into the sea. (Disk memory evolved into hard disk mass storage as we know it today through the use of movable read/write heads.) Early core memory had an access time of 6 microseconds (μ), which fell to 600 ns nanoseconds (ns; here, 0.1 microsecond) when the technology was mature in the mid-1970s. This was comparable to the purely electronic memory in very early personal computers like the Altair and Apple II.

Core memory was fast for its day, but it was difficult to manufacture and very expensive. This is why it was used in mainframe computers and later minicomputers, but never to any extent in personal computers. By the mid-1970s something else had appeared to change the nature of computing even more than core memory did.

## Static Random Access Memory (SRAM)

You might wonder where transistors enter our story. Computer memory built from discrete (individual) transistors did exist, but it was bulkier and more expensive than magnetic core memory. It was also volatile. Although discrete transistor flip-flop memory was faster than core memory, its disadvantages kept it from being a broad commercial success.

Besides, in the late 1950s, engineers did the obvious and began placing multiple transistors on a single tiny chip of silicon. Texas Instruments (TI) engineer Jack Kilby added resistors to the same wafers, allowing all the necessary elements of computer logic gates to be integrated on one silicon wafer. The _integrated circuit_ (IC) was born. The famous 7400-series of transistor-transistor logic (TTL) devices was introduced in 1966 and they were used to build new generations of computers that were faster and more compact than ever before.

Although TTL computer memory appeared along with gates and counters, it was not until 1969 that Intel’s TTL 64-bit 3101 chip became the first commercial IC computer memory device. Intel’s 256-bit 1101, introduced only a few months later, was slower but contained more bits and was less expensive. The 1101’s use of metal-oxide semiconductor (MOS) technology was a watershed. MOS transistors are field-effect devices, in which electron flow is controlled by electric fields, as in vacuum tubes, whereas TTL chips use the older bipolar junction transistor (BJT) technology. BJTs operate by using small current flows to control larger current flows, with total current flows many times that of MOS transistors. MOS techniques could put many more transistors on a single chip while reducing power dissipation and waste heat. Except in very specialised applications, MOS soon drove TTL out of the memory market.

The 1101 and 3101 were static random access memory (SRAM) devices. They were random-access because a single bit could be accessed “at random” without any need to wait on sequential access or sift through other bits. They were static because bits written to the chips would remain in their written state as long as the chips had power—even if the computer’s clock was slowed or stopped. Both chips have now been obsolete for decades, but apart from packing more bits into a package, today’s SRAM chips work in very much the same way.

The basic logic element in SRAM chips is the flip-flop. A flip-flop is a logic circuit with an output that can be in one of two states, and that can be switched from one state to the other by a pulse or voltage change on an input. It will hold that state until another pulse switches it to its opposite state, or until power is removed from the circuit. Because it has two states, and because binary digits have two possible values, a flip-flop can “remember” a single bit.

SRAM bits are stored in _cells_, each of which is basically a flip-flop circuit. SRAM cells require at least four transistors. To improve speed and reliability, some designs use six transistors, at the cost of additional complexity and a smaller number of bits stored per device.

Technology has moved on quite a bit since SRAM was introduced. Except in very specialised applications that require the shortest possible access times, SRAM has been replaced by DRAM, as we’ll explain shortly. But first, let’s look at what SRAM and DRAM have in common: memory addressing.

---

> [!NOTE]

Read more about DRAM later in this chapter in the “[Dynamic Random Access Memory (DRAM)](#06_9781119183938-ch03.xhtml#c03-sec-0010)” section.

## Address Lines and Data Lines

As we saw with core memory, putting multiple bits in a memory device requires some way of selecting bits within the device to read or write. Core memory uses an _x/y_ addressing scheme very much like a Cartesian plane in geometry to select one core from all the cores in a core memory plane. Inside an SRAM or DRAM chip, memory cells are arranged in a matrix, and they’re selected using a system of _x/y_ addressing. Computers don’t locate cells in a memory system through _x/y_ coordinates. Additional circuitry is needed to convert a binary memory address to a pair of _x/y_ values that select one cell from the many.

The job of this circuitry is called _memory addressing_. Think of a computer memory system as a black box. On one side is a group of wires called address lines. On the other side is a group of wires called _data lines_. The number of wires in each group varies, depending on how much memory the system contains and how it’s organised. The address lines are used to select which memory location is to be read or written to. The data lines carry data either out of the system, when a value is read, or into the system, when a value is written. There are also a smaller number of wires called _control lines_. These have various functions, the most important of which is to specify whether a selected memory location is to be read from or written to.

In reality, although memory systems may consist of a single memory chip (as the Raspberry Pi’s does—more on that later) memory systems are generally put together from smaller units, either chips or groups of chips mounted on small circuit boards.

The best way to begin is to look at a very simple memory chip and how it works internally. The chip shown in Figure 3-2 doesn’t actually exist, but the general principles apply to nearly all memory chips of whatever size.

![[FIGURE 3-2:](#06_9781119183938-ch03.xhtml#rc03-fig-0002) How a memory chip addresses cells](./media/images/9781119183938-fg0302.png)

At the heart of the chip are 64 memory cells, arranged in a matrix of eight cells by eight cells. Each cell holds a single binary digit, which may be either a 1 or a 0. There are six address lines on the chip. Six is enough, because a six-digit binary number can express 64 different values, from 0 to 63.

Inside the chip are two decoders. A decoder is a logic element that accepts a binary number as an input value and uses it to select one, and only one, of several output lines. There is one output line for every binary value that the input lines can express. In our example, each decoder accepts a 3-bit binary number and selects one of eight output lines. A 3-bit binary number can express eight values, from 0 to 7. The decoder’s output lines are numbered 0 to 7. Put the binary value 101 (equivalent to 5 in our everyday decimal notation) on the input lines, and output line 5 is selected. (In [Figure 3-2](#06_9781119183938-ch03.xhtml#c03-fig-0002), this is shown for the _y_ decoder.)

Each of the two decoders handles one of the two axes (_x_ and _y_) in the matrix. The 6-bit binary address is split into two 3-bit parts. One 3-bit value is applied to the _x_ decoder and the other to the _y_ decoder. The cell at their _x,y_ intersection is the cell selected for reading or writing. The state of the read/write control line determines whether the selected cell will be read from or written to. When the control line is set to 0, a read is performed and whatever value is stored in the selected cell is placed on the data line. When the control line is set to 1, a write is performed and whatever value is on the data line is written to the selected cell.

## Combining Memory Chips into Memory Systems

The imaginary memory chip in [Figure 3-2](#06_9781119183938-ch03.xhtml#c03-fig-0002) can store and retrieve 1 bit at a time. Since the 1972 appearance of Intel’s ground-breaking 8008 CPU, however, computers use at least 8 bits at a time. Pulling an 8-bit byte out of a memory chip with a single data line can be done, but it would require eight memory-read operations to gather the whole 8 bits. A memory system like that would reduce the speed of any CPU to a crawl.

One common solution is to distribute 8 bits of data across eight physically separate chips. Figure 3-3 shows how this is done. This time, the scenario is real. The memory chips are the classic 2102 device, which was manufactured by several firms and was very popular in the 1970s. Each 2102 chip stores 1,024 bits. The 2102’s 10 address lines are connected in parallel, so all 10 address lines connect to all eight chips. An address placed on the address lines will select the corresponding bit in each chip. That bit will be delivered to each chip’s data pin. Because the chips work in parallel, a full 8-bit byte will be available on the row of 10 data pins with only one read from memory.

![[FIGURE 3-3:](#06_9781119183938-ch03.xhtml#rc03-fig-0003) A 1,024 × 8 memory system](./media/images/9781119183938-fg0303.png)

In [Figure 3-3](#06_9781119183938-ch03.xhtml#c03-fig-0003), eight chips, each containing 1,024 bits, are combined into the equivalent of a single memory chip holding 8,192 bits. But more to the point, the arrangement of bits in the memory system shown is 1,024 × 8, and not 8,192 × 1. A full 8-bit byte can be written to the memory bank with a single memory access—and read back just as quickly.

> [!NOTE]
> that the memory system has 10 address lines. To access a single byte from among the 1,024, the value placed on the address bus must be able to express values from 0 to 1,023 in binary. 1,023 in binary is 1111111111. Ten binary digits require 10 address bus lines.

A group of digital lines connecting a memory system of any kind to a computer is called a _bus_. The 10 address lines in [Figure 3-3](#06_9781119183938-ch03.xhtml#c03-fig-0003), taken together, form the address bus. The eight data lines form the data bus. However many control lines the memory system may have (the number’s not important in this example) together make up the control bus.

The old 2102 chip was organised as 1,024 × 1 bit. This was a common organisation for a long time, but it was far from the only one. For example, there are SRAM chips that are organised in many other ways, from 256 × 4 in ancient times, to 1,031,072 × 16 today. (There are much larger memory chips in modern systems, but they’re all DRAM, which we get to shortly.)

The number of storage locations in a memory chip or system is called its _depth_. The number of bits at each storage location is a memory chip’s or system’s _width._ The size of a memory chip or system is the number of bits (not bytes!) that it contains. This is defined as the depth times the width.

Some examples:

- The old 2102 chip has a depth of 1,024 and a width of 1. Its size is 1,024 bits. - The old 6116 chip has a depth of 2,048 and a width of 8. Its size is 16,384 bits. - The modern Cypress 62167 chip has a depth of 1,048,580 and a width of 16. Its size is 16,777,280 bits.

The literal numbers describing a chip’s size become ungainly beyond a certain point. Powers of 2 do not convert to round numbers in decimal notation. In talking about memory chips and systems, we use shortcuts, as shown in Table 3-1.

<figure> <figcaption>

[Table 3-1](#06_9781119183938-ch03.xhtml#rc03-tbl-0001) Conventional Terms for Powers of 2

</figcaption>

---------------- --------------- ------ 2<sup>10</sup> 1,024 1K 2<sup>11</sup> 2,048 2K 2<sup>12</sup> 4,096 4K 2<sup>13</sup> 8,192 8K 2<sup>14</sup> 16,384 16K 2<sup>15</sup> 32,768 32K 2<sup>16</sup> 65,536 64K 2<sup>17</sup> 131,072 128K 2<sup>18</sup> 262,144 256K 2<sup>19</sup> 524,288 512K 2<sup>20</sup> 1,048,576 1M 2<sup>21</sup> 2,097,152 2M 2<sup>22</sup> 4,194,304 4M 2<sup>23</sup> 8,388,608 8M 2<sup>24</sup> 16,777,216 16M 2<sup>25</sup> 33,554,432 32M 2<sup>26</sup> 67,108,864 64M 2<sup>27</sup> 134,217,728 128M 2<sup>28</sup> 268,436,480 256M 2<sup>29</sup> 536,870,912 512M 2<sup>30</sup> 1,073,745,824 1G 2<sup>31</sup> 2,147,483,648 2G 2<sup>32</sup> 4,294,967,296 4G ---------------- --------------- ------

</figure>

In recent years, there’s been an effort to distinguish these shortcuts (which refer to powers of 2) from the equivalent ISO prefixes (which refer to powers of 10) by introducing new shortcuts and prefixes. One kibibyte (1KiB) is the precise quantity 1,024 bytes, formerly referred to as a kilobyte (KB); under this scheme a kilobyte is now 1,000 bytes, just as a kilogram is 1,000 grams. Likewise, 1 mebibyte (1MiB) is the precise quantity 1,048,576 bytes and 1 gibibyte is the precise quantity 1,073,745,824 bytes. The new terms were defined in IEEE standard 1541, released in 2002. They are not widely used at this writing, but it’s worthwhile to keep them in mind, especially when reading the scientific and engineering literature.

## Dynamic Random Access Memory (DRAM)

Each SRAM memory cell is a complete flip-flop circuit that, at a minimum, consists of four transistors. SRAM is fast, certainly the fastest mass-market memory technology ever devised. It’s still in use, when speed is required above all else. (We talk about how speed affects computer memory systems later in this chapter.) SRAM has two major disadvantages:

- It’s big, in terms of space per bit on a silicon chip. - It doesn’t shrink well, at least past a certain point.

These limitations keep SRAM at a certain size and a certain cost per bit. This was recognised by researchers early on. In 1968, IBM fellow Robert H. Dennard proposed a radically different memory technology that did away with flip-flop data storage altogether. His memory technology stored bits as the presence or the absence of charge in a miniscule capacitor. The presence of charge represented a binary 1 and the absence of charge represented a binary 0. (This assignment of meaning is arbitrary and could be the reverse, as long the memory chip presents the proper voltage levels on its data lines.)

A Dennard memory cell consists of only one transistor and one capacitor. Even with early fabrication technologies, this was less than half as large as an SRAM cell. Dennard also had a hunch that this technology could be scaled far more easily than SRAM. He meant that the physics of a Dennard cell would allow future fabrication technology to shrink individual cells far beyond what was possible with SRAM. He was right, to an extent that no one, not even Dennard himself, could have predicted in 1968.

With metal-oxide-semiconductor (MOS) transistors designed specifically for memory cell use, Dennard’s memory cells used far less power and generated far less waste heat. (This also helped with scaling—more bits could be placed on a single chip without fear of the chip “cooking” itself with its own heat.)

The trade-off lay in the physics of charge stored in a capacitor: even in the best and purest silicon chip capacitors, over time a stored charge leaks away. Large capacitors can store so much charge that they can be used as batteries sometimes. The microscopic capacitors in Dennard’s scheme were so small that their charge leaked away in mere hundredths of a second. As with the ancient mercury delay-line memory systems, capacitor-based memory has to be refreshed (read and then rewritten) periodically. Thus, this memory technology is dynamic and goes by the name _dynamic random access memory_ (DRAM).

### How DRAM Works

Like both core memory and SRAM, DRAM memory chips are based on two-dimensional arrays of memory cells. Cells are addressed by _x_ and _y_ coordinates, using address decoders (look back at [Figure 3-2](#06_9781119183938-ch03.xhtml#c03-fig-0002)). Each individual cell consists of a single MOS transistor and a single capacitor, as shown in Figure 3-4. The three connections to the transistor are well known to electronics hobbyists: the gate is an electrical switch toggle that either connects the source to the drain or insulates them from each other. (The source and the drain are different in minor ways that do not affect this description.)

![[FIGURE 3-4:](#06_9781119183938-ch03.xhtml#rc03-fig-0004) DRAM cells](./media/images/9781119183938-fg0304.png)

[Figure 3-4](#06_9781119183938-ch03.xhtml#c03-fig-0004) shows four DRAM cells within a matrix of identical cells that may number into the billions. Cells are organised into rows and columns. A row (the horizontal dimension in [Figure 3-4](#06_9781119183938-ch03.xhtml#c03-fig-0004)) is linked by a common connection to all cell transistor gates called a _word line._ The word line is used to select one row from all rows in the memory chip. It “flips the switch” of all the MOS transistors in a row at once, causing them to either conduct or not conduct. Cells in each column are linked by a common connection to all transistor drain leads, called a _bit line._ At the end of each column’s bit line is a sense amplifier, which allows an almost unimaginably small unit of charge to be reliably interpreted as a 1 or a 0. In very general terms, the word lines are used to select cells and the bit lines are used to read and write data in cells.

An MOS transistor is a solid-state switch. When the transistor is switched on, the capacitor is electrically connected to the bit line. When a cell’s transistor is switched off, the capacitor is isolated and charge (or lack of charge) is retained inside the capacitor. The charge leaks away in a fraction of a second unless the cell is refreshed. (More on that shortly.) The general idea is to select a cell and either read the state of its charge or write a charge state to the cell depending on whether a 1 or a 0 is to be written. This is not done individually, but in almost all cases to an entire row of cells at once.

DRAM operation has a familiar resemblance to the operation of core memory. DRAM, like core memory, uses destructive reads: the physics of reading the charge from the cell destroys the charge, which then has to be written back in a refresh operation. There are crucial differences; unlike core memory, which is static, DRAM needs to be refreshed regularly whether it is read or not.

These steps outline how a bit is read from a DRAM cell:

1. The cell’s bit line must be given an initial voltage (a precharge) that places it precisely halfway between a full charge on the capacitor and complete discharge. 2. When the precharge is complete, the precharge circuitry is disconnected and the bit line is switched to the sense amplifier. 3. The cell’s word line is selected. This turns on the MOS transistor of the selected cell (as well as all the other cells in the row) and connects the capacitor to the bit line. 4. The capacitor’s charge state affects the voltage on the bit line. If the capacitor has been charged, the bit line’s voltage goes up slightly. (Very slightly!) If the capacitor has been discharged, the bit’s line’s voltage goes down slightly. This change in voltage is exceptionally small and could amount to the difference of only one million electrons. 5. The sense amplifier converts this tiny change in voltage to a digital state of either 1 or 0. 6. The read operation destroys charge in the capacitor of the selected cell and all the other cells in the row. The state that was read must then be refreshed and written back to all cells in the row.

Writing to a DRAM cell is done this way:

1. The cell’s bit line is given a voltage corresponding to the value to be written to the cell. Typically, a 1-bit is represented by full voltage and a 0-bit by no voltage. 2. The cell’s word line is selected. This turns on the MOS transistor and allows the voltage applied to the bit line to pass into the cell’s capacitor.

> [!NOTE]
> that DRAM cells are not accessed one at a time. Because they share a word line, an entire row of cells is accessed at once. We talk about “opening” a row (reading the values from an entire row of cells into temporary storage at the edge of the SDRAM chip) and “closing” a row (writing back any changes from the temporary storage to the cells themselves). (More on SDRAM later.) This sounds like it might be a waste of time, but in modern computers, system memory is almost always read and written in chunks called _cache lines,_ which are maintained in fast memory stores called _cache,_ as we explain later in this chapter.

A row is refreshed under two circumstances:

- Any time a cell in that row is read - Every 5 to 50 milliseconds, to prevent electron leakage from destroying cell data

Rows are refreshed simply by reading the state of the cells in the row and then immediately writing it back to the cells. This reading and writing is not done through the CPU, or in fact with any involvement of the CPU at all. A separate subsystem called a _memory controller_ handles the refresh operation and a great many other housekeeping details that allow the CPU to access memory with as little delay as possible. Taken together, the memory controller and the DRAM chips that it manages are called a memory system.

The speed with which data moves between memory systems and the CPU can dominate the overall performance of the entire computer. Memory system performance is a complex business, with two different metrics that are often in tension with one another:

- **Access time:** The time it takes between the moment a memory access is requested by the CPU and the time the access is completed - **Bandwidth:** The amount of data transferred to or from memory per unit time

Much of the rest of this chapter addresses issues related to improving the effective access time and bandwidth experienced by the CPU when accessing memory.

### Synchronous vs. Asynchronous DRAM

DRAM has dominated computer memory systems since 1980 or so, and dominates them to this day. Quite apart from scaling (that is, making DRAM cells smaller), DRAM has been improved in many ways. Perhaps the most dramatic improvement was the move to synchronous DRAM (SDRAM) in the late 1990s.

Prior to that time, all DRAM was asynchronous. The operation of asynchronous DRAM is managed directly from the memory controller. The controller can open a row by presenting a row address on the unidirectional data bus and bringing the row address strobe (RAS) command line low; having done so, it can read or write cells within the open row by presenting a column address and bringing the column address strobe (CAS) command line low. A bidirectional data bus is used to transfer data to or from the DRAM; the direction of travel is determined by the write enable (WE) and output enable (OE) command lines. An asynchronous DRAM device starts performing an operation as soon as it detects an RAS or CAS transition, but requires a finite amount of time (called _latency_) to perform each operation. The device’s datasheet will typically contain timing parameters indicating how long (in nanoseconds) we must wait between, for example, opening a row and starting an access to a column in that row (the RAS to CAS latency), or starting a read access to a column and expecting to receive valid data on the data bus (the CAS to valid data out latency, or just CAS latency). The memory controller must be programmed with these timing parameters for memory operations to occur reliably.

A critical disadvantage of asynchronous DRAM is that it is only possible to perform one memory access operation at a time. While we’re waiting for a row to open, the data bus is completely idle, “wasting” potential throughput. Fast page mode (FPM) DRAM, which became popular around 1995, mitigates this problem to some degree by allowing a burst of multiple accesses to an open row (multiple CAS transitions per RAS transition), but inefficiency remains when switching between rows.

The eventual solution to the wasted throughput problem was the introduction of SDRAM. The key innovation in SDRAM is the splitting of the DRAM cell matrix into multiple independent banks, which can be thought of almost as separate asynchronous DRAMs. Fine-grained control of these banks is delegated to logic inside the SDRAM itself, running off a clock (and therefore “synchronous”) generated by the memory controller. The memory controller passes commands to the logic inside the SDRAM using a unidirectional control bus*,* which takes the place of the address bus and control signals used by asynchronous DRAM. By maintaining a short queue of upcoming memory access requests from the CPU and other bus master peripherals, the memory controller is able to schedule the commands that it issues so as to hide the latency of precharge and row-open operations, potentially keeping the data bus completely busy. For example, while receiving the results of a multi-cycle burst read from an address in bank 0, the controller might issue a command to open a row in bank 1 and then a command to close the current row in bank 2, precharging that bank so that it is ready for a future row-open command. This technique of overlapping operations on multiple banks is referred to as _pipelining_ and it’s the main contributor to the improved performance of SDRAM over asynchronous DRAM.

To gain more flexibility in how it pipelines memory operations, the memory controller may, under some circumstances, choose to reorder the requests in its queue. There is generally a signalling scheme between the CPU and the memory controller to help the controller understand which accesses can be reordered safely. The controller typically also reorders requests to group multiple reads and writes together, minimising the number of bus turnarounds, where the direction of flow on the data bus changes, necessitating a small amount of dead time.

Operations on the individual banks inside the SDRAM device have characteristic latencies, just as with asynchronous DRAM. Once again, these timing parameters are typically specified in the datasheet for the device; in the case of SDRAM they’re generally specified as a number of clock cycles at the device’s maximum supported clock frequency, rather than directly in nanoseconds. The memory controller programs these parameters into the SDRAM’s internal logic at boot-up time, and relies on them to know how many cycles to wait between issuing commands on the bus and receiving data.

### SDRAM Columns, Rows, Banks, Ranks and DIMMs

In the previous section you saw that an SDRAM device is composed internally of a collection of equal-sized independent banks. Each bank is structured as a matrix of a number of rows, and the bits in each row are grouped into columns of a specific width. A row in a modern SDRAM chip contains tens of thousands of bits, and a column is typically 8, 16 or 32 bits wide. A row and a column address together specify a starting point within the bank’s grid of memory cells and the cells beginning at that starting point are read and written as a unit, out to the width of the column.

Typically there are 2, 4 or 8 banks on each chip. The banks themselves may be of different sizes for different chips. A common 128MB SDRAM memory chip contains 8 banks, each of which contains 16,384 rows of 1,024 columns of 8 bits. The total number of bits in the chip is thus 8 banks × 16,384 rows × 1,024 columns × 8 bits per column = 1,073,745,824 bits. It’s called a 128MB chip because 1,073,745,824 bits divided by 8 bits per byte is 134,217,728 bytes. (Refer to [Table 3-1](#06_9781119183938-ch03.xhtml#c03-tbl-0001) to see why that number is considered to be 128MB.)

SDRAM chips are organised as they are as a consequence of how the chips themselves are combined into memory systems. For desktop and conventional laptop computers, multiple chips are assembled onto small “stick” printed circuit modules. Until the late 1990s these were single in-line memory modules (SIMMs) because the corresponding edge connector contacts on both sides of the printed circuit board were identical and tied together. (It does not mean, as some think, that memory chips are present on only one side of the module!) SIMMs can transfer 32 bits to or from the data bus at one time.

Having the same signals on both sides of the edge connectors on SIMMs limits the number of electrical connections that can be made between the SIMM and the data bus. A SIMM typically has 72 connectors on its edge. Making the two sides of the edge connector independent at least doubles the number of connections that can be made between a module and the data bus. With this change, modules became dual in-line memory modules (DIMMs), which have dominated desktop and laptop memory systems since 2000 or so. DIMMs typically have 168 or more separate connectors, and transfer 64 bits to or from the data bus at once.

For physical compactness, many laptops and netbooks use a different, smaller type of DIMM module called a small outline DIMM (SODIMM). Seventy-two-pin SODIMMs are 32 bits wide and 144-pin SODIMMs are 64 bits wide.

On a modern DIMM, each side of the module is a separate bus-addressable memory block called a _rank_. A rank is defined as a group of memory chips sharing the same chip-select control line. A rank’s chips thus appear on the data bus together. Each chip within the rank contributes 8 bits to the 64 bits that the rank reads or writes at once.

Figure 3-5 shows how a typical DIMM is organised. Precise numbers aren’t stated because different modules are built from SDRAM chips of different sizes and different internal organisation.

![[FIGURE 3-5:](#06_9781119183938-ch03.xhtml#rc03-fig-0005) How a typical DDR SDRAM DIMM is organised](./media/images/9781119183938-fg0305.png)

### DDR, DDR2 DDR3 and DDR4 SDRAM

The first generation of ordinary SDRAM is today referred to as _single data rate_ (SDR) SDRAM. The term only became necessary in the late 1990s, when improvements to SDRAM technology gave us _double data rate_ (DDR) SDRAM. SDR SDRAM is called “single data rate” because it can transfer a single data word per clock cycle. The size of the data word depends on the design of a specific memory system (specifically the number of wires in the data bus linking the memory controller to the SDRAM). In most modern desktops and laptops, it’s 64 bits. In the early Raspberry Pi models, it’s 32 bits. For the Raspberry Pi 3, it’s 64 bits.

In DDR SDRAM, two memory transfers occur for each clock cycle. In SDR technology, a memory transfer happens on the rising edge of each clock cycle. In DDR, memory transfers happen on both the rising edge and the falling edge of each clock cycle, essentially doubling the rate at which memory transfers happen. This is called _double pumping_. See Figure 3-6.

![[FIGURE 3-6:](#06_9781119183938-ch03.xhtml#rc03-fig-0006) SDR vs. DDR timing](./media/images/9781119183938-fg0306.png)

Increasing the memory transfer rate by increasing the clock rate causes various electrical problems. Higher clock rates for anything increase power usage and waste heat. Reliably driving a high-speed clock across a board introduces challenging signal integrity issues for chip and PCB designers; sooner or later you reach a limit in terms of edge rate; that is, the number of times a wire can change from 0 to 1 or 1 to 0 in a second. In an SDR system, the clock changes twice per cycle (from 0 to 1 and back again), whereas no data line changes more rapidly than once per cycle; in such a system you hit the wall on clock edge rate before data edge rate. By allowing the data lines to change twice per cycle, DDR signalling makes the most of a given technology’s capabilities.

At around the time of the introduction of DDR SDRAM, the internal speed at which SDRAM devices operated (as distinct from the external interface speed) stopped increasing significantly. Why? Well, the speed at which you can read a row of cells from the array is dominated by signal propagation time, which is determined by wire length and the time required for the sense amplifiers to detect the faint charge on the bit lines. Successive generations of SDRAM devices pack more storage into the same area, instead of getting smaller, and the charges stored on the capacitors in the array become smaller and harder to detect. As a result, process shrinkage, which has done so much to sustain Moore’s Law for logic devices, has had little effect on the internal speed of SDRAM.
