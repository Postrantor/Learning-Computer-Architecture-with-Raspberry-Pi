Chapter 9

# Video Codecs and Video Compression

**A VIDEO IS** a sequence of images that are shown one after the other. In principle, you could store them as a digital flip-book, with a picture for each frame. Without compression, roughly 3 bytes per pixel (one to store each of the red, green and blue colour components) are required to avoid introducing perceptible _quantisation artefacts_ (visible steps in brightness or colour). If you wanted to store video even at a relatively low resolution (640 × 480 pixels) at 25 frames a second, each second would then take up 3 \* 640 \* 480 \* 25 bytes, which works out to just more than 23 megabytes (MB) per second. A two-hour film would take up more than 165 gigabytes (GB), which is equivalent to 10 double-sided, double-layer DVDs. Applying a generic lossless compression algorithm such as ZIP might make it a little smaller, but you’d still need several of these disks.

Storing footage as just described would basically make almost any form of digital video distribution completely impractical. Changing the side of a DVD every six minutes would be annoying, and downloading a TV show would take days. YouTube would only work for clips a few seconds long. Video chat would require either an image too small to be useful or the fastest Internet connection available.

In order to make digital video distribution possible, it’s essential to find ways to make the videos much smaller. This shrinking of files is known as compression. There are two basic types of compression: lossless and lossy. In _lossless_ compression, the file is shrunk in such a way that it’s possible to recreate the original file perfectly from the compressed file, down to the level of individual bits. This is how file formats such as `.zip`</code> or `tar.gz` work. However, there’s a limit to how small you can make a file with lossless compression, and lossless compression on its own generally isn’t sufficient for most video applications.

In contrast with lossless compression, _lossy_ compression makes a file smaller by removing some of the information. After lossy compression, it is no longer possible to recreate the original file perfectly from the compressed file. As a trivial example of lossy video compression, imagine simply halving the horizontal and vertical resolution of each image in the video stream. The resulting video file would shrink by a factor of four, at the cost of a significant reduction in visual fidelity. The art of designing lossy video compression algorithms and encoder implementations lies in keeping the perceived quality of the decoded stream as high as possible while making the file as small as possible.

Most video encoders use both lossless and lossy compression techniques to get the files as small as possible.

## The First Video Codecs

The International Telecommunication Union (ITU) developed the first widely used video compression standard (known as H.261) to enable video calls over Integrated Services Digital Network (ISDN) lines in 1988. In comparison to more modern standards, H.261 delivered relatively poor image quality for a given bit rate, but it is notable for having laid the technical foundations for future video compression standards. Compression standards are often known informally as codecs, a mash-up of *co*der-*dec*oder. More formally, the term codec refers to an implementation of a standard in software, hardware or a combination of the two.

The Moving Picture Experts Group (MPEG) was formed in 1988 by the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC) to take this foundation and build it up to support higher video quality than was possible over ISDN lines. Both the ITU and MPEG continue to develop codecs, often in collaboration with one another. Since 2001 much of this work has been done under the auspices of the Joint Video Team (JVT), which was responsible for the successful H.264/MPEG-4 AVC codec. The MPEG series of standards includes more than just video. It includes the file structure, audio and other parts needed to make a fully functional video file.

The first standard developed by MPEG (known as MPEG-1) was released in 1993. There are two ways the designers of MPEG-1 sought to minimise file size while maximising image quality:

- Preferentially removing information that humans find hard to perceive (exploiting the eye) - Exploiting the sort of information that videos hold (exploiting the data)

### Exploiting the Eye

Our eyes possess two types of receptors that detect light: rods that detect brightness and cones that detect colour. Rods are more sensitive than cones, which is why we lose the ability to see colours when it gets dark, although we can still make out shapes. We also have about 20 times as many rods as cones. This means that we’re far better at making out fine variations in brightness than in colour. It’s a little quirk of human physiology that can be exploited when compressing video since there’s no point in storing information that the eye can’t see.

To treat brightness and colour differently in a codec, it is helpful first to transform the image from the RGB colorspace, where each pixel is represented by a red, a green and a blue value, to the so-called Y’C<sub>b</sub>C<sub>r</sub> colorspace, where each pixel is represented by a _luma_ (brightness) value Y’ and two _chroma_ (colour) values C<sub>b</sub> and C<sub>r</sub>. Luma corresponds to perceived brightness and is computed as a weighted sum of the original red, green and blue values. There are several slightly different YC<sub>b</sub>C<sub>r</sub> colorspaces, which are used for different applications. The weights and sums for the commonly used ITU-R BT.601standard are:

Y’ = 0.257R + 0.504G + 0.098B +16

Cr = 0.439R – 0.368G – 0.071B + 128

Cb = -0.148R – 0.219G + 0.439B + 128

If we visualise the 24-bit RGB colorspace as a cube, increasing luma moves us roughly along a leading diagonal from black (0,0,0) to white (255,255,255), through 254 shades of grey. The chroma values represent movement away from the diagonal: roughly speaking, C<sub>b</sub> and C<sub>r</sub> represent how much of a blue or red tint the colour has, respectively.

Changing the colorspace like this doesn’t make the image any smaller (each pixel is still represented by three numbers, and each number requires roughly the same number of bits of precision as before), but it splits up the brightness from the colour. In effect, there are three independent images, or _channels_: one of brightness, one of “redness” and one of “blueness”. The individual pixel values that make up a channel are referred to as _samples_. These are displayed together, but they can be stored in different ways. Because we have so many more rods, which are for seeing detail, it doesn’t matter if the colour values are at a lower resolution. The first, and simplest, stage of MPEG-1 compression is chroma subsampling. This leaves the luma channel at full resolution, but halves the horizontal and vertical resolution of both chroma channels, shrinking the space they occupy by a factor of four (see Figures 9-1, 9-2 and 9-3). The overall space occupied by the image is thus halved (because 1 + ¼ + ¼ = ½ × 3) at no cost in visual quality. Not bad for a first step!

![[FIGURE 9-1:](#12_9781119183938-ch09.xhtml#rc09-fig-0001) The luma channel of the image](./media/images/9781119183938-fg0901.png)

![[FIGURE 9-2:](#12_9781119183938-ch09.xhtml#rc09-fig-0002) The chroma red channel of the image](./media/images/9781119183938-fg0902.png)

![[FIGURE 9-3:](#12_9781119183938-ch09.xhtml#rc09-fig-0003) The chroma blue channel of the image](./media/images/9781119183938-fg0903.png)

### Exploiting the Data

The second technique that video compression can use is to make assumptions about the properties of the content being transmitted. Typically, each image in a video isn’t completely different to those before and after it. If it was, the screen would just display different unrelated images in quick succession, and you wouldn’t be able to make much sense of what is happening. Instead, most frames are very similar to the ones before and after it. Perhaps most of the background is the same with just a few static or slowly changing objects moving around, or perhaps the whole frame moves as the camera pans. Either way, this means that most of the information is already available in a preceding frame. Sometimes the whole image changes as the video cuts to a new scene, but this is infrequent considering there may be between 24 and 60 frames per second.

To take advantage of this feature of video data, an MPEG-1 encoder splits the sequence of frames into I frames, P frames and B frames.

#### I Frames

Intra-frames, or I frames, are stored in a way that allows them to be decoded by themselves, without reference to any other frame in the video. From a technical perspective, I frames are encoded in a very similar way to the JPEG format for storing still images; the compression techniques you’ll see used on I frames work in much the same way to keep photographs small.

The first stage is to split each channel (Y’, C<sub>b</sub> and C<sub>r</sub>) of the I-frame image into 8×8 sample blocks. Because the chroma channels have already been subsampled, a single 8×8 block in the chroma channel corresponds to four adjacent 8×8 blocks in the luma channel. This collection of six blocks (one C<sub>b</sub>, one C<sub>r</sub> and four Y’) is known as a macroblock. We’ll look at how these macroblocks are used later on, but first let’s take a look at the other types of frame.

#### P Frames

Predicted frames, or P frames, depend on image data from the preceding I or P frame. They don’t describe the whole image, just the bits that have changed. As such, they can’t be decoded without the preceding I or P frame being decoded first. The P frames are divided into macroblocks in exactly the same way as I frames.

As previously mentioned, a large portion of each image will probably be the same as in the previous image, just moved slightly. When encoding a P frame, the encoder looks at each macroblock in the image in turn and tries to find similar macroblock-sized areas in the preceding frame; this procedure is known as _motion search_. If the encoder finds a similar area, it doesn’t encode the new macroblock from scratch; instead it encodes a motion vector indicating where in the previous frame the match has been found. A macroblock encoded in this way is known as a P macroblock; when the decoder comes to decode a P macroblock it decodes the motion vector and copies the appropriate area of the preceding frame. If the encoder fails to find a sufficiently similar macroblock in the previous frame, it stores the macroblock in exactly the same way that macroblocks are stored in I frames. A macroblock encoded in this way is known as an I macroblock.

Even if motion search has identified a good candidate for prediction, it’s likely that there will still be some small differences between the current frame and the corresponding section of the preceding frame. For example, a macroblock may contain a bird flying across the screen. As it flies, it also changes shape as its wings flap. This difference is known as the _prediction error_ or _residual_. The encoder may choose to encode the residual using the same techniques that it would use for I macroblock image data and store the encoded residual along with the motion vector; when the decoder comes to decode the macroblock, it decodes the residual and combines it with the image data copied from the preceding frame.

The smaller the residual, the less information there is to store, and therefore the smaller the file size. In order to capture movement as accurately as possible, MPEG-1 motion vectors can be specified down to the half-pixel (also called half-pel) level in both _x_ and _y_ directions. If it decodes a half-pel motion vector for a macroblock, the decoder must do more than just copy pixels from the previous frame: it must also have a scheme for generating the “missing” pixel values that lie halfway between the real pixel values. This process is called interpolation. If you visualize a single line of pixels, in a single channel, you’ll have a single sample for each pixel. These could be plotted on a graph to show how the sample value changes along the line of pixels. The easiest interpolation scheme, used by MPEG-1, is to draw a straight line between the two points and plot the middle point on this line (mathematically, we take the average of the two adjacent samples); this is known as linear interpolation.

Of course, in a 2D picture, we need to do this vertically as well as horizontally. Motion vectors that have integer _x_ or _y_ components (for example (1, ½) or (3½, 2)) are straightforward, as they require linear interpolation in one direction only. Motion vectors that have half-pel _x_ and _y_ components (for example (2½, ½)) require us to average four adjacent samples in the source image. This is known as bilinear interpolation. See Figure 9-4 for details.

![[FIGURE 9-4:](#12_9781119183938-ch09.xhtml#rc09-fig-0004) The location of the full pixels (squares) and the half-pixel values (crosses) on a 2×1 grid. Later video standards also use quarter-pixel values (circles).](./media/images/9781119183938-fg0904.png)

This encoding of movement is obvious if you’ve ever seen a corrupted MPEG video. Parts of the frame still move around, but because the preceding frame is wrong, the images that are moving are incorrect.

#### B Frames

Bi-directional frames, or B frames, are much like P frames except that they can contain elements from the preceding I or P frame and the subsequent I or P frame;

> [!NOTE]
> that no frame is ever predicted off a B frame.

Each macroblock in a B frame can be predicted off areas in one or both of these frames. If it’s predicted off both, then the encoder must store two motion vectors, and the decoder computes a weighted average of the two areas before combining it with the residual (if any).

If the video stream has a large number of B frames in a row, one of the reference frames could be quite a way ahead. This could create a problem for the decoder because it would have to read forward all the way to the reference frame and decode that before coming back to decode the B frame. In order to simplify this problem, the encoder doesn’t write frames to the file in the order they appear on the screen, but so that the reference frames are always before the frames that are predicted off them. Table 9-1 shows an example video stream.

<figure> <figcaption>

[Table 9-1](#12_9781119183938-ch09.xhtml#rc09-tbl-0001)  Example Group of Pictures with Frame Types

</figcaption>

-------------- --- --- --- --- --- --- --- Frame number 1 2 3 4 5 6 7 Frame type I B B P B B I -------------- --- --- --- --- --- --- ---

</figure>

This would be stored in the order 1,4,2,3,7,5,6. First the decoder gets to frame 1. This is an I frame so it can be decoded independently. Then the decoder gets to frame 4. This is a P frame, so it’s predicted off an earlier frame. Because frames 2 and 3 are B frames, it’s predicted off frame 1, which has already been decoded. Then the decoder gets to frame 2, which is a B frame. The two reference frames (1 and 4) have already been decoded, and likewise for frame 3, which has the same reference frames. The same method is used to reorder the second half of the frames.

This reordering doesn’t change the order in which the frames are displayed on the screen (the _presentation order_). That is still done in numerical order. They’re just stored like this to make life easier for the decoder.

There isn’t a set way for MPEG-1 encoders to split the video into I, P and B frames. Each piece of encoding software does it a little differently. Most videos follow the pattern shown in [Table 9-1](#12_9781119183938-ch09.xhtml#c09-tbl-0001), with I frames at regular intervals, P frames evenly spaced between I frames, and B frames for the rest. An I frame and its successive P and B frames are known as a _group of pictures_ (GOP); the pattern of P and B frames is known as the _GOP structure_; and the size of the gap between I frames is known as the _GOP size_.

The GOP size and structure can be set by the encoder depending on the required bit rate, and how easily we wish to be able to seek through a video. Because only I frames can be decoded independently, we can only seek to a GOP boundary; a small GOP size makes it easier to seek to an arbitrary location in the video, generally at the cost of requiring a higher bit rate for a given quality due to the increased number of expensive I frames.

Available bandwidth is particularly important because videos are usually intended to be played without buffering. We typically think of bandwidth today in terms of an Internet connection, but when the MPEG-1 codec was designed, other factors were more important because streaming video over the web wasn’t yet possible. MPEG-1 was designed to work at a range of bit rates, but the key one for the designers was the speed at which a CD-ROM drive could read data (1.5 megabits per second (Mbits/s)). MPEG-1 provides roughly the same quality as a VHS video cassette at this bit rate. The Video CD format, a precursor to DVDs, stores 74 minutes of MPEG-1 video on a standard CD.

Video CD is a good example of a “constant bit rate” format: it isn’t possible to run the CD faster to get more bit rate to encode a rapidly changing scene, or to run it more slowly to conserve space when the scene isn’t changing. Modern streaming video codecs often vary their bit rate (within limits) according to scene complexity. There are a variety of techniques an encoder can use to keep the video stream at the required bit rate. First you need to understand how MPEG-1 encodes image data and residuals.

### Understanding Frequency Transform

As previously described, MPEG-1 exploits the eye’s inability to distinguish fine changes in colour through chroma subsampling. Another helpful (for codec designers) attribute of the human visual system is that we find it harder to detect fine (high-frequency) changes in either brightness or colour than to detect coarser-grained (low-frequency) changes. In principle, we can represent high-frequency details in the scene less accurately, or even discard them altogether, without compromising perceptual quality.

Just as we transformed the image into the Y’C<sub>b</sub>C<sub>r</sub> colorspace to allow us to separate and subsample chroma, we must transform our data again to allow us to discard high-frequency details. This time, the four 8×8 luma blocks and two 8×8 chroma blocks that make up a macroblock are each passed through a discrete cosine transformation (DCT). The mathematical details are beyond the scope of this book, but the key is that after applying the DCT we no longer store a grid of 8×8 sample values, one for each point (a _spatial representation_), but instead store details of how the samples change as we move across the block in the _x_ and _y_ directions (a _frequency representation_).

It’s easiest to understand the DCT by first thinking of the one-dimensional case: a single line of an 8×8 sample block. This one line has eight sample values that could be displayed as a line graph. The DCT decomposes this line into a weighted sum of several cosine waves (_basis functions_) of different frequencies. When added together, these functions have the same value as the line (at least at the sample points). The cosine waves are described by coefficients that give the amplitude of each cosine wave. It turns out that eight coefficients (and eight waves of different frequencies) are enough to accurately capture the original signal—we’ve traded eight spatial-domain samples for eight frequency-domain coefficients. In the two-dimensional case, it turns out that we need 64 two-dimensional cosine waves (surfaces that vary at different rates in the _x_ and _y_ directions) and 64 coefficients.

It is helpful to write the 64 coefficients in an 8×8 block, with the ones in the top left representing the lower frequencies, and the ones in the bottom right representing the higher frequencies (see Figure 9-5). The top-left value is known as the DC coefficient and is always equal to the average value of all the samples in the block. In other words, it’s the value of the block without taking into account any of the spatial changes.

---

> [!NOTE]

The name DC comes from direct current and is a relic from when similar methods were used to analyse electricity.

![[FIGURE 9-5:](#12_9781119183938-ch09.xhtml#rc09-fig-0005) The spatial frequencies that each coefficient represents](./media/images/9781119183938-fg0905.png)

All the other values are called AC (alternating current) coefficients. In [Figure 9-5](#12_9781119183938-ch09.xhtml#c09-fig-0005), the top line represents changes in purely the horizontal direction, with the leftmost AC coefficient (next to the DC) holding the lowest frequency data, while the rightmost one holds the highest frequency data. Similarly, the leftmost column holds information about changes in purely the vertical direction. The other values hold information about changes in both directions. For example, the rightmost value on the second row corresponds to high-frequency change in the horizontal direction and low-frequency change in the vertical direction.

Just as when we transformed from RGB to Y’CbCr, the DCT hasn’t compressed the image itself: the cosine coefficients take up roughly the same amount of space as the original data. However, once again it has set us up to be able to apply a subsequent lossy compression step in a way that minimises the perceived impact on visual quality. The lossy step in this case is _quantisation_—dividing each coefficient by a number and rounding down during encoding, and then multiplying by the same number during decoding to get back a similar (but generally not quite identical) number. As human eyes are more able to detect errors in low-frequency data than high-frequency data, the encoder generally quantises the high-frequency coefficients more coarsely.

Take a look at Figures 9-6, 9-7 and 9-8, which contain a frame with increasingly large amounts of compression. As the file size gets smaller, more and more errors start to creep into the higher-frequency portions of the image.

![[FIGURE 9-6:](#12_9781119183938-ch09.xhtml#rc09-fig-0006) At maximum quality, there’s little quantisation, so both the high- and low-frequency portions of the image are displayed well. This image has about 6 bits per pixel.](./media/images/9781119183938-fg0906.png)

![[FIGURE 9-7:](#12_9781119183938-ch09.xhtml#rc09-fig-0007) As the quantisation starts to come in, the high-frequency portions of the image (like edges) start to lose definition. This image has 0.9 bits per pixel.](./media/images/9781119183938-fg0907.png)

![[FIGURE 9-8:](#12_9781119183938-ch09.xhtml#rc09-fig-0008) With a high level of quantisation, only the low-frequency image is really visible, and you can see the boundaries between macroblocks. This image has 0.3 bits per pixel.](./media/images/9781119183938-fg0908.png)

Per-coefficient quantisation is performed by applying a _quantisation matrix_, which can be varied on a frame-by-frame basis to hit the target bit rate. This matrix has the same dimensions as the matrix holding the output from the DCT, and each entry in the quantisation matrix is the level of quantisation for the corresponding coefficient in the DCT output. The DCT value is divided by the quantisation value, and this result is rounded down. This reduces the numbers to a smaller range, and smaller ranges take less space to store.

In general, quantisation usually reduces many of the higher-frequency portions of the DCT to 0. Having a large proportion of the numbers the same makes the next step (entropy coding) efficient.

Figures 9-9, 9-10 and 9-11 show the quantisation matrices that were used in [Figures 9-6](#12_9781119183938-ch09.xhtml#c09-fig-0006), [9-7](#12_9781119183938-ch09.xhtml#c09-fig-0007) and [9-8](#12_9781119183938-ch09.xhtml#c09-fig-0008), respectively. You can extract these from JPEG images (which are very similar to MPEG-1 I frames) on your Raspberry Pi using `djpeg`. First you need to install it with

```
sudo apt-get install libjpeg-progs
```

![[FIGURE 9-9:](#12_9781119183938-ch09.xhtml#rc09-fig-0009) There’s no quantisation, which means that all of the frequency data is kept.](./media/images/9781119183938-fg0909.png)

![[FIGURE 9-10:](#12_9781119183938-ch09.xhtml#rc09-fig-0010) Quite a lot of quantisation, and heavily focused on the higher-frequency portions](./media/images/9781119183938-fg0910.png)

![[FIGURE 9-11:](#12_9781119183938-ch09.xhtml#rc09-fig-0011) Much of the high-frequency data is removed entirely.](./media/images/9781119183938-fg0911.png)

Then you can run the following where `image.jpeg` is the name of the JPEG file:

```
djpeg -verbose -verbose image.jpeg > /dev/null
```

Normally this gives two matrices—one for the luma values and one for the chroma values—but these images are black and white so only have luma values. If you’re testing the output of the Raspberry Pi camera module, you may find that the quantisation matrix is set to not quantise at all (that is, it’s all 1s) unless you specify a quality option in the `raspistill` command (for example, `-q 50`).

As you can see in [Figures 9-10](#12_9781119183938-ch09.xhtml#c09-fig-0010) and [9-11](#12_9781119183938-ch09.xhtml#c09-fig-0011), the quantisation values tend to be roughly equal along trailing diagonals. This is because these lines contain coefficients to which our eyes have roughly the same sensitivity. For example, in [Figure 9-11](#12_9781119183938-ch09.xhtml#c09-fig-0011), the fourth value across the top line corresponds to the same frequency horizontally as the fourth one down on the leftmost line does vertically. One has a quantised coefficient of 80, whereas the other has one of 70; they’re not exactly the same because empirical studies have found that slightly asymmetric matrices yield slightly better perceptual quality at a given bit rate. The two coefficients between them (65 and 70) correspond to DCT basis functions with slightly lower horizontal frequency and slightly higher vertical frequency, and are similar in magnitude.

The matrix of quantised DCT coefficients has to be serialised into a single stream of numbers in order to be stored in a file or transmitted over a network. Usually when serialising a matrix like this, each row is sent one after the other; in this case, however, the final step (entropy coding) is more efficient if the matrix is serialised in a zig-zag pattern.

This zig-zag pattern starts with the coefficient that our eyes are most sensitive to and moves through them in the order of decreasing sensitivity. This should roughly equate to quantisation levels getting higher and higher. As the quantisation level gets higher, more and more of the quantised values will be zero, and this string of zeros is very effectively compressed in the next stage. See Figure 9-12 for an example.

![[FIGURE 9-12:](#12_9781119183938-ch09.xhtml#rc09-fig-0012) The process of quantising and serialising the output from MPEG-1 I frame DCT](./media/images/9781119183938-fg0912.png)

### Using Lossless Encoding Techniques

The final part of the MPEG-1 encoding process applies lossless compression techniques to the quantised coefficients and other data including mode choice flags and motion vectors. There are three methods that MPEG-1 uses to get the file size as small as possible:

- Differential pulse-code modulation (DPCM) - Run-length encoding (RLE) - Huffman coding

Certain parameters, notably DC coefficients and motion vectors, are strongly correlated between successive macroblocks. DPCM exploits this correlation by storing only the difference between the last value and the current value. The differences have a tighter frequency distribution than the values themselves, and so respond better to Huffman coding.

RLE is simply the process of shortening strings of the same value. For example, if a quantised DCT matrix ends with a series of 40 zeros, this could be represented by 40 repetitions of the number zero. However, in RLE, it’s represented by the zero once, and a count of 40 times. This is particularly effective in MPEG encoding because the zigzag ordering of coefficients ensures that this situation happens very frequently, especially at higher quantisation levels.

Huffman coding also removes duplicated data, but it works on sequences of symbols that are repeated at different locations in the data, not simply blocks of repeated identical symbols. Sequences that occur frequently are assigned short binary representations, while those that occur rarely are assigned longer representations. For example, if the text of this chapter were Huffman coded, the encoder might see that the word “encoded” is repeated many times and so replace this with a representation that’s only 1 byte long, saving space. The statistics of the MPEG-1 symbol stream are such that Huffman encoding typically performs very well.

## Changing with the Times

These basic techniques in MPEG-1 still form the basis of modern video compression today, more than 20 years after they were first introduced, that is:

- Colorspace transformation, where incoming data in the RGB colorspace is transformed into the Y’C<sub>b</sub>C<sub>r</sub> colorspace and subsampled to exploit the eye’s differential sensitivity to luma and chroma - Splitting into GOPs comprising I, P and B frames and applying motion compensation to exploit the fact than many frames are similar - A frequency domain transform (DCT or other) to transform the spatial representation into a frequency representation - Quantisation that reduces the amount of data in the DCT coefficient matrix, exploiting the eye’s differential sensitivity to high- and low-frequency data - Entropy coding

However, modern video codecs have developed more efficient ways of performing each of these tasks.

MPEG-1 started the digital video revolution, but some limitations quickly became apparent. It supported only two audio channels (stereo) but not surround sound. Also, it didn’t properly support interlaced video .

---

> [!NOTE]

Interlaced video is a video technique to increase the apparent frame rate of a video. It’s commonly used in broadcast TV.

With these weaknesses corrected, MPEG-2 was the first digital video format to really become popular. Although nearly 20 years old at the time of writing, it still forms the basis of much commercial video compression, such as broadcast digital TV and DVDs. These applications place a premium on quality at reasonable file sizes. The MPEG-2 video compression standard is the same as the ITU’s H.262 standard.

Initially, an MPEG-3 video encoding standard was designed as an extension to MPEG-2. However, many of the proposed techniques were incorporated into MPEG-2 and the formal MPEG-3 designation was retired. The audio standard commonly known as MPEG-3 is, in fact, MPEG-1 layer 3, the most sophisticated of three possible audio encoding schemes specified alongside the MPEG-1 standard.

In parallel with the development of new standards, numerous encoder techniques have been developed that can be used to improve compression or perceptual quality in older standards as well as in newer ones.

Just as our eyes are more sensitive to lower spatial frequencies, they’re also more sensitive to changes in brightness at some levels, particularly in the middle of the brightness range. _Lumi masking_ is the process where the encoder preferentially removes detail from areas of the image that are either very bright or very dark. This can assign more bitstream space to encoding medium-brightness blocks, which means that the frame looks better for a particular bit rate.

Encoders have considerable latitude in how they choose to quantise individual transform coefficients. The straightforward division-with-rounding approach described in the earlier “[Understanding Frequency Transform](#12_9781119183938-ch09.xhtml#c09-sec-0008)” section minimises error, but does not fully take into account the bit rate benefits of choosing small coefficients, which have shorter entropy codes, and of discarding isolated non-zero coefficients that break up long runs of zeros, which are cheap to encode using RLE. Techniques such as uniform or adaptive deadzone and trellis quantisation attempt to capture these benefits by biasing coefficients toward zero. Trellis quantisation, as implemented by the popular x264 encoder, uses a comparatively detailed model of a codec’s RLE and entropy coding schemes to identify quantisation choices that yield significant bit rate savings.

Both lumi-masking and trellis quantisation can be used in older and newer MPEG standards.

### The Latest Standards from MPEG

Just as digital video using MPEG-2 was starting to become popular, the landscape changed when home Internet connections started to become fast enough to download video.

If you’re putting a film on DVD, it really doesn’t matter what size the file is as long as it’s smaller than the capacity of the disc (4.7–9.4GB per side depending on the type of disk). However, when streaming video over the Internet, every megabyte counts. Smaller file sizes mean cheaper storage costs, less buffering for the viewer and lower bandwidth. What’s more, the playback devices for Internet viewing tend to be more powerful than digital TV set-top boxes. This extra processing power can be used to perform more complex decoding.

There are two video compression sections to MPEG-4: parts 2 and 10. Part 2 was the first to become popular, and it introduced many new features such as quarter-pel motion vectors and global motion compensation. The term MPEG-4 is generally used informally to refer to the part 2 standard. Two implementations, Xvid and DivX, were particularly popular in the early days of illegal file sharing because they had small file sizes compared to the MPEG-2 DVDs from which the files were usually ripped. The implementations differed slightly in their implementation of the standard, so the implementations wouldn’t always play the same files.

MPEG didn’t release the entire MPEG-4 suite of standard in one go. MPEG-4 part 2 was released in 1999, whereas part 10 (more commonly known by its equivalent ITU designation, H.264) didn’t emerge for another four years. This meant that hardware had again improved, and part 10 is more complex (with a corresponding increase in compression performance) than any of the preceding standards.

One of the main ways in which H.264 improves on its predecessors is by increasing the flexibility and precision of the motion compensation scheme.

In previous standards, B frames could be predicted off two adjacent reference frames; H.264 increases this to a (probably excessive) maximum of 16 nearby frames, depending on resolution. As with MPEG-4, motion vectors are specified to quarter-pel precision. Earlier we said that MPEG-1 uses bilinear interpolation to calculate values halfway between integer sample positions. With quarter-pel motion vectors, there are three possible locations between any two pixels. It is possible to use the same bilinear interpolation to generate estimated sample values at these locations, but better results are possible with a more advanced interpolation method. Think again of the graph showing the sample values for a single channel along one line of pixels. Using linear interpolation, we said, was the same as drawing a straight line between adjacent samples and reading the value off that. A more effective way would be to try to fit a smooth curve against the line of samples using more than just the two immediately adjacent values, and using this to calculate the intermediate values.

The sub-pel values are calculated in two stages in H.264. Firstly, the half-pel values are calculated using a six-tap filter. It has six taps because it takes into account the value of six nearby samples when calculating the value; in contrast, a bilinear filter has two taps. Although this enables it to calculate the half-pel values more accurately, it can take more time and energy to perform. So once the half-pel values have been calculated, we use linear interpolation to derive quarter-pel values as the average of two adjacent half- or whole-pel values.

Where MPEG-1 performs motion compensation on a whole macroblock at a time, H.264 macroblocks can be split into smaller partitions for motion compensation. These can be as small as 4×4 luma pixels (which corresponds to 2×2 chroma pixels because these have been subsampled). These smaller areas can capture some motion better, though, of course, there is a decreased return because more motion vectors need to be stored should the small partitions be used.

H.264 also allows more efficient entropy coding methods including Context Adaptive Binary Arithmetic Coding (CABAC) and Context Adaptive Variable Length Coding (CAVLC). Both these coding methods exploit the fact that some things are more likely to appear when surrounded by other pieces of data. For example, look at this sentence:

Europe and America are separated by the \*\*\*\*\*\*\*\*\* ocean.

You can probably guess that the missing word is Atlantic. Moreover, whenever you see the word ocean, it’s probably preceded by Atlantic, Pacific, Arctic, Indian or Southern. In these cases, you’re getting some information about what the word is, based on its context. Both CABAC and CAVLC use this sense of context to perform the final, lossless, stage of encoding more efficiently than the Huffman coding used in earlier video compression formats. As always, the trade-off is that these schemes require significantly more processing power to encode and decode.

At high levels of quantisation, DCT-based video compression methods have a tendency to introduce _blocking artefacts_. These appear at the borders of transform blocks (8×8 pixel DCT blocks in the MPEG-1 case) and manifest as sudden step changes in brightness or colour. Prior to the introduction of H.264, some decoders implemented _deblocking filters,_ which are context-aware, low-pass filters that act to reduce blocking artefacts by tweaking the sample values on each side of transform block edges that are deemed to be “blocky”. These filters were not standardised and were generally out-of-loop, which is to say that they were applied immediately prior to displaying a frame and dependent P or B frames would fetch them from the non-deblocked image.

H.264 introduced a sophisticated, standardised, in-loop deblocking filter. This is applied as the last stage of the frame decoding process, generally before the frame is written to memory, so dependent P or B frames now fetch their motion-compensation data from the (hopefully higher quality) deblocked image.

Take a look at Figures 9-13 and 9-14 to see the improvements in I frame compression between MPEG-1 and MPEG-4 part 10. Both of these are compressed at the same bit rate (0.9 bits per pixel). Because these are I frames, the image quality isn’t helped by the improved motion compensation.

![[FIGURE 9-13:](#12_9781119183938-ch09.xhtml#rc09-fig-0013) MPEG-4 part 10 compressed I frame. Notice how there are few artefacts (the slight blurriness is because it’s zoomed in to show detail).](./media/images/9781119183938-fg0913.png)

![[FIGURE 9-14:](#12_9781119183938-ch09.xhtml#rc09-fig-0014) MPEG-1 compressed I frame. At this level of compression, the quantisation errors are significant.](./media/images/9781119183938-fg0914.png)

None of these improvements fundamentally changes the overall shape of the video codec pipeline. In fact, it hasn’t really changed since MPEG-1. However, the improvements do result in a significant increase in compression at the cost of requiring significantly more processing power for both encoding and decoding.

On the Raspberry Pi, VideoCore is capable of doing more or less all the work of video decoding. This is controlled through Open Media Acceleration (OpenMAX), an API that allows programmers to utilise the hardware acceleration in a standard way. Not all video software for the Raspberry Pi makes full use of VideoCore capability, but Raspbian does come with the source code for a simple H.264 player to demonstrate how to use VideoCore.

To test video encoding on your Raspberry Pi, the first thing you need to do is compile the example code. Launch LXTerminal and type the following command:

```
cd /opt/vc/src/hello_pi./rebuild.sh
```

Then you can run the example video by entering this:

```
cd hello_video./hello_video.bin test.h264
```

An H.264 video plays in full-screen mode. The first thing you should notice when the video is finished is that it didn’t use much CPU power (the green graph at the bottom right stayed quite low).

Similarly, you can use OpenMAX to help encode video. You can test this out using the `hello_encode` example program with the following:

```
cd ../hello_encode./hello_encode.bin
```

You may notice that this process hogs the CPU for a few seconds because not all of the encoding functions can be run on the GPU. However, it’s still far faster than CPU-only encoding.

This creates a file called `test.h264`. You can play this file with the hello_video player with:

```
../hello_video/hello_video.bin test.h264
```

### H.265

H.264 is the most advanced video codec in wide use at the time of writing. However, it doesn’t represent the end of the road for video compression. Work has recently finished on the High Efficiency Video Codec (HEVC) standard, generally known by its ITU name, H.265.

The goal of H.265 is to reduce bit rate at constant quality by 50 percent compared to H.264 without significantly increasing the computational complexity of the decoding process.

To achieve this goal, H.265 uses a new structure for storing the information. Instead of macroblocks, it uses coding tree units (CTUs). These fulfil roughly the same role as macroblocks in motion compensation, but they can be much larger (up to 64×64 luma pixels), and are recursively subdivided as needed. Larger CTUs allow graphically simple regions, such as clear blue sky or plain-coloured walls, to be encoded simply, whereas smaller CTUs allow regions with finer detail to be properly captured.

The final ITU H.265 standard was released in April 2013, but is only beginning to see widespread use. One reason for this is the lack of available decoding hardware. Although high-power CPUs like the ones found in modern desktop computers can decode HEVC, lower power devices, such as smartphones or the Raspberry Pi, need the assistance of the GPU; older GPUs aren’t able to decode H.265.

## Motion Search

As we have seen, one of the key ways that encoders compress videos is by finding motion vectors that accurately describe the movement of one block compared to the previous frame, thereby minimising the residual and its associated bitstream requirement.

This begs the question, how do you calculate these motion vectors? In principle—for P frames at least—the process is easy. You take each block and compare it to all the potential locations on the preceding I or P frame (or other eligible frames depending on the compression standard). For each location, you compute the residual and work out the length of bitstream required to encode it, remembering to add the bits required to encode the motion vector: the DPCM and Huffman coding (in the case of MPEG-1) applied to the vector components means that it is generally cheapest to encode a vector very similar to that of the previous macroblock. The location that results in the smallest overall number of bits emitted “wins” and is used to construct the final stream (unless its cost exceeds that of simply encoding an I macroblock).

The problem with this is that it would take far too long to calculate all the differences, so instead encoders use algorithms that cut down the search area in some way, often using a hill-climbing hierarchical approach.

One such option is the diamond search. In this search, nine points in the reference frame are chosen in a diamond pattern around the place the block is located in the frame to be encoded. Then the search step takes place, which is:

- If the centre point has the lowest errors, move on to the final step. Otherwise, centre a new diamond around the point that has the lowest error rate. - Once you have located a diamond where the lowest error rate is in the middle, the final step is to switch to a smaller diamond. This subdivides the centre of the diamond into five sections, and the one with the smallest difference is chosen.

Figure 9-15 shows this algorithm in action. Step 1 shows the starting grid around the point (the circle). In Step 2, the grid moves again. In Step 3, the point with the smallest difference is the centre point, so it moves on to the final step with a smaller grid. This is only performed once in Step 4, and the point with the smallest difference is used.

![[FIGURE 9-15:](#12_9781119183938-ch09.xhtml#rc09-fig-0015) The diamond motion search algorithm. The circle represents the starting point for that step and the hollow shape is the point with the smallest difference.](./media/images/9781119183938-fg0915.png)

You may think that there’s a good chance that this algorithm misses the actual motion, particularly in the case of fast motion, corresponding to many pixels per frame. This is correct, but the point here is not to create a perfect algorithm; the algorithm only needs to be good enough and run quickly enough to be useful. The described diamond search runs quite fast, but it doesn’t always find the optimum motion vectors. Remember that the residual (the difference between the motion compensation result and the actual source frame) is encoded, so even if the motion estimation isn’t perfect, the frame can still have a high image quality; it just needs to include more data in each frame.

When encoding a video, you have to make a choice between the quality of the motion estimation and the amount of time you want the encoding to take. Lower quality motion estimation results in either lower quality videos or larger file sizes (depending on how you set up the encoder), but lower quality motion estimation also results in a correspondingly shorter execution time.

Take a look at Table 9-2 for the difference between encoding time and file size for a video using the `avconv` command on the Raspberry Pi.

<figure> <figcaption>

[Table 9-2](#12_9781119183938-ch09.xhtml#rc09-tbl-0002)  A Comparison of Motion Search Algorithms

</figcaption>

**Motion search algorithm** **Filesize (bytes)** **Time taken to encode (seconds)** ----------------------------------- ---------------------- ------------------------------------ Exhaustive search algorithm (esa) 89961 39 Diamond (dia) 90713 23 Hadamard exhaustive search (tesa) 90004 44

</figure>

The times in [Table 9-2](#12_9781119183938-ch09.xhtml#c09-tbl-0002) are based on a 2-second, 200×200 pixel video recorded on the Raspberry Pi camera module. You may have noticed that this takes far longer than `hello_encode.bin` despite this video being shorter. This shows just how much of a speed-up the GPU gives the encoder.

You can experiment with this yourself using the `avconv` command:

```
sudo apt-get install libav-tools
```

The basic usage is

```
avconv -i inputfile -vcodec libx264 -me_method method-name -crf 15 -g GOPsize outputfilename.mp4
```

where `method-name` is replaced with `dia`, `esa` or `tesa`, and `GOPsize` is replaced with the group of pictures size you want.

At the end of the encoding, the output is various pieces of information, including details of the different frame types. Here’s the section from one of the preceding runs:

```
[libx264 @ 0x8b6360] frame I:3     Avg QP:12.69  size:  3229
[libx264 @ 0x8b6360] frame P:32    Avg QP:15.66  size:  2050
[libx264 @ 0x8b6360] frame B:13    Avg QP:18.11  size:   973
```

This shows you the breakdown in the number of frame types as well as their average size. QP is the quantisation parameter and is used to select a quantisation matrix on a per-frame or even per-macroblock basis; higher QPs mean higher quantisation. Remember that P frames and B frames can use I macroblocks as well as P macroblocks, so this also accounts for some of the size of the P frames in the preceding example.

Try this for yourself to see how different quality settings (the number after `-crf`)—between 0 (very little compression) and 51 (very high compression)—change these numbers.

If you remove `-g GOPsize` the encoder calculates which size it thinks is best. You can use this with different CRF values to see how this changes things.

### Video Quality

We’ve talked about how the encoder can get rid of some of the information to make the file smaller. As more and more information is removed to make the file size smaller, the video quality gets worse—but how much worse?

This is actually a difficult question to answer because the important fact is how good we humans perceive the quality to be. Something like the chroma subsampling would be very obvious to a computer checking for distortion, but it’s difficult for the eye to see. Other things are very obvious to the eye, but have less difference to synthetic quality metrics.

The best way to assess video quality is to get real people to watch video samples and rate the comparative quality. This is the method MPEG uses when comparing different proposals for inclusion in a standard. However, it’s not really practical for most video encoding. Instead, you need a way of estimating the quality computationally. The most common method is the peak signal to noise ratio (PSNR).

PSNR is calculated by comparing what the image should be (the signal) to the difference between the image as it should be and the image as it is displayed after compression (the noise). The error rate is squared, so the PSNR can be calculated with the following equation:

PSNR = 20 \* log<sub>10</sub>(Max/squareroot(MSE))

MSE (which stands for mean squared error) is calculated by taking the difference between the correct and actual values for each pixel, squaring this value and then taking the average across all the pixels. Max is the maximum value that a pixel can take. In most cases, video has 8 bits per colour channel, so this will be 255.

It’s important to realise, though, that PSNR doesn’t correlate exactly to image quality as it’s perceived by the eye.

PSNR looks at the image like a computer does: as a grid of data. The Structural Similarity (SSIM) index is an alternative that attempts to look at the image like a person does. Therefore, it doesn’t look at an image on a pixel-by-pixel basis; instead it compares the image in three different ways: the luminance, the contrast and the structure. Each thing is calculated across the image as a whole and then compared with the results from the frame before compression.

### Processing Power

Playing video is often thought of as a basic function of a computer. After all, even cheap DVD players can do it without a problem. However, it actually entails a huge amount of processing power to perform, and this increases as demand for higher and higher resolution video increases. Many computers use the extra power in their GPUs to help them perform quickly. That’s not the only use of the GPU, as we’ll explore further in the next chapter.
