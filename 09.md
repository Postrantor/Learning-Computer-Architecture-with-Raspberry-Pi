Chapter 5

# Programming

**COMPUTER HARDWARE AND** computer software are traditionally considered two separate continents on Planet Computing. The term “computer architecture” usually means hardware architecture, to the extent that a great many university-level computer architecture books don’t cover programming at all, much less cover the higher level discipline of software architecture and design.

This may be a mistake, especially for pre-university students who have had no formal instruction in either hardware or programming. Separating the study of hardware and software into two disciplines is a convenience only. Anyone who has a serious interest in computing needs to study both. It’s too glib to say that software wouldn’t exist without hardware. The truth is that modern hardware requires software to design and manufacture it, and, more to the point, all computers (which are hardware) require software to make them operative and useful.

Keep in mind that this book is primarily about hardware. Teaching programming using specific languages and tools is best done in separate books, many of which already exist, especially for Python, which is in some sense the “default” language for the Raspberry Pi. What we’re going to do in this chapter is present a broad picture of the _idea_ of programming, with an eye towards giving you a head start on choosing a programming language and an overall approach to the challenge of building your own software.

## Programming from a Height

By now you should understand that computers do what they do by performing a very large number of very small steps in carefully arranged sequences. (Flip back to [Chapter 2](#05_9781119183938-ch02.xhtml), “Recapping Computing,” if this isn’t clear to you.) The steps are called _machine instructions_, and we’ve spoken of them informally all along. They are the “atoms” of a computer program, and cannot be broken down into smaller units of action (see [Chapter 4](#07_9781119183938-ch04.xhtml)).

What we call _computer programming_ is the process of writing and arranging these steps, verifying that they do what we need them to do, and keeping them current over time as those needs evolve. These three components of the programming process are called _coding_, _testing_ and _maintenance_.

Prior to coding, there has to be a design stage. Writing program code off the top of your head (and observing the consequent error messages) is useful while you’re learning a new programming language, but long-term it’s a losing strategy for writing any sort of software that must do a real job over a period of time. Computer programs of any significant size must be designed before the programmer writes the first of those many steps. Different people or groups may do the design work versus the programming work, especially for large software systems that span different computers across networks.

Software design is a separate, necessary discipline on which programming depends. For the sort of simple programs you may write while you’re first learning programming, the design step may seem almost trivial. For larger systems, design may become the toughest challenge you’ll face during the entire project, and inadequate design will likely doom the project to failure.

### The Software Development Process

Irrespective of what programming language or tools you use, the process of software development follows a pretty consistent map, which is shown in Figure 5-1. It begins with an idea that solves some sort of problem. An idea is just an idea; once you begin fleshing it out and taking notes you’ve already stepped off square one and have begun designing your program.

![[FIGURE 5-1:](#08_9781119183938-ch05.xhtml#rc05-fig-0001) A map of the software development process](./media/images/9781119183938-fg0501.png)

With some sort of design in hand (and there are a multitude of ways of performing software design) you sit down in front of your programming tools, open an editor window and begin writing actual program code. Although purists frown on the notion, it’s true that the design and coding stages are not completely distinct. It’s in the nature of the creative process that making an idea concrete generates not only insights about the idea but also new ideas. Especially while you’re still building your programming skills, coding may cause you to realise that something in your design won’t work or doesn’t serve the mission of the project. Going back to the design process temporarily isn’t exactly “following the map” but it may keep the entire project from going off the rails later on, leaving you with hundreds, thousands or (yes, it happens!) tens of thousands of lines of essentially useless code.

At some point you’ll have one or more files containing program code that represent a working program. This is called your _source code_. The next step is to turn your programming language loose on it as you build an executable program from the textual code files that you’ve written in your editor. The term “build” contains one or more steps that depend on the programming language and toolset that you’re using. For some languages, like Python, much of the build process happens “behind the scenes”, whereas for others, like C, you are required to explicitly invoke tools such as compilers and linkers (which are described later in the “[High-Level Languages](#08_9781119183938-ch05.xhtml#c05-sec-0007)” section). For now, think of it this way: the build process crunches your code and either gives it a (qualified) clean bill of health or presents you with a list of compile-time errors.

---

> [!NOTE]

A _compile-time error_ is something in your code that prevents your toolset from creating an executable program. All programming languages have _syntax_; that is, a set of rules about what program elements are called and how they’re put together in your source code files. Violate that syntax, and you get an error. In statically typed languages, some errors will be type mismatches, which means a conflict between the type of data you’ve defined (text, numbers, etc.) and what your code is trying to do with it. Dynamically typed languages give you more leeway at compile time: type mismatches make themselves known at runtime, when the offending statement is executed. This is called a _runtime error_.

Error messages provide hints as to what you did wrong, and a line in a source code text file represents the point at which your toolset noticed the error. This is not necessarily where the error itself lies! You’ll have to think a little about what you wrote and how it adheres to or violates your language’s syntax or type rules. While you’re learning, you’ll doubtless spend time digging through a syntax chart or reference on your chosen language. Once you’ve internalised the language, it will take a lot less time and effort to spot errors.

Fixing errors requires you to return to the text editor, change the problem source code and save a new version of the file or files. After that, you build the program again (and probably again, for several or many more iterations) until your toolset no longer gives you a list of errors. Done!

Well, not exactly done. Not even close. Once you have a program that can be run, you have to run it and see what it does. With that you move to the testing stage, during which you evaluate your program’s behaviour against what you’ve set out in your design. The program may run but then crash, and if you’re fortunate your toolset will give you a run-time error providing some hints as to why. Even if it runs, the program may do unexpected things. This sort of problem is known as a _bug_.

---

> [!NOTE]

The first person to use the term bug in the context of computing was Admiral Grace Hopper of the United States Navy, who found a dead moth stuck in a relay of an early electromechanical computer in 1947. Although technically a hardware rather than a software problem, Admiral Hopper’s moth kept her program from running correctly, and she said she had to “debug” the computer to make things work again. She taped the moth itself to her log book, where it remains to this day at the Smithsonian Institution. Since then, anything that keeps a program from running correctly is called a bug.

Debugging software is an art and a discipline all to itself. Identifying a bug does not imply understanding what you actually did wrong in your source code. Working out how to fix a bug takes some study and sometimes a walk around the block to clear your head. Once you’ve figured out the problem (or _think_ you’ve figured out the problem) you again return to your code editor, make your changes and then rebuild the program.

Getting the bugs out of a program can take longer than writing the program itself, especially while you’re still learning the game. There will come a time when you realise that your list of bugs has all been repaired, and the program is finally doing useful things in the ways that you had planned. Now you’re done!

### Waterfall vs. Spiral vs. Agile

But you’re still not really done. One of the tenets of modern software development is that software is rarely if ever “done” in the sense that nothing more needs to be changed, now or ever. The programming process is inherently _iterative_—that is, it’s a series of feedback loops that take into account a program’s design goals, its bug list, and new insights about how what needs to be done could be done better.

Programming wasn’t always like this. In its early years, the software development process was often conceptualised as a sort of construction task like erecting an office building, in which the entire blueprint must be complete, fully understood and costed before the first shovel of dirt is thrown. In this world, user requirements are gathered and a detailed design document for a piece of software that meets these requirements is produced; the design is implemented in code and tested; all known bugs are fixed; and then the implementation phase is deemed complete and the project is placed into an ongoing maintenance mode. This linear sequence of steps is now called the _waterfall model_ because it proceeds inexorably from the top to the bottom. In the model’s purest incarnation, user requirements cannot be changed after the design document is underway, and the design document cannot be changed after coding has begun. If the users do not understand their own needs, or if they cannot communicate their needs to the designers, what they get in the end might not help them or, in some cases, can be worse than nothing at all.

After recognising the shortcomings of the waterfall model, software designers began to explore something a little more like what’s shown in [Figure 5-1](#08_9781119183938-ch05.xhtml#c05-fig-0001). The insight was that, realistically, many projects cannot be fully understood by _anyone_ before at least some code has been written. Programmers take the user requirements and create a simple, feature-limited prototype and let the users play with it. Based on user feedback, the programmers then expand the prototype or in some cases scrap it entirely and begin again, correcting initial misunderstandings even if they were fundamental to the design. After users see their requirements implemented in software, they will as often as not update their requirements to reflect the insights that playing with a prototype have triggered. The requirements, design and coding steps are visited not once but many times, going around in a loop much like that in [Figure 5-1](#08_9781119183938-ch05.xhtml#c05-fig-0001). The prototype grows by increments; these development methodologies, of which Barry Boehm’s spiral model is the best-known example, are therefore known as _incremental models_. Figure 5-2 shows the waterfall and spiral models side by side.

![[FIGURE 5-2:](#08_9781119183938-ch05.xhtml#rc05-fig-0002) Waterfall model vs. spiral model](./media/images/9781119183938-fg0502.png)

Although traditional incremental models generally represent an improvement on the waterfall, they are heavyweight, with an emphasis on up-front planning and top-down management of the development process. From the mid-1990s onwards, a variety of lightweight incremental models emerged, which emphasised flexibility and responsiveness. These approaches came to be known as _agile software development_, or simply _agile_. The (commendably brief) Agile Manifesto, issued in 2001, summarises the goals of agile software development:

<aside epub:type="sidebar">

---

We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:

- **Individuals and interactions** over processes and tools - **Working software** over comprehensive documentation - **Customer collaboration** over contract negotiation - **Responding to change** over following a plan

That is, while there is value in the items on the right, we value the items on the left \[the bolded items\] more.

<figure>

------------------- ---------------- ------------------ Kent Beck James Grenning Robert C. Martin Mike Beedle Jim Highsmith Steve Mellor Arie van Bennekum Andrew Hunt Ken Schwaber Alistair Cockburn Ron Jeffries Jeff Sutherland Ward Cunningham Jon Kern Dave Thomas Martin Fowler Brian Marick ------------------- ---------------- ------------------

© 2001, the above authors. This declaration may be freely copied in any form, but only in its entirety through this notice.

</figure>

---

Agile development is a “big picture” strategy, and the fine details of how the work is actually done may vary between teams and projects. Common agile practices include:

- **Timeboxing:** A large project is divided into discrete smaller projects of fixed duration, each with its own schedule and deliverables, simplifying short-term time management. - **Test-driven development:** A developer first produces a unit test for a new feature, and then writes the simplest good-quality implementation that passes the test. - **Pair programming:** Two programmers (the driver and the observer) work together at a single terminal, providing continuous code review and a separation between the strategic and tactical aspects of programming. - **Frequent or continuous integration:** Developers regularly commit their changes to the shared code base, avoiding “integration hell”. - **Frequent stakeholder interaction:** Regular releases are made and feedback sought, providing early notice of requirement changes. - **Scrum meetings:** Short daily team meetings promote team cohesion and provide a forum for team members to share progress, plans and impediments.

The following are two of the best-known agile methodologies:

- **Scrum:** A framework in which development proceeds as a sequence of sprints, each allocated a certain limited amount of time. (This is called _timeboxing_.) At the start of each sprint, outstanding tasks from the project backlog are prioritised, and a subset is selected to form the sprint backlog. Daily scrum meetings are held during the sprint. At the end of each sprint, the product should be releasable (albeit incomplete if there are tasks remaining on the project backlog). - **Extreme programming:** A variety of practices—including pair programming, and continuous integration, testing and deployment—that are, in a sense, “extreme” variants of accepted best practices. The development process consists of four mutually supporting activities: coding, testing, listening (that is, gathering user feedback) and designing. The overriding goal is to remain responsive to requirement changes.

One way to think about agile development is that it does not so much design software as evolve it, through continuous feedback from users triggering continuous improvement by programmers. In a way, the design emerges from experience. Although old-school programmers sometimes consider the agile process chaotic, across a range of problem domains it appears to produce better software faster than either the waterfall or traditional incremental models.

### Programming in Binary

Programming is an old, hard game. In the very beginning, there were no tools, and programmers wrote sequences of machine instructions as binary numbers. These could then be loaded from paper tape or punch cards or, particularly in the case of “bootstrap” startup code, written into memory manually through toggle switches on the CPU cabinet front panel. An “up” toggle indicated a binary 1, and a “down” toggle indicated a binary 0. Programmers would flip the row of toggles until it reflected a binary machine instruction, and then push a button to store the binary pattern in memory. Then they did it again, flipping switches and storing the next instruction, and so on. The rows of switches you may have seen in movies on the control panels of gigantic old computers were for exactly this purpose. Front panel switches lingered until the late 1970s, particularly on cost-sensitive hobbyist computer systems like the Altair 8800, but better tools have long since made them unnecessary.

Writing a program in binary was done by first writing a description of a machine instruction, and then looking up the binary pattern for that instruction. For simple programs on machines with simple instructions sets, this was time-consuming but not terribly difficult. The manufacturers of early single-chip central processing units (CPUs) like the Motorola 6800 and Zilog Z80 would publish reference cards with tables showing the hex encoding for all instructions in common forms. The need to write more complex programs, on CPUs with more complex instruction sets, quickly turned binary programming into slow, painful drudgery that cost far more in time and trouble than it was worth.

### Assembly Language and Mnemonics

As early computers came to be used by a broader audience of academic and commercial users, simple tools were developed to automate the mechanical aspects of the programming process. As described in [Chapter 4](#07_9781119183938-ch04.xhtml), a typical machine instruction consists of an _opcode_ (literally an operation code, describing what sort of operation the instruction performs) and zero or more _operands_ (which define where a data processing instruction finds its input data and stores its result, or where a branch instruction branches to). If you assign a short, notionally meaningful name called a _mnemonic_ to each opcode, and come up with a textual convention for specifying the operands, code becomes much easier to write. For example, a machine instruction that moves data from one place in the computer to another might use “mov” as the mnemonic for its opcode.

Following is a short sequence of machine instructions expressed as human-readable opcode mnemonics and operands. The mnemonics are on the left, and the operands are to the right of the mnemonics. There are several kinds of operands, including numbers, memory addresses, register names and qualifiers of various sorts. Any single opcode may have more than one operand, or none at all.

```
	mov edx,edi
	cld
	repne scasb
	jnz Error
	mov byte [edi-1],10
	sub edi,edx
```

A software utility can translate the mnemonics and operand descriptions directly into binary, saving the programmer the work of doing the translation manually. This utility is called an _assembler_, as it does the work of assembling a binary machine instruction from information given in the mnemonic and operand descriptions; the textual description of a machine code program is called _assembly language_. ([Chapter 4](#07_9781119183938-ch04.xhtml) briefly mentioned assembly language.)

Although nominally human-readable, assembly language is terse and reveals little about what the instructions are intended to accomplish. Programmers often include comments in their assembly language source code files to describe briefly an instruction’s purpose:

```
	mov edx,edi            ;Copy starting address into EDX
	cld                    ;Set search direction to up-memory
	repne scasb            ;Search for null (0 char) in string at EDI
	jnz Error              ;REPNE SCASB ended without finding null
	mov byte [edi-1],10    ;Store an EOL where the NUL used to be
	sub edi,edx            ;Subtract position of NUL from start address
```

> [!NOTE]
> that comments describe not only the instruction but also its role within the program. In spite of any marketing hype, _no computer language is self-explanatory_. All computer languages allow comments, and you will always need comments to remind yourself what a given line of code is doing in the larger scheme of things. This is especially true after you’ve set a program aside long enough that its details are no longer fresh in your mind.

### High-Level Languages

Assembly language still exists, and you can write assembly language programs for the Raspberry Pi with the GNU tools that come free with the Raspbian operating system and all other flavours of Linux. We’ll have more on this tool set later on, in the section entitled “[A Tour of the GNU Compiler Collection Toolset](#08_9781119183938-ch05.xhtml#c05-sec-0056).” Unless you’re trying to eke every last drop of performance out of a system, however, it’s a lot more work than it needs to be. Assembly language describes the behaviour of a program at a low level of abstraction: one line of assembly language is translated by the assembler directly to one single machine instruction. Early on, computer scientists developed more sophisticated, expressive languages in which one textual command (generally called a _statement_) corresponded to a sequence of machine instructions. Such languages were called high-level languages because they allowed the programmer to describe the desired behaviour of a program at a higher level of abstraction than the very literal assembly language could.

---

> [!NOTE]

The term GNU refers to a large group of free and open-source software (FOSS) products, from assemblers to compilers to the Linux operating system itself, which is formally named GNU Linux. The Term “GNU” is an acronym, for “GNU’s Not Unix,” which is how the computer scientist Richard Stallman meant to indicate that he was writing an operating system called GNU that was similar to Unix, but not a literal clone.

The earliest high-level language to see wide use was FORTRAN, developed at IBM by a team led by John Backus in the early 1950s, and made available to IBM’s customers in 1957. FORTRAN (from FORmula TRANslator) reduced the number of statements necessary in a program by a factor of 20. The classic “Hello, world” program written in early FORTRAN was simplicity itself:

```
	PRINT *, "Hello, world!"
	END
```

In addition to the obvious benefit of reducing the textual size of a program’s source code and making it easier to read, FORTRAN hid the details of the workings of the computer from the programmer. Programmers did not need to know how the CPU controlled the various mechanisms of the system printer, if all they wanted to do was print a line of text. The word PRINT was translated into a middling number of machine instructions that moved text across a cable to the printer and told the printer to print that text to paper. Furthermore, if the machine instructions for printing text to paper were always the same, it was a waste of effort to include them in every single program. The machine instructions for printing were necessary, but they were stored in a separate file. The utility that translated FORTRAN statements to machine instructions compiled the machine instructions from several sources (some of which would later be called libraries) to form the final executable program. The translator program was thus called a compiler.

FORTRAN was developed and used primarily for mathematical and scientific computing. It was quickly followed by COBOL, created by a group led by Admiral Grace Hopper (of “bug” fame) in 1960. Hopper’s COmmon Business Oriented Language went on to become one of the most-used languages in the history of computing. The minimal “Hello, world!” in COBOL is a little more complex than in FORTRAN:

```
	IDENTIFICATION DIVISION.
	PROGRAM-ID.HELLO-WORLD.
	PROCEDURE DIVISION.
	DISPLAY "Hello, world!"
	    STOP RUN.
```

One of COBOL’s goals was to make program source code easier to read. It strove to put everything right there in front of the programmer in plain language. Why? A fair bit of long-horizon thinking went into COBOL, including the insight that long-term use of COBOL programs would require maintenance by different programmers over time, each of whom would have to learn how a program worked so it could be fixed or extended. There was thus value in making COBOL programs as easy to understand as possible. Long-horizon thinking definitely worked, and COBOL remained in common use on mainframe computers (that is, large systems designed for centralised use) for almost 40 years. COBOL still sees occasional use on legacy mainframe systems.

Prior to the mid-1960s, computers were _batch-oriented_ systems. This means that programmers wrote their programs on paper, entered them to a stack of Hollerith punch cards, and handed the cards to the technicians who operated the mainframe systems in that era. (Figure 5-3 shows a punch card containing a FORTRAN statement.) The technicians would queue up stacks of cards, and drop them into card readers when a stack’s turn came. The card readers would read the cards and submit the code they contained to be compiled and then executed on the mainframe. The mainframe would either print a list of compiler errors or (if the program had compiled correctly) the program’s results. The printout would be stored with the stack of punch cards and handed back to the programmer some time later, depending on how busy the mainframe was and how many stacks were waiting their turn.

![[FIGURE 5-3:](#08_9781119183938-ch05.xhtml#rc05-fig-0003) A punch card from a 1970s FORTRAN program](./media/images/9781119183938-fg0503.png)

By the mid-1960s, the price of computers, printers and card punches was falling to the point where universities and even the occasional secondary school could afford them. Terminals could be placed outside the “glass walls” of the computer room itself, allowing people other than technicians to submit programs. At first, these terminals were Teletype machines or IBM terminals incorporating their Selectric printing technology. The Teletypes could punch and read paper tape, and many of the IBM Selectric terminals had card readers attached. Dozens of terminals could be attached to a single mainframe computer through a mechanism called _time sharing_, in which the mainframe would give each terminal a little slice of time to work in round-robin style. Each slice might be a fraction of a second, but that was enough time to read a keystroke or print a character. Unless the system got too busy, programmers sitting at the terminals had a convincing illusion that they had the entire machine to themselves.

Selectric terminals with card readers were still used mostly for submitting batch jobs to mainframes, but the presence of keyboards allowed something new: interactive computing. A programmer could type a sequence of lines comprising a simple program, and then submit them for immediate compilation and execution, without having to use punch cards. On a good time-sharing system the response time was almost immediate.

In 1964, two researchers at Dartmouth College, John Kemeny and Thomas Kurtz, designed a programming language specifically for use by students at interactive terminals. Their Beginner’s All-Purpose Symbolic Instruction Code (BASIC) language owed a lot to FORTRAN and could be used for many of the same things. A BASIC program could consist of a single line, which reduced the “Hello, world!” test program to something close to a minimum:

```
	10 PRINT “HELLO, WORLD!”
```

BASIC grew popular at universities, and popularity became ubiquity when personal computers appeared in the mid-1970s. BASIC was easy to implement, even on very simple computers, and easy to learn. Through the end of the 1970s and into the early 1980s, BASIC was often the only language available to personal computer owners. IBM even put a version of BASIC in read-only memory (ROM) on its seminal IBM PC in 1981. It may still be true that more people have been introduced to programming through BASIC than any other single language.

### Après BASIC, Le Deluge

FORTRAN, COBOL and BASIC represent the deep roots of three cultures within computing: scientific, business and educational. They were not the only programming languages within those cultures. Thousands of programming languages have been designed and tried, nearly all of them now forgotten or used only by small groups of diehard enthusiasts and preservationists.

These were not wasted efforts. Most languages are designed around a specific idea, often a new take on an existing idea and sometimes a new idea entirely. Here are a few early examples:

- Lisp (from LISt Processor) appeared at MIT in 1958, to explore the use of lambda calculus (a mathematical mechanism for expressing computation in terms of functions), recursion and tree-structured data. - Pascal was created by Swiss researcher Niklaus Wirth in 1970 to explore structured programming and data structures. Wirth later created the similar languages Modula-2 and Oberon to explore his take on modular programming. - In 1972, Bell Labs computer scientist Dennis Ritchie defined the C language (so named because it replaced the now-vanished B language, which in turn was based on Martin Richards’ BCPL, which happily is available on the Raspberry Pi) as a sort of CPU-independent higher-level assembly language. A key motivator for C was to allow easy implementation of the Unix operating system on different hardware architectures, and it remains a popular language for system-level programming. The Linux kernel used on the Raspberry Pi is written almost entirely in C. - Researchers at Xerox’s PARC research lab developed the Smalltalk language during their exploration of object-oriented programming (OOP) concepts. (Read more about OOP in the section entitled “[Object-Oriented Programming](#08_9781119183938-ch05.xhtml#c05-sec-0051)”.) First released in 1980, Smalltalk lives on today mostly through an open-source implementation called Squeak. Squeak may be run on the Raspberry Pi.

The insight to be taken from this is that different challenges require different approaches and, more fundamentally, that you have to try things to see what works. Computer science, like all science, builds on and sometimes abandons earlier knowledge. All languages in use today descend from earlier languages and earlier, simpler versions of themselves. C++ and Objective C are very nearly supersets of C. Pascal in 2014 draws on Wirth’s later languages, as well as FORTRAN and C. Ada was developed as a rigorously robust version of Pascal.

If you intend to be a programming enthusiast, develop the habit of experimenting with as many different computer languages as you can. Being multilingual in programming languages has another, more subtle benefit: you’ll be better able to identify the common ideas used across languages, which makes learning new languages in the future even easier.

### Programming Terminology

Before we go on, it may be helpful to sketch out what a typical program looks like conceptually. We can’t cover all current terminology in one chapter in one book, just as we can’t explain any particular programming language in detail. Instead, our goal is to define a few terms that we’re going to use for the rest of this chapter (and elsewhere in this book). A word of caution: much of what we present here relates specifically to imperative programming languages such as C and Python, which model computation as a sequence of discrete steps that modify state. Functional programming languages, such as Haskell, model computation in terms of functions, and are beyond the scope of this chapter. In Figure 5-4 we’ve sketched out a simple and very generic computer program and its most important components. There are a lot of details that will have to wait until later. Objects, for example, are vital in modern programming, but they don’t summarise well in 25 words or less.

![[FIGURE 5-4:](#08_9781119183938-ch05.xhtml#rc05-fig-0004) Fundamental programming terminology](./media/images/9781119183938-fg0504.png)

Here are the concepts you need to be familiar with right now:

- **Variable:** A named storage location whose value may change during execution. In contrast, a constant is a named or unnamed value that cannot be changed during execution.
- **Expressions:** These combine the values of one or more variables and constants using operators to compute a result. In the expression `a+b*4`, `a` and `b` may be either variables or constants (depending on context), `4` is a constant, and `+` and `*` are operators.
- **Statements:** Sequential units of action. The simplest example in most languages is an assignment of the result of an expression to a variable; more complex statements can be built by concatenating together simpler statements, or by using conditional and looping constructs like `if`</code> and `while`.
- **Functions (sometimes called procedures or subroutines):** Named blocks of code that may or may not return a value. Variables that are defined within a function are only accessible from inside the function and are said to be _local_ to it. Local variables are generally stored in the CPU register file or on a stack; the stack also stores function return addresses and preserves values for which there is no room in the register file. A function can call another function, meaning that the flow of control takes a temporary detour into the function, returning when it has finished its work.

  Variables that are defined outside any function are said to be _global_ and are accessible from (almost) anywhere.

  Some languages, including C, require all statements to be inside a function. The `main` function, which is called by the system when execution starts, marks the entry point to the program. Other languages, including Python, allow statements outside functions; execution starts with the first such statement in the program file.
- **Arguments:** Values passed to a function from its caller. Parameters are special-purpose local variables that receive the argument values when execution of the function begins. In this Python example:

  `def foo(a, b, c): ` return a\*b+c
  ``

  `print foo(1, 2, 3) ` a `, ` b `and` c `are parameters, whereas` 1 `, ` 2 `and` 3` are arguments.
- **Heap:** A pool of memory where programs may allocate memory to store arbitrary-sized data items. _Pointers_ are values that describe the location of data in the heap, generally as a memory address.

## How Native-Code Compilers Work

The job of a native-code compiler is to take a source code file written in a high-level language and generate an equivalent object code file composed of binary machine instructions. (Do not confuse the terms “object code” and “object,” as used in OOP. The two are unrelated.)

Compilers process their input in several steps or passes. Although object code is the ultimate goal, the compiler may write one or more other files to disk along the way, and may delete such temporary files when they’re no longer needed.

The compilation process can be broken down into the following steps:

- Preprocessing (optional) - Lexical analysis - Parsing - Semantic analysis - Intermediate code generation - Optimisation - Target code generation

---

> [!NOTE]

Many of the preceding steps (particularly the first few) are common to both native-code and bytecode compilers, which are covered later in this chapter in the “[Bytecode Interpreted Languages](#08_9781119183938-ch05.xhtml#c05-sec-0028)” section and the sections that follow it; we’ll refer back to this section during that discussion.

Let’s look at each step in a little more detail. As we do, keep in mind that we’re not describing any single compiler product, and all compilers handle compilation a little differently. Some compilers simplify the process by combining two or more passes into a single pass.

### Preprocessing

Languages that incorporate a preprocessing pass, including C, perform a stage of text-based manipulation of the incoming source code before presenting it to the compiler proper. The C preprocessor performs several tasks:

- **Removing comments:** All text enclosed by comment delimiters (or in some other way marked as comments) is removed because it’s for the sake of humans reading the source code and is of no use to the compiler. There are some exceptions in certain languages that place instructions to the compiler within specially marked comment blocks. How those are handled is both language and compiler dependent. - **Defining and expanding macros:** Object-like macros provide a way to define constants. You might define a macro called `PI` to be `3.14159`; the preprocessor replaces each occurrence of `PI` in the source code with the literal `3.14159`. Function-like macros provide a way to define simple inline functions. You might define a macro called `RADTODEG(x)` to be `((x)*180/PI)`. The preprocessor replaces an occurrence of `RADTODEG(a+b)` in the source code with `((a+b)*180/3.14159)`. - **Conditional compiling:** Sections of code can be conditionally excluded from compilation. This is often used to remove debugging code from release builds of software or to change behaviour depending on the target platform. - **Including files:** The contents of other files can be incorporated wholesale into the source code. A C example is the `stdio.h` include file, which defines commonly used C input and output functions.

### Lexical Analysis

During the lexical analysis stage, a part of the compiler called the _lexer_ scans the stream of characters making up the preprocessed source code and identifies all the various language features in the text. These include reserved words (also called keywords) like `break`, `begin`, and `typedef`, identifiers like `foo` and `bar`, symbols like `+` and `<<`, string literals like “`foo”` and numeric literals like `5` or `3.14159`. The lexer emits a stream of tokens, one for each keyword, identifier, symbol or literal. Any text that can’t be identified as a token understood by the compiler is flagged as a compilation error.

---

> [!NOTE]

You will see the identifiers “foo,” “bar,” “bas,” and perhaps a few others come up in code examples within programming tutorials. These are called _metasyntactic_ identifiers because they’re used while describing programming language syntax in tutorials and demonstrations of language features. Metasyntactic identifiers are not treated specially by compilers and are used by convention among programmers, specifically programmers with roots in Unix and C.

The stream of tokens from the lexer is then scanned by the parser, which checks to see if the tokens follow the structural rules of the language. The lexer identifies tokens individually; the parser makes sure the tokens are arranged in a legal fashion. A `do` keyword must have a matching `while` keyword. An opening brace must have a closing brace, and so on, for the full description of a language’s syntax. Any deviation from that syntax is flagged as a compilation error. The output of the parser is a structure called an _abstract syntax tree_ (AST), which represents the structure of the program. The AST is directly analogous to a sentence diagram for a natural language that identifies a sentence’s subject, verb, object and so on.

### Semantic Analysis

During semantic analysis, the compiler checks the AST to be sure that the syntactically correct program is meaningful. Much of this work involves creating a symbol table of named items in the program, and then checking whether variables and constants of supported data types (numeric, text, Boolean, and so on) are used together in ways that make sense. A statement written in a statically typed language that adds a Boolean value to a character might well be correct in terms of syntax:

```
	junk = true + ‘a’;
```

However, what does it mean to add `true` to `‘a’`? Nothing, of course! Although syntactically correct, the statement is semantically meaningless, and the compiler will flag it as a type mismatch error. Syntactically correct but semantically meaningless sentences appear in natural languages too: Noam Chomsky famously offered “Colourless green ideas sleep furiously” as an example of a syntactically valid English sentence that is semantically meaningless.

Keep it straight in your head: Syntax is about _structure_. Semantics is about _meaning_.

### Intermediate Code Generation

After the compiler verifies that the program is both syntactically correct and semantically meaningful, it is able to begin generating intermediate code. Using the parse tree as a guide, the compiler creates a linear sequence of instructions that expresses the logic of the program. These instructions are not generally the native machine instructions of the target CPU architecture. Instead, they are a sort of “artificial” instruction set belonging to a _virtual machine_ (VM) that acts as an “ideal” CPU that is a notch higher in abstraction than a real, silicon CPU. For example, a VM may have a great many registers in its definition—and sometimes, as many registers as the logic of the program calls for. No CPU has hundreds of registers, so a later pass has to rewrite the intermediate code to attempt to fit those “virtual registers” into the limited register set of the real CPU, spilling those that don’t fit to memory. This process is known as _register allocation._

### Optimisation

The intermediate code’s primary role is to simplify the implementation of one or more _optimisation_ passes. During optimisation, the compiler looks for ways to eliminate code duplication and rearrange intermediate code instructions to make the program more compact and faster to execute. The development of optimisation techniques is an area of ongoing research in both the academic and commercial domains.

### Target Code Generation

With the creation of an optimised intermediate code file, we reach a fork in the road. Up to this point, the compilation process is close to the same, whether the compiler is a native code compiler or a bytecode compiler, and we’ll pick up the discussion again in the next section on bytecode languages. The next, and final, step in native code compilation is _target code generation_. During this step, the intermediate code is converted to a sequence of native machine instructions that can execute on a specific CPU.

But which CPU? A compiler is not limited to creating code for the machine on which the compiler is running: a compiler running on an Intel CPU can be configured to generate code for the one of the ARM instruction set architectures (ISAs), and vice versa. This is called _cross-compilation_. A compiler is hosted on a specific CPU, which means that it is a native-code program compiled to run on that CPU. However, it may generate code that targets any CPU for which the compiler incorporates a code generator. Cross-compilation is especially useful for the creation of software to run on low-power embedded systems that don’t contain enough memory or disk storage to run the compiler itself. In your early work with the Raspberry Pi you’ll probably write programs and compile them right on the Raspberry Pi system itself. Many people who use the board as an embedded system develop code on Intel PCs by using a compiler that is hosted on Intel-based Windows or Linux and targets the ARMv6 ISA, which includes the ARM11 CPU . The generated code is almost always operating system-specific as well.

With the creation of a native code object file, the compilation process is complete.

---

> [!NOTE]

A _platform_ is the combination of a specific CPU running a specific operating system. An Intel CPU running Microsoft Windows is a platform. (It’s commonly called “Wintel.”) An Intel CPU running Linux is an entirely separate platform, as is an ARMv6 CPU running Linux. The output of a cross-compilation operation is generally specified as being for a specific platform.

### Compiling C: A Concrete Example

Let’s take a look at the various stages involved in compiling a simple function, written in C. This section requires close attention, and perhaps a little experience in the C language itself.

The example function takes three integer arguments `a`, `b` and `c`, and a pointer to an area of memory, `d`. It writes the ten integers `b*c`, `a+b*c`, `2*a+b*c` … `9*a+b*c` into memory, starting at address `d`. The number of integers written can be changed at compile time by adjusting the constant `COUNT`, which is set using the C preprocessor directive `#define`:

```
	#define COUNT 10
	` 

	void foo(int a, int b, int c, int *d)
	{
	  int i = 0;
	  do {
	   d[i++] = i * a + b * c;// fill in table
	  } while (i < COUNT);
	}
```

#### Preprocessor

The preprocessor discards the comments, and replaces the use of the macro `COUNT` with its value, 10. Few modern languages have preprocessors; in this case, constants and inline functions take the place of macros, and comments would be discarded by the lexer:

```
	void foo(int a, int b, int c, int *d)
	{
	  int i = 0;
	  do {
	   d[i++] = i * a + b * c;
	  } while (i < 10);
	}
```

#### Lexer

The lexer analyses the character stream that makes up the program, and groups characters into tokens. Each token may be one or more characters in length, and represents a reserved word, an identifier (shown as a double-outlined box in Figure 5-5), a symbol or a literal. Whitespace is not syntactically meaningful in C, and so is discarded from the token stream at this stage.

![[FIGURE 5-5:](#08_9781119183938-ch05.xhtml#rc05-fig-0005) Tokens generated by a C compiler’s lexer](./media/images/9781119183938-fg0505.png)

#### Parser

The parser attempts to build an AST out of the stream of tokens from the lexer. It is powered by a set of rules, often expressed in a descriptive notation called Backus-Naur Form (BNF). BNF is perhaps the most-used metasyntactic notation system in computer science. It abstracts the structure of a programming language into a set of rules called a _grammar_. A grammar precisely describes the syntax of a programming language and can be used to determine if a given program is syntactically correct. The standard GNU utility, bison (derived from the older UNIX tool yacc—bison is GNU yacc), can automatically generate a parser for a programming language, given a BNF description. A selection of BNF grammars for various common programming languages may be found here: [`www.thefreecountry.com/sourcecode/grammars.shtml`](http://www.thefreecountry.com/sourcecode/grammars.shtml).

As an example, imagine a simple language that consists only of expressions containing multiplication, addition and identifiers. This language would have three rules, which might appear in the input file to bison in roughly this form, using BNF:

```
	add_expr  : mul_expr         { $$ = $0; }
	   | add_expr ‘+’ mul_expr;   { $$ = ADD_EXPR($0, $2); }
	   ;
	mul_expr  : identifier             { $$ = $0; }
	   | mul_expr ‘*’ identifier; { $$ = MUL_EXPR($0, $2); }
	   ;
	identifier : ID               { $$ = $0; }
	   ;
```

Each rule has three parts:

- A _name_ (in this case `add_expr`, `mul_expr` or `identifier`). - One or more _productions_. A production describes something you might see in the token stream that this rule will match. - For each production, an _action_; this is often used to create a node in the AST as a result of matching a rule. In a yacc grammar, actions can return values by assigning to the pseudovariable `$$`, and make use of values returned by the rule’s children (represented by pseudovariables `$0`, `$1` and so on).

---

> [!NOTE]

A _pseudovariable_ is a sort of placeholder in a grammar rule. It tells us where a value may be substituted for the pseudovariable. Pseudovariables keep a rule abstract and independent of any particular type, value or values.

Our language description says that a valid `mul_expr` can be either an identifier, like “`a`”, or another (shorter) valid `mul_expr` followed by a “`*`”, followed by an identifier. So “`a`” (being an identifier) is a valid `mul_expr`, and so is “`a*b`” (because “`a`” (being an identifier) is a valid `mul_expr`, and “`b`” is an identifier), and so is “`a*b*c`” (because “`a*b`” is a valid `mul_expr` and “`c`” is an identifier). As the parser recognises “`a*b*c`”, the actions first build a `MUL_EXPR` node for “`a*b`”, and then a `MUL_EXPR` node that refers to the first node and represents “`(a*b)*c`”. The final AST could be written as:

```
MUL_EXPR(MUL_EXPR(a, b), c)
```

Satisfy yourself that the rule for `add_expr` successfully recognises the expression “`a*b+c*d`” and produces the following tree:

```
ADD_EXPR(MUL_EXPR(a, b), MUL_EXPR(c, d))
```

A pleasing side effect of the way that these rules have been written is that multiplication is more “sticky” (or, more formally, it has higher precedence) than addition, so `a*b` and `c*d` have been correctly grouped together according to the precedence rules you remember from school. Applying a simplified version of the full C grammar to the earlier token string might yield the following AST:

```
	FUNC_DEF (
	          name: foo
	          params: [(a, INT), (b, INT), (c, INT), (d, INT*)]
	          returns: VOID
	          body: SEQ_STMT (
	                          stmt[0]: AUTO_DECL (
	                                 name: I
	                                 type: INT
	                                 initialize: 0
	                          )
	                          stmt[1]: DO_LOOP_STMT (
	                                 body: EXPR_STMT (
	                                       expr: ASSIGN_EXPR (
	                                              lhs: INDEX_EXPR (
	                                                    array: d
	                                                    index: i
	                                              )
	                                              rhs: ADD_EXPR (
	                                                    lhs: MUL_EXPR (
	                                                          lhs: i
	                                                          rhs: a
	                                                    )
	                                              rhs: MUL_EXPR (
	                                                    lhs: b
	                                                    rhs: c
	                                              )
	                                        )
	                                 )
	                          )
	                          test: LESS_THAN_EXPR (
	                                 lhs: i
	                                 rhs: 10
	                          )
	                )
	          )
	)
```

#### Semantic Analysis

Armed with the AST, the compiler can construct a symbol table that describes the type of each formal parameter and local variable within function foo:

```
	a: int
	b: int
	c: int
	d: int*
	i: int
```

From this, it can determine that both `d[i]` and `i * a + b * c` have type `int`, and that `d[i]` is an lvalue. An _lvalue_ is a suitable target for an assignment: `a` and `d[i]` are lvalues, whereas `b * c` is not. The assignment `d[i] = i * a + b * c` is therefore determined to be semantically valid.

#### Intermediate Code Generation

When we have a semantically valid AST, we can set about converting it into intermediate code. The intermediate code generator knows how to convert each type of AST node into one or more intermediate code instructions, and these rules are applied recursively. For example, to convert an `ADD_EXPR` node we first convert its left and right children (called `lhs` and `rhs` in the example from the “[Parser](#08_9781119183938-ch05.xhtml#c05-sec-0020)” section), and then emit an `ADD` instruction to combine the results. To convert a `DO_LOOP_STMT` we emit a label, then convert the body of the loop and the loop test expression (called `body` and `test` in the example), and finally emit a conditional branch back to the start of the loop, which is predicated on the result of the test:

```
	FUNCTION foo(p0, p1, p2, p3)
	MOV           t0, #0             ; temporary 0 stores count
	label:
	  MUL           t1, t0, p0         ; calculate i * a
	  MUL           t2, p1, p2         ; calculate b * c
	  ADD           t3, t1, t2         ; calculate i * a + b * c
	  MUL           t4, t0, #4         ; index = count * sizeof(int)
	  ADD           t5, p3, t4         ; calculate address
	  STW           [t5], t3           ; store i * a + b * c in d[i]
	  ADD           t0, t0, #1         ; increment loop count
	  BRANCHLT      t0, #10, label     ; branch if count < 10
```

#### Simple Optimisation

Notice that `b * c` is calculated each time around the loop, when it’s only dependent on the formal parameters `b` and `c`, which don’t change. We say that `b * c` is _loop invariant_, and apply loop-invariant code motion to hoist the computation out of the loop, saving nine cycles. As we only need one register to store `b * c`, rather than two registers to store the separate values, we’ve also usefully reduced _register pressure_ (the number of values that need to be remembered at any given point in the program) by one, which improves the chances of fitting all the values we need into the target CPU architecture’s registers. If we had needed `b` and `c` on their own as well as `b * c` then this optimisation would have required more registers than might be available, and the compiler would need to apply a heuristic (that is, a mechanism used to solve a particular code-generation case that might not apply to all cases) to see whether the trade-off was worth making.

```
	FUNCTION foo(p0, p1, p2, p3)
	  MOV            t0, #0
	  MUL            t2, p1, p2    ; hoist loop-invariant calculation
	label:
	  MUL            t1, t0, p0
	  ADD            t3, t1, t2
	  MUL            t4, t0, #4
	  ADD            t5, p3, t4
	  STW            [t5], t3
	  ADD            t0, t0, #1
	  BRANCHLT       t0, #10, label
	  RET
```

#### More Aggressive Optimisation

A more aggressive optimiser might be able to detect that both the address, which we’ll de> > [!NOTE]
`a(i)`, and the value stored, which we’ll de> > [!NOTE]
`v(i)`, change by a fixed amount each time we go around the loop:

```
	a(0) = d         a(i+1) = a(i) + 4
	v(0) = b*c       v(i+1) = v(i) + a
```

Also we leave the loop just before we write to address `a(10) = d + 40`. It can therefore eliminate the potentially costly multiplication instructions, which can be hard to schedule due to their long pipeline depth, instead keeping a running value of `a(i)` and `v(i)`, and replace the test `i < 10` with the test `a(i) < a(10)`. This class of optimisation is known as _induction variable elimination_:

```
	FUNCTION foo(p0, p1, p2, p3)
	  MUL            t1, p1, p2
	  MOV            t2, p3
	  ADD            t3, t2, #40
	label:
	  STW            [t2], t1
	  ADD            t1, t1, p0
	  ADD            t2, t2, #4
	  BRANCHLT       t2, t3, label
	  RET
```

#### Target Code Generation (Register Allocation, Instruction Scheduling)

Now we have an optimised program represented in intermediate code; the final step is to convert that program into assembly language for our target platform. The key challenges are finding a machine register to store each value computed by the program between the point it is defined and the last point at which it is used (this is called _register allocation_), implementing each intermediate instruction by using one or more machine instructions, and ordering those machine instructions so as to avoid triggering interlocks inside the CPU pipeline (this is called _instruction scheduling_):

```
	  ; In the ARM EABI calling convention, the first four
	  ; arguments are in provided r0-r3
	` 

	  ; r0-r3 may also be used as scratch registers without
	  ; saving to the stack
	` 

	foo::
	  mul            r1, r1, r2     ; r1 = b * c (reuse r1)
	  add            r2, r3, #40    ; r2 = d + 40 (reuse r2)
	label:
	  stw            [r3], r1       ; store v(i) at a(i)
	  add            r3, r3, #4     ; a(i+1) = a(i) + 4
	  add            r1, r1, r0     ; v(i+1) = v(i) + a
	  cmp            r3, r2         ; have we reached a(10) yet?
	  Blt            label          ; if not, loop
	  B              lr             ; return to link address
```

### Linking Object Code Files to Executable Files

When the compilation process is done and the smoke clears, what you have is not quite an executable program file. Most modern compilers generate an object code file that requires one additional step before you can run it: _linking_. The key to understanding linking lies in these two points:

- Nearly all workaday programs (as opposed to simple test or learning programs) are written in several pieces, each of which is compiled separately to an object code file. - Nearly all programs make use of code libraries that are object code files containing useful functions and data definitions that may be considered “standard parts” in software development.

Of course, the simple programs you write as you learn a programming language or toolset will be small enough to create in one piece. However, whether you realise it or not, even your simple test programs probably make use of existing code libraries. Nearly all high-level languages have a runtime library containing standard functions implementing support for text strings, higher maths, date and time manipulation, and so on; the runtime library also contains startup code, which runs before your main function and initialises data structures used by other library functions. Other libraries may contain code specific to a particular operating system, for access to displays, printers and file systems.

What a linker does is combine multiple object code files and functions from statically linked libraries into a single executable code file that may be run on the target computer. This requires more than just writing out the object code files nose-to-tail. Code in one object code file may call functions, or use data definitions, from libraries and other object code files. Calling a function requires the memory address of the function. There’s no way to specify a memory address in another object code file stored somewhere else on disk or solid-state drive (SSD.) Instead, the compiler puts a placeholder into the spot where such an external address needs to go. The placeholder says, in effect, “address to be determined”.

While the linker is combining separate object code files into a single executable file, it looks for such placeholders and calculates addresses that, in most cases, are offsets from the beginning of the executable file. The long and winding road from source code files to a finished executable program file is shown in Figure 5-6.

> [!NOTE]
> the way that references to identifiers in one object code file “plug in” to the actual functions or variables in another object code file.

![[FIGURE 5-6:](#08_9781119183938-ch05.xhtml#rc05-fig-0006) How the compiler and linker create a single executable program file](./media/images/9781119183938-fg0506.png)

## Pure Text Interpreters

In the preceding section, we briefly mentioned the concept of bytecode compilation. Before we elaborate on this, it is helpful to take a brief detour back into programming history. Early versions of the BASIC language were modelled on FORTRAN and were compiled on mainframes and minicomputers just as FORTRAN was. In the mid-1970s, the first personal computers often had too little memory for a real operating system, much less a compiler. To enable users to learn programming and write their own software, a different kind of BASIC language system appeared: the _text interpreter_.

In a text interpreter system, a program is written in the form of a textual source code file, just as with native code compilation. However, there is no compilation step at all; when a program is run, the source code file is opened by a piece of software called an _interpreter_. The interpreter reads the first line from the source code file and then performs whatever work that line specifies. When the first line is done, the interpreter reads the next line, performs the work it specifies and so on, through the source code file. The key characteristic of text interpreters is that they process a single line of program source code at a time. Figure 5-7 illustrates this process.

![[FIGURE 5-7:](#08_9781119183938-ch05.xhtml#rc05-fig-0007) A text interpreter for the BASIC language](./media/images/9781119183938-fg0507.png)

A text interpreter takes each line of source code apart after it’s read from the file. It then calls subroutines to evaluate arithmetic expressions like `Height * Width` and process keywords like `INPUT` and `PRINT`. The text interpreter creates variables in memory as the source code introduces them, and manages them while the program runs. Values are read from variables as needed in calculations, and new values are given to variables when a program line assigns or recalculates a variable’s value. The text interpreter handles displaying the program’s output on the computer monitor, and the reading of text input from the computer keyboard.

Text interpreters for simple dialects of BASIC were comparatively straightforward to write and (more importantly) were compact. An interpreter consisted of a simple line lexer and parser, and then a collection of functions to execute the various keywords and features of BASIC. Many early personal computers, from the Commodore VIC-20 up to the original IBM PC, had a BASIC interpreter stored on read-only memory (ROM) chips soldered to the motherboard. In many cases, the BASIC interpreter stood in for a simple operating system, and allowed single commands to be entered to an interactive command line.

Pure text interpreters for programming languages like BASIC were everywhere in the 1970s and 1980s, but are nearly extinct today. Where text interpreters are still used, it is for creating command files for operating systems, database managers and large, complex applications that allow commands to be “batched” in text files. This was once called _scripting_, but that term has broadened to include programming for any language that incorporates interpretation at any level.

## Bytecode Interpreted Languages

One useful characteristic of text interpreters is that they insulate a running program from the fine details of the underlying platform. A BASIC program’s `PRINT` keyword does the same thing, whether it’s running on DOS, Linux or any other operating system. The interpreter itself is a native-code machine-language program, and deals with hardware and operating system specifics, but a BASIC program will run identically on any text interpreter, on any platform, that understands the appropriate dialect of BASIC.

This attribute of BASIC programs is called _portability_; the portability of applications became an important consideration once computers grew cheap enough to be commodities, with hundreds and later thousands of different and often incompatible designs up and down the market. There were hundreds of different ways to write characters to a display, to send text to a printer, and to read and write data to storage devices. Programs had to be written in a slightly different way on each system, in order to take advantage of that system’s features. The portability problem plagues us to this day, and the best solution we now have centres on an evolved form of interpretation.

### P-Code

In the mid-1970s, researchers at the University of California, San Diego developed a new kind of compiler for the Pascal programming language. The UCSD Pascal compiler operated in much the same way as the native-code compilers we described earlier. The resemblance stopped at the point where UCSD Pascal generated intermediate code. Native code compilers take their intermediate code and use it as a guide for generating native code. The UCSD compiler’s intermediate code was written to a file, and then that file of intermediate code was executed by an interpreter installed on a computer. As with BASIC’s text interpreter, the UCSD interpreter insulated the program from the details of the underlying computer. A program written in the UCSD Pascal syntax could theoretically be compiled once, and then the intermediate code could be run in an identical manner on any machine for which an interpreter had been written. The code was thus extremely portable between otherwise incompatible computers.

This technology was dubbed the P-System, where the “P” originally stood for “pseudocode” and later “portability code”. (Both are now-obsolete terms for “bytecode”, which we will discuss in the next paragraph.) The intermediate code (p-code) generated by the UCSD compiler was not textual. It was a sequence of binary instructions that resembled machine instructions but were actually instructions understood and executed by the interpreter program. These instructions represented an instruction set for a virtual machine; that is, a CPU that did not exist in silicon, but was emulated using a p-code interpreter.

The P-System was the first technology of its kind to win wide acceptance. The notion of p-code was soon taken up by other researchers for other languages. The underlying idea of a virtual instruction set for a virtual machine does not depend on Pascal or any other specific programming language, and the P-System was later expanded with support for languages including Modula-2, BASIC, and FORTRAN. The term p-code was eventually abandoned in favour of bytecode, but the meaning is the same: bytecodes are synthetic machine instructions generated by a bytecode compiler and intended to be executed by a bytecode interpreter. The term comes from the fact that most bytecode systems use 8-bit (1 byte) instructions. However, there is nothing inherent in the bytecode concept limiting instructions to a single byte. For example, the Dalvik bytecode technology, which forms part of the Android operating system, uses 16-bit instructions in its bytecode.

The firm Western Digital introduced an interesting product line in 1979: the Pascal MicroEngine, which was a custom microprocessor that executed UCSD p-code as its native instruction set. P-code ran much more quickly as native code without an interpreter between itself and the CPU, but the MicroEngine was eclipsed by the release of the IBM PC in 1981 and never hit critical mass. The concept of “hardware assist” for bytecode execution is a recurring theme: several vendors have released microprocessors that directly execute Java bytecode, and some members of the ARM family of CPUs include special features to execute Java language bytecode in hardware efficiently. ([Chapter 4](#07_9781119183938-ch04.xhtml) touches on this briefly.)

### Java

Bytecode never went entirely out of use after the P-System was released, but it was uncommon until the early 1990s, when James Gosling at Sun Microsystems (now a subsidiary of Oracle) developed the Java programming language and virtual machine as a bytecode system. The overriding goal with Java was portability: programs compiled to Java bytecode would run identically on any computer supporting the Java Runtime Environment (JRE). Sun popularised the slogan, “Write Once, Run Anywhere” to emphasise Java’s big selling point.

Even in its first release, the Java system was much more sophisticated than the P-System ever was. The JRE includes the Java Virtual Machine (JVM), which implements the Java bytecode interpreter, as well as the Java runtime code libraries and various software tools that allow Java code to run inside web browsers and from web servers. Programmers who want to write Java programs need the Java Development Kit (JDK), which in addition to the JRE includes the Java language compiler and a number of other tools supporting software development.

The JVM does more than simply execute Java bytecode. It manages an area of memory reserved for the use of Java programs, in which data items are created, used and then destroyed when no longer needed, with their memory space automatically reclaimed by a utility called a _garbage collector_. The JVM also monitors data manipulation and watches for program code that attempts to do undefined things with data that might potentially crash a program and damage the JRE or other software outside the JRE, like the operating system. The JRE became a model for similar bytecode systems created by others, and today such a system is more generally called a _managed runtime environment_ (MRE). The way bytecode programs are compiled and run in an MRE is shown in Figure 5-8.

![[FIGURE 5-8:](#08_9781119183938-ch05.xhtml#rc05-fig-0008) Bytecode executed in an MRE](./media/images/9781119183938-fg0508.png)

An MRE is not by itself an operating system, and there is an operating system running underneath every MRE. The operating system manages the physical hardware of the computer on which it runs. To make itself operating-system independent, the MRE includes an operating system abstraction layer that gives the bytecode programs executed in the MRE a standard “view” of the operating system that is always the same, regardless of what operating system exists below the MRE.

Java was a spectacular success almost immediately. Microsoft soon saw the value in the Java idea and released its .NET Framework system in 2002 as a competitor. Architected by Anders Hejlsberg (the creator of Turbo Pascal) it included a new Java-like language, C#, which compiles to bytecode called the Common Intermediate Language (CIL), which in turn runs on the Common Language Runtime (CLR) VM.

Many books have been published on programming Java with the JDK. One of the most popular is _The Java Tutorial: A Short Course on the Basics_, 5<sup>th</sup> edition, by Sharon Zakhour, Sowmya Kannan and Raymond Gallardo (Addison-Wesley, 2013). For younger students (aged 10 and up) _Java for Kids_ by Philip Conrod and Lou Tylee (Kidware Software, 2013) may be more accessible.

### Just-In-Time (JIT) Compilation

Portability and security are the big value-adds in bytecode systems like Java and .NET, but they come at a cost: execution speed. Interpreted bytecode, while faster than interpreted source code text in languages like BASIC (largely due to the elimination of repeated lexing and parsing) is still significantly slower than native code. One solution to this problem came out of research involving the Smalltalk language, and was first widely implemented for Java: _just-in-time_ (JIT) compilation.

The idea behind JIT compilation is fairly simple: instead of having the system interpret bytecode, a JIT compiler (informally called a _jitter_) compiles bytecode to native code “on the fly”, as it is needed. The whole file isn’t compiled at once and, on most systems, bytecode that is never executed isn’t compiled at all. Compilation is usually done in blocks; a block may be anything from a few consecutive bytecode instructions to an entire function. Once a block of bytecode is compiled to a block of native code, the MRE can branch directly to the native code rather than interpreting the bytecode for the block instruction by instruction. Because blocks of code are often executed multiple times during a program session, the native code blocks generated by the jitter are not discarded, but are stored in a software-managed cache (see Figure 5-9).

![[FIGURE 5-9:](#08_9781119183938-ch05.xhtml#rc05-fig-0009) How JIT compilation works](./media/images/9781119183938-fg0509.png)

Due to the initial overhead of JIT compilation, execution of a bytecode program is slow when the program is first run. As blocks of native code accumulate in the cache, execution occurs in native code more often, and performance improves. In general, the performance is never quite as good as a well-written program compiled with an optimising native-code compiler, but because much of the work of compilation is done when the program is first compiled from source code to bytecode, JIT compilation can be done with surprising speed.

There is a sort of 80/20 effect in code execution, meaning that a relatively small proportion of program code ends up running the majority of the time. Newer versions of the Java JIT compiler contain logic that analyses a compiled Java program to determine where these “hotspots” are. It then focuses its attention on optimising those hotspots. The JIT’s analysis is _heuristic_—that is, it compiles statistics on what elements of a program impact code performance (this is called _tracing_) and “learns” as execution continues. Such a JIT compiler is called a tracing JIT. As the JIT accumulates trace data, it applies progressively more sophisticated optimisations to those code paths that execute most often.

A sophisticated tracing JIT can learn enough about a program during execution to actually rewrite portions of the code based on the types and even the values of function arguments. In certain circumstances these optimisations are so good that hotspots can run faster than an equivalent native program, which cannot typically be rewritten at runtime.

### Bytecode and JIT Compilation Beyond Java

Java remains the single most common use of bytecode technology. Since Java’s appearance, many other languages have either been designed to use bytecode or converted from pure text interpretation to bytecode, sometimes with a JIT compiler. Here’s a list of a few of the most popular:

- Ruby, inspired by Smalltalk, is commonly used with a web-application framework called Rails. Ruby and Rails are both available for the Raspberry Pi. - JavaScript is a browser-based language supported by all modern web browsers. The current release of Mozilla Firefox includes the IonMonkey JIT compiler for JavaScript. - Lua is a scripting language for control scripts within operating systems and applications, especially game engines. A separate implementation of the Lua language called LuaJIT uses a trace JIT compiler and achieves much higher performance than Lua 5.2. Both Lua 5.2 and LuaJIT are now distributed with Raspbian. - Python is a bytecode language, and a JIT compiler implementation of Python called PyPy is now part of the standard Raspbian image.

### Android, Java and Dalvik

Oddly enough, one of the biggest uses of the Java programming language is not for the JRE at all. The Android operating system for smartphones and tablets is integrated with and depends on a bytecode MRE called Dalvik. Native code applications may be run on Android, but the Dalvik MRE is available on every Android device, without exception. An application that runs on any instance of Dalvik should run on all of them.

The recommended way to write applications for Android is first to write them in Java, and compile them to Java bytecode. The Android Software Development Kit (SDK) then takes the Java bytecode and compiles it to the completely different bytecode understood by the Dalvik MRE. Dalvik contains a JIT compiler that converts Dalvik bytecode to blocks of native code for whatever CPU the system runs on.

## Data Building Blocks

Earlier in this chapter, [Figure 5-4](#08_9781119183938-ch05.xhtml#c05-fig-0004) showed a simple program in diagram form to define some common terms. [Chapters 3](#06_9781119183938-ch03.xhtml) and [4](#07_9781119183938-ch04.xhtml) described the physical mechanisms by which data are stored (memory) and instructions are executed (the CPU). Now we take a closer look at some of the features that high-level languages provide to enable programmers to describe data and code.

The emphasis here is on understanding fundamental concepts, rather than on the syntax of any one specific language. The same concepts can be expressed in very different ways in different languages, but a solid grasp of the underlying principles will be of use regardless of which language you end up using.

### Identifiers, Reserved Words, Symbols and Operators

In a programming language, an _identifier_ is a human-readable name given to something in the program. Most modern languages share a common lexical form for identifiers: a sequence of alphanumeric characters and underscores, where the first character is not a digit. `DelaySinceMidnight`, `Error17`, and `radius` are all identifiers. `2.746` and `42fish` are not. Some sequences of characters that would otherwise be valid identifiers may be considered _reserved words_ or _keywords_, which have special meaning to the compiler and can be used only in certain ways within the rules of the language’s syntax. The words `while` and `if` are reserved words in most languages, whereas `otherwise` is a reserved word in some languages but not others. The only way to be sure whether a word is reserved for a given language is to look in a language reference manual for that language.

Certain non-alphanumeric characters may have special meaning in a language. Characters or short groups of characters with special meaning are called _symbols._ In C, the group `//` is a symbol called a _comment delimiter_. Anything from the `//` group to the end of a source code line is a comment that is ignored by the compiler at the preprocessing stage. (Comments, again, are meant to be read by programmers and not compilers.) In Pascal, pairs of curly braces enclose comments. In C, pairs of curly braces group statements and variable declarations to form compound statements. In C the semicolon character is a symbol called a _statement terminator_; it tells the compiler where a statement ends.

Some symbols are used as _operators_, which combine values to generate new values, exactly as familiar symbols like + and – do in an algebraic expression. There are operators in most languages: for familiar operations like addition, subtraction, multiplication, division and raising to a power; for bitwise and logical operations like AND, OR and XOR; for manipulation of character strings and sets; and a few odds and ends like address extraction and modulo maths. Unary operators like negation (-x in C) and bitwise NOT (~x) take one operand; binary operators like addition (x+y) and multiplication (x\*y) take two operands; some languages have ternary operators, which take three operands.

### Values, Literals and Named Constants

A _value_ is a single piece of data used by a program. The numbers 42 and 7.63, and the string “foo” and the Boolean values true and false (which implement Boolean logic in computer languages) are all values. Operators operate on values to create new values. In the expression `42+23`, `42` and `23` are both values (in this case they are referred to as _literal values_ or _literals_ because they appear literally in the expression), as is the result 65, which is created by the + operator at runtime.

It’s often useful to give names to literals. Many languages provide a mechanism to define _named constants_, which allow an identifier to be used in place of a literal for more readable code. For example, you may be writing a program that compresses its database after more than 10,000 records are written to the database. You can define a named constant called `CompressionThreshold` with the value 10,000. This allows you to write a statement like this:

```
	If RecordCount > CompressionThreshold:
	   CompressDatabase()
```

Named constants allow you to name a value _once_ in your program, and use the named value everywhere in your program (which might be hundreds or thousands of places) in place of a literal. That way, if necessary, you can change the definition of the named constant at one place in your program and the compiler will “plug in” the changed literal value consistently everywhere you’ve used the constant’s name. It’s either that or change a literal value at all the necessary spots in your source code and just hope you don’t miss any!

### Variables, Expressions and Assignment

Literals and named constants are values, and by definition are constant at runtime. If you need to change one, you must change its definition in the source code and rebuild. In contrast, _variables_ are not values but containers for values. Your program must fill them at runtime with either values given as constants or values computed by an expression. This is called _assigning_ a value to a variable, and it’s done with an assignment statement, as in the following examples:

- C, C++, Java: `TheAnswer = 42;` - Python: `TheAnswer = 42` - Pascal: `TheAnswer := 42`

Although these examples look very similar, there is a little subtlety here. In Python and Pascal the assignment statement is a fundamental syntactic element of the language, whereas in C, C++ and Java assignment is performed as a side effect of the `=` operator in an expression.

An _expression_ is a formula for the runtime calculation of a value using a language’s operators and syntax. Expressions may contain literals, named constants and variables that already contain values. If variable `R` contains the radius of a circle, the circle’s area may be computed by using the mathematical formula pi × radius<sup>2</sup>. When expressed in a programming language, such a formula becomes an expression. Precisely how it’s written depends on a language’s syntax. Some languages, including Python, have a separate exponentiation operator. C, C++, Java, and Pascal do not:

- C, C++, Java, Pascal, and many others : `Pi * (R * R)` - FORTRAN, Python, Ada, and others : `Pi * R**2`

In most languages, parentheses are used to set order of evaluation in expressions, just as they are in mathematical formulae.

### Types and Type Definitions

Each data item that a program uses is represented in memory as one or more binary numbers. The meaning of a particular binary number is context-dependent: the byte 00000001<sub>2</sub> might represent the number 1, or the Boolean value `true`; the byte 01000001<sub>2</sub> might represent the number 65, or the character “A” in ASCII encoding. Most high-level languages have a type system, which associates a type with each value. The type allows the compiler or runtime to perform the appropriate operations when values are used, and to detect operations that are semantically meaningless (such as adding a Boolean to a character in many languages, or adding two pointers together in C).

_Primitive types_ are the building blocks of a language’s type system. Common primitive types include:

- **Booleans:** These take two values, true and false. A Boolean value can occupy as little as a single bit of storage, though for convenience at least 8 bits (1 byte) are generally used. Although not a requirement, it is common to use zero to represent false, and any non-zero number to represent true.
- **Integers:** Whole numbers, like 42 and –12. Unsigned integers must be positive, and can be represented as straight binary numbers; signed integers may be positive or negative, and are generally stored in two’s complement format (which is discussed in more detail in the “[Two’s Complement and IEEE 754](#08_9781119183938-ch05.xhtml#c05-sec-0040)” section). The range of representable integer values depends on the number of bits allocated to the number. C compilers for 32-bit architectures generally allocate 32 bits (4 bytes) to store an integer, giving a range of 0 to 4,294,967,295 for unsigned values.
- **Floats** (floating-point numbers): Can take on fractional values, like 3.4 and –10.77. Floats are often represented in memory as 32 or 64 bits of data, into which are packed a sign bit `s`, an exponent (the magnitude of the value) `e` and a mantissa (the value’s significant digits) `m`. The value represented is given by the formula:

```
    `m * 2e` if s == 0 or

    `-m * 2e` if s == 1
```

```
The IEEE 754 standard (which is covered in more detail later in the     “[Two’s Complement and IEEE     754](#08_9781119183938-ch05.xhtml#c05-sec-0040)” section) specifies     ways of packing `s`, `e` and `m` into words of various lengths,     and rules for performing arithmetic operations on numbers stored in     this form. Most modern architectures conform to this standard.
```

- **Characters:** Small (generally 8- or 16-bit) integers, each of which represents a character of printed text.
- **Strings:** Sequences of characters. Some languages provide strings as primitive types, whereas others implement strings as arrays of characters. C strings are null-terminated: the end of the string is marked by placing a special null character (with binary representation zero) in memory. Other languages store the length of the string separately alongside the array of character data or, in the case of Java, define a special class of object to represent strings. Even if strings are not primitive types, it is common to provide language features to make them appear to be. For example, in Java, where each string is represented by an instance of the system class `java.lang.String` (we will cover objects, instances and methods at the end of this chapter), it is legal to write:

```
    `String s = “foo” + “bar”;
```

```
and the compiler silently translates this into a series of calls to     methods of the `String` class.

In addition to providing primitive types, most languages provide     ways of progressively building up more complex composite types by     combining multiple primitive types or simpler composite types.     Common varieties of composite types include:
```

- **Arrays:** Ordered sequences of variables, treated as a unit. Individual elements of an array are selected by an index, often specified using square brackets as index delimiters; for example, `GradeArray[42]`. Arrays may have more than one dimension, and each dimension may be a different size.
- **Structs** (also called _records_ or _tuples,_ depending on the language): Groups of non-ordered named variables. Each variable in a struct is called a _member_ or a _field_. Fields within a struct are selected by name, often using the dot (`.`) field selection operator. Suppose you have a struct type named `ContactStruct` that includes a field named `LastNameField`, and a variable with type `ContactStruct` called `contact`. You would then refer to that field of `contact` using the syntax `contact.LastNameField`.
- **Sets:** Unordered collections of values, with the property that any value may not be present more than once. The internal implementation of a set is generally optimised to make testing for the presence of a particular value cheap, and facilities are provided to compute the union, intersection and differences of sets efficiently.
- **Maps or dictionaries:** Provide a mechanism for storing a collection of values, each of which is indexed by a key. This can be a seen as a generalisation of the array composite type, often using the same square bracket notation, but allowing keys of (nearly) arbitrary types rather than just integers and eliminating the requirement to specify a maximum size when the array is created.
- **Enumerations:** Unordered collections of values, each given an arbitrary name by the programmer; the value chosen to represent each member is generally chosen automatically by the compiler. They can be used as a type-safe alternative to named constants if we have, for example, a parameter that controls the behaviour of a function and can take one of a small number of distinct values.
- **Pointers:** These specify the location of another value in memory and are generally defined to point to an instance of a specific type. When we have a pointer, we can dereference (follow) it to manipulate the underlying value. Careless use of pointers can lead to hard-to-debug crashes and security exploits, which is one reason that some languages, especially Java, do not include unrestricted pointer types but instead provide runtime-checked, type-safe references to objects or arrays.

### Static and Dynamic Typing

Programming languages can be broadly divided into statically and dynamically typed languages based on how they treat types. In _statically typed_ languages such as C, types are associated with variables when the code is written, and the type of a value stored in a variable is implicitly that of the variable itself; the compiler is able to allocate storage for variables, and for the intermediate results generated when evaluating expressions, ahead of time, which is efficient, and can perform semantic analysis (as we saw in the section on compilers) to detect and flag operations between incompatible operands at compile time.

In the following fragment of C code, the variable `foo` has type `int`, and the variable `bar` has type `float`. The compiler knows it can allocate either a single machine register or a 4-byte section of stack to hold each value (on a typical 32-bit machine), and that when adding them together it must (according to the C type rules) emit an instruction to convert or cast `foo` into a floating-point value, followed by a floating-point add instruction:

```
	int foo = 42;
	float bar = 98.2;
	…
	float baz = foo + bar;
```

Throughout the lifetime of a variable in a statically typed language, only compatible values may be assigned to a variable, so the following C example will result in a compilation error:

```
	int foo = 42;               // foo has type int
	char *bar = “hello world”;  // bar has type “pointer to char”
	foo = bar;                  // error!
```

By contrast, in _dynamically typed_ languages such as Python and JavaScript, types are associated with values at runtime. Variables have no type: they merely contain a reference to a typed value; in a naïve implementation, storage for the value (and a description of its type) will be allocated on the heap and recovered when it is no longer needed by a process of garbage collection. Semantic checks on the types of operands occur at runtime; this is potentially expensive, though the development of tracing JITs for dynamically typed languages has reduced the cost substantially.

In the following fragment of Python code, the function `add()` is invoked three times. On the first invocation, `x` and `y` refer to two values of type `int`, so the `+` operator is deemed to represent integer addition. On the second, `x` and `y` refer to two values of type `string`, so the `+` operator is deemed to represent concatenation. On the third, `x` and `y` have different types, so the attempt to add them causes a `TypeError` to be thrown. A tracing JIT, such as that found in PyPy, would potentially compile two versions of this function and invoke the appropriate one based on the operand types:

```
	def add(x, y):
	   return x + y
	` 

	print add(1, 2)                    # prints “3”
	print add(“hello ”, “world”)       # prints “hello world”
	print add(“foo”, 1)                # gives TypeError
```

As you will see shortly, statically typed object-oriented languages such as C++ and Java provide some dynamic features through the use of subtype polymorphism. Programmers can declare several types B, C or D, which are derived from type A and rely on dynamic dispatch to do different things depending on which type a particular value is an instance of. Polymorphism comes into play through object-oriented programming, which we’ll cover later in the section “[Object-Oriented Programming](#08_9781119183938-ch05.xhtml#c05-sec-0051)”.

### Two’s Complement and IEEE 754

There are a number of possible ways of representing signed integers as strings of binary digits. Perhaps the most obvious is _sign and magnitude_ notation, in which we have a single bit that is set to one if the number represented is negative and a string of digits that represents the unsigned version of the number (its magnitude). Although this is simple to understand, it is unsatisfying that zero has two representations (+0 and –0), and arithmetic operations are somewhat difficult to implement: when we add two signed numbers, we must inspect the sign bits, decide whether to add or subtract the unsigned magnitudes, and then perform conversions to get the result back into sign and magnitude format.

The vast majority of architectures represent numbers using _two’s complement_ notation. To compute the two’s complement representation of a negative number, we write the regular binary representation of the positive number and then invert every bit and add one. For example, the 8-bit binary representation of five is:

```
5 = 00000101<sub>2</sub>
```

To find the representation of -5, we invert each bit:

```
11111010<sub>2</sub>
```

and add one:

```
11111011<sub>2</sub> = -5
```

Table 5-1 shows the 8-bit binary and hexadecimal representations of the numbers from 3 down through 0 to -3.

```
<figure> <figcaption>

[Table 5-1](#08_9781119183938-ch05.xhtml#rc05-tbl-0001) A Two’s Complement Countdown

</figcaption>

  -------------------------------------------------- ----------------------------------------------------- --------------------------------------------------------   `Binary`     `Hexadecimal`   `Signed decimal`   `00000011`   `03`            `3`   `00000010`   `02`            `2`   `00000001`   `01`            `1`   `00000000`   `00`            `0`   `11111111`   `FF`            `-1`   `11111110`   `FE`            `-2`   `11111101`   `FD`            `-3`   -------------------------------------------------- ----------------------------------------------------- --------------------------------------------------------

</figure>
```

The useful property of two’s complement notation is that regular unsigned addition now works to calculate the sum of signed values, regardless of whether they are positive or negative. So, for example:

```
1 + -3 = 00000001<sub>2</sub> + 11111101<sub>2</sub> = 11111110<sub>2</sub> = -2

-1 + -2 = 1111111<sub>2</sub> + 11111110<sub>2</sub> = 11111101<sub>2</sub> (with 1 carried out) = -3
```

The situation for real numbers (values that may have decimal parts) is more complex. One possibility is to multiply the real number by a large constant (often a power of two), and then round the result to an integer, which can then be represented using two’s-complement notation. We might choose the constant 256 = 2<sup>8</sup>, so the number 1.0 would be represented as 256, and 2.125 would be represented as 544. This is referred to as a _fixed-point_ representation, because a fixed number of bits in the representation (in this case 8) are allocated to storing the fractional part of the number, with the rest allocated to storing the integer part.

In most applications, the operations that are performed on real numbers involve values of widely varying magnitude; this can make it hard to choose an appropriate multiplier for a fixed-point representation. It is therefore customary to use a floating-point representation for real numbers, in which there is no fixed number of digits to the right of the decimal point. Floating-point numbers consist of a mantissa (the significant bits of the value), an exponent (the magnitude of the value) and a sign, positive or negative, packed into a single binary word. The representation and range of floating-point values, and the exact results of floating-point operations, were compiler-dependent until the IEEE 754 floating-point number standard appeared in 1985. IEEE 754 defines several floating-point formats that may be used as types in programming languages. The range of some is breathtaking: the 128-bit floating-point number can express positive values as high as 10<sup>6144</sup>. (To put this number into perspective, consider that there are “only” about 10<sup>80</sup> atoms in the entire observable universe.) Figure 5-10 shows how the three elements of a floating-point value (the sign, the mantissa and the exponent) are packed into an IEEE 754 64-bit value.

![[FIGURE 5-10:](#08_9781119183938-ch05.xhtml#rc05-fig-0010) Inside a 64-bit floating-point number](./media/images/9781119183938-fg0510.png)

## Code Building Blocks

A single-threaded program in an imperative programming language is a description of a series of steps required to perform an operation. A _statement_ is a complete description of one of those steps. It’s the equivalent of a sentence in a language spoken by humans. Put some number of statements in sequence, and you have a program. In broad terms, there are really only four kinds of statements:

- **Assignment statements:** These give a value to a variable or an element of a compound variable, as explained a little earlier in this section. - **Function calls:** These are invocations of functions defined in a library or elsewhere in the program; for example, `print()` or `factorial()`. A function call is typically made simply by naming the function and providing zero or more arguments. - **Control statements:** These alter the sequence of execution within the current function. - **Compound statements:** These are sequences of statements treated as a group within some sort of control statement.

Control statements and compounds statements are inextricably connected, and we’ll treat them together.

### Control Statements and Compound Statements

Being able to change the course of execution of a program at runtime is fundamental to the programming idea. Some statements must be executed under some circumstances but not others. This is called _conditional execution_. Some statements must be executed not once but multiple times. This is called _looping_. An imperative programming language provides varieties of control statement to implement each of these behaviours.

Compound statements are written as sequences of statements between delimiters. In C, C++, C#, Java, and languages descended from them, these delimiters are generally curly braces (`{` and `}`). In Pascal and Ada, the delimiters are the keywords `begin` and `end`. Python is rare among languages in that it lacks delimiters completely. Compound statements in Python are delimited by indentation in the source code. We’ll show you how this works in the examples for the control statements.

### If/Then/Else

The most fundamental control statement is the `if/then/else` statement, which exists in some form in all programming languages. The general structure of the statement is illustrated in Figure 5-11.

![[FIGURE 5-11:](#08_9781119183938-ch05.xhtml#rc05-fig-0011) The `if/then/else` statement](./media/images/9781119183938-fg0511.png)

The simplest form of `if` statement tests a condition and executes a statement if the condition evaluates to `true`. If the condition is not true, execution falls through and continues with the statement immediately following the `if` statement.

To reiterate: _don’t obsess on syntax._ You can always look up syntax in a language reference. Focus on the logic. A simple example will give you a sense of the different ways that programming languages express the same logic:

```
	if (I > 99) FieldOverflow(Fieldnum, I);           C and its descendants`

	if I > 99 then FieldOverflow(Fieldnum, I)         Pascal`

	if I > 99:FieldOverflow(Fieldnum, I)                     Python`
```

> [!NOTE]
> from the examples above that the C family of languages lacks the keyword `then`, and in Python the colon, line break and indentation are an essential part of the syntax. If you’re coming to Python from some other language (especially C or its relatives) it’s crucial to remember: Python considers whitespace (line breaks, spaces, and tabs) significant. Very few other programming languages do.

```
If` statements may contain an optional `else` part, which specifies a statement or compound statement to execute when the tested condition is not true. This is the last portion of the diagram in [Figure 5-11](#08_9781119183938-ch05.xhtml#c05-fig-0011). In between `then` and `else` you have the opportunity to insert additional tests, each governing execution of a statement or a compound statement. There may be any reasonable number of such nested tests, which are called `else/if` structures.
```

What are multiple `else/if` s good for? One metaphor would be sorting categories out of a disordered pile. If you have a jar full of coins and want to bag them up for deposit at the bank, you first sit down at the table and sort them. Is the coin a penny? If so, slide it to the penny pile; otherwise, is it two pence? If so, slide it to the two-pence pile; otherwise, is it five pence? If so, slide it to the five-pence pile, and so on, up to the two-pound denomination. This form of logic is called a _multi-way branch_.

### Switch and Case

Multi-way branches are so common in programming that in many languages a special type of control statement is provided to implement them. Different languages implement multi-way branch logic in different ways, using different keywords. The C family calls it a `switch` statement and uses the keyword `switch`. Pascal and Ada call it a `case` statement and use the keyword `case`. (A few languages, including FORTRAN and some versions of BASIC, use `select case`.)

Unfortunately, the logic behind C’s `switch` is not quite the same as the logic behind Ada’s and Pascal’s `case`. The two are different enough, in fact, so that they should be mapped out separately. The general form of a `case` statement is shown in Figure 5-12. The general form of a `switch` statement is shown in Figure 5-13.

![[FIGURE 5-12:](#08_9781119183938-ch05.xhtml#rc05-fig-0012) The `case` statement](./media/images/9781119183938-fg0512.png)

![[FIGURE 5-13:](#08_9781119183938-ch05.xhtml#rc05-fig-0013) The `switch` statement](./media/images/9781119183938-fg0513.png)

The `case` statement is the simpler of the two. In a `case` statement, a variable is tested against a list of cases. Each case contains an individual value or list of values, generally expressed as constants. If the variable’s value matches one of the cases, the statement or compound statement belonging to that case is executed. In the coin metaphor, the case values on the left would literally be the values of each denomination. The statement associated with the penny case would increment a counter that tallies pennies, and so on. In a `case` statement, once a match is found and the case’s action is taken, the `case` statement is done, and execution continues with the next statement in the program. If no match is found, an optional `otherwise` case can be used to take a “none of the above” action. In our metaphor, this might be the action taken when a foreign coin like an American quarter or Mexican peso is found in the coin pile.

The `switch` statement is similar, but with a very important twist: once a value is found, the case containing that value is executed, as are all the cases that follow it. If only one case is to be executed, a `break` statement must be placed at the end of the statements present in that case. A `break` statement ends the `switch` statement, and causes execution to continue with the next statement in the program. As with `case`, an optional “none of the above” case (this time referred to as the `default` case) can be defined.

This may seem bizarre to beginners, especially if they’ve used languages with the simpler `case` statement. The reason for case-action fall-through in `switch` is historical; it’s descended from a FORTRAN statement called a computed goto. In modern practice, there’s a `break` statement at the end of every case except in rare circumstances. When every case ends with a `break` statement, `switch` works the same way as `case`. We’ll see the `break` statement appear again shortly, in connection with loops.

---

> [!NOTE]

Python offers neither `switch` nor `case`, and multi-way branches must be written either as `else`/`if` sequences or by using Python dictionaries and functions, as in the following example:

```
	def case_penny():
	  print "Got a penny!"
	def case_tuppence():
	  print "Got a tuppence!"
	def case_fivepence():
	  print "Got a five pence!"
	def default():
	  print "Got something else!"
	` 

	Coincases = {"1": case_penny, "2": case_tuppence, "5": case_fivepence}
	` 

	x = raw_input("Coin value? ")
	` 

	if Coincases.has_key(x):
	  Coincases[x]()
	else:
	  default()
```

### Repeat Loops

When a statement or compound statement must be executed multiple times, it’s done within a framework called a _loop_. There are three general types of loop in programming:

- **`repeat`** **loops:** These take some action and then test a condition. If the condition evaluates to `true`, the loop ends. Otherwise, the action is repeated. - **`while`** **loops:** These test a condition first. If the condition evaluates to `true`, some action is taken. Otherwise, the loop ends. - **`for`** **loops:** These take action once for every value in a collection of values. In computer science this is called _iteration_.

The `repeat` loop is the simplest to understand. It’s illustrated in Figure 5-14. The sense of the logic is that some action is repeated until a condition becomes `true`. At that point the loop ends. If the test at the end of the loop turns up `false`, execution returns to the top of the loop and begins again. What’s important to remember is that a `repeat` loop’s action is always performed at least once.

![[FIGURE 5-14:](#08_9781119183938-ch05.xhtml#rc05-fig-0014) The `repeat` statement](./media/images/9781119183938-fg0514.png)

The `repeat` statement uses the `repeat` and `until` keywords in Pascal and languages descended from Pascal. In C and other C-like languages, `repeat` loops are implemented with the keywords `do` at the beginning of the loop and `while` at the end. The flow of control is the same, but the sense of the test is reversed, so the loop terminates when the test returns `false`.

### While Loops

The `while` loop is like a `repeat` loop upside-down: The test is made at the _beginning_ of the loop rather than at the end. The condition is tested, and if the test returns `true`, the loop’s action is performed. After each pass through the loop, the condition is again tested at the top. When the condition returns `false`, the loop ends. If the condition is initially found to be `false`, the loop ends immediately and its action is never taken at all. See Figure 5-15.

![[FIGURE 5-15:](#08_9781119183938-ch05.xhtml#rc05-fig-0015) The `while` statement](./media/images/9781119183938-fg0515.png)

### For Loops

There are times when you need to perform an operation once for each element in a collection of values, rather than looping until a condition becomes `true` or `false`. This is called a `for` loop. Some languages restrict `for` loops to iterate over a sequence of monotonically increasing or decreasing integers that differ by a fixed step. So, for example, in Pascal we might write:

```
	FOR i := 10 TO 20 DO  { Display every integer from 10 to 20 }
	  WRITELN(i);
```

or in some dialects of BASIC it would look like this. (The `REM` means that the line is a remark; that is, a comment):

```
	REM PRINT 0, 2, 4, 6, 8, 10
	` 

	FOR I = 0 TO 10 STEP 2
	  PRINT I
	NEXT
```

The variable that takes on the integer value for the current iteration is referred to as the _loop counter_. It’s possible that the loop counter is used simply as a counter and takes no part in the work done by the loop statements other than to dictate the number of times that the statements in the loop are executed. Most of the time, however, the loop counter is used to access elements in an array or to take part in some calculation.

Python supports iteration over arbitrary collections of values, so we might write the following. (In Python, a line beginning with “#” is a comment):

```
	# print "foo", "bar", "baz"
	for s in ["foo", "bar", "baz"]:
	  print s
```

A BASIC-like `for` loop can be implemented in Python using the built-in function `range()`, which generates the sequence of integers between a start and an end value with an optional step value. We could write the preceding BASIC example like this:

```
	# print 0, 2, 4, 6, 8, 10
	for i in range(0, 12, 2):   # ranges do not include the end value
	  print i
```

C provides a very flexible `for` loop construct that behaves like a generalised `while` loop. It allows the user to specify an initialisation operation to occur before the loop, a loop test that is evaluated before each iteration and must be non-zero for the loop to proceed, and an operation to perform to move to the next element. So we might write the following to iterate over and print every element in a linked list:

```
	LINK_T *link;
	for (link = start; link != NULL; link = link->next)
	  printf ("%d\n", link->payload);
```

Figure 5-16 shows the logic of `for` loops.

![[FIGURE 5-16:](#08_9781119183938-ch05.xhtml#rc05-fig-0016) The `for` statement](./media/images/9781119183938-fg0516.png)

### The Break and Continue Statements

Many languages provide two special-purpose control statements that are used almost exclusively in loops. A `break` statement ends the loop unconditionally. Execution continues with the next statement after the innermost enclosing loop. `break` may be placed anywhere in the loop, usually under the control of an `if`/`then`/`else` statement inside the loop. (As we saw earlier, the `break` statement is also used in `switch` statements.)

The `continue` statement may also be placed anywhere inside the loop, generally under the control of an `if`/`then`/`else` statement. When executed, `continue` jumps immediately to the test that governs the loop, so that the test is made again. In a sense, `continue` “short-circuits” the current pass through the loop. See Figure 5-17 to see the operation of `break` and `continue` shown side by side.

![[FIGURE 5-17:](#08_9781119183938-ch05.xhtml#rc05-fig-0017) The `break` and `continue` statements](./media/images/9781119183938-fg0517.png)

---

> [!NOTE]

The example shown in [Figure 5-17](#08_9781119183938-ch05.xhtml#c05-fig-0017) is a `while` loop, but `break` and `continue` work in all loop types.

It’s worth remembering that `break` and `continue` are not necessarily present in all programming languages. Some languages support one or both under different keywords; for example, `continue` is implemented in Ruby as the `next` keyword.

### Functions

In an imperative programming language, a _function_ is a named sequence of statements. When the function is called from elsewhere in the program, its statements are executed until execution reaches the end of the function or a `return` statement, at which point the function ends and execution continues at the statement following the call to the function (see Figure 5-18). Functions allow common tasks to be defined in one place and used whenever necessary, keeping duplication of code to a minimum.

![[FIGURE 5-18:](#08_9781119183938-ch05.xhtml#rc05-fig-0018) Function calls and returns](./media/images/9781119183938-fg0518.png)

That’s how functions operate from an execution standpoint. They have another very important trick: you can pass data values into a function. The function, having made use of those data values, can return one (or in some languages more than one) new value to the code that calls it. Because functions may return values, they can be used in expressions as well as statements. Figure 5-19 shows how this works. The `CalculateArea` function accepts a numeric value representing the radius of a circle, and returns a value calculated as the area of a circle. Radius in, area out.

![[FIGURE 5-19:](#08_9781119183938-ch05.xhtml#rc05-fig-0019) Passing values to and from functions](./media/images/9781119183938-fg0519.png)

A function can take zero or more _parameters_, which are special-purpose variables that “carry” values across the gap between the function and the code that calls it. The names and (for statically typed languages) types of a function’s parameters are given when the function is defined in your source code. In [Figure 5-19](#08_9781119183938-ch05.xhtml#c05-fig-0019), `CalculateArea` has a single parameter, `R`.

When a function is called, we must specify an argument corresponding to each of the function’s parameters. An _argument_ may be a literal or named constant, or a variable, or the result of an expression. In [Figure 5-19](#08_9781119183938-ch05.xhtml#c05-fig-0019), the main program declares a variable named `Radius`. `Radius` is assigned the value 17, and is then used as the argument to `CalculateArea`, providing the initial value of the parameter `R`. `CalculateArea` can use `R` as a variable during calculations. It defines its own variable `A`, and assigns the calculated area value to it. `A` is then specified as the function’s return value. The function takes the value from `A` and carries it back to the statement that called it. The main program’s variable `Area` accepts the value from the function and can display it or use it anywhere else a value may be used.

### Locality and Scope

A function may define its own constants, variables, types, and even (in many languages) its own functions, like Russian nested dolls. If you’re perceptive, the question will soon arise: what if the identifiers that a function defines conflict with those defined elsewhere in the program? If a function defines a variable called `Area`, and there is already a variable called `Area` defined outside the function, which variable is accessed when you use the `Area` identifier?

This problem involves the _scope_ of an identifier, which may be simply defined as the places in a program where a given identifier may be “seen” by the code. In most languages, identifiers that are defined within a function are local to that function. Anything defined outside a function is not local to anything so its scope is said to be _global_.

Figure 5-20 illustrates global scope. The example program defines two functions: `CalculateArea` and `CalculatePerimeter`. It also defines the constant `pi` and two variables: `Area` and `Radius`. All of these definitions are global. Each of the two functions has its own local definitions. Both define a named constant: `TheAnswer`. `CalculateArea` defines a local variable called `Area`. Each function defines `TheAnswer` with a different value. Several questions arise:

- If the main program references `TheAnswer`, which value does it get: 17 or 42? - Can the `CalculateArea` function call `CalculatePerimeter`? - Can one of the functions redefine `pi` as 3.0? - If `CalculateArea` assigns a value to its local variable `Area`, is the global variable `Area` affected? How about vice versa?

![[FIGURE 5-20:](#08_9781119183938-ch05.xhtml#rc05-fig-0020) Global and local scope](./media/images/9781119183938-fg0520.png)

These questions can be answered by applying four general rules:

- Local can see global. - Global can’t see local. - Local can’t see other local. - Local can define a local item under the same identifier as a global item, and thus hide global.

Let’s use these rules to answer the four questions:

- The main program can’t reference either local definition of `TheAnswer`. Global can’t see local. - `CalculateArea` can call `CalculatePerimeter`. Both functions were defined at global scope, and local can see global. - Either function could define an identifier called `pi`, giving it the value 3.0, 17.76 or anything else. In doing so it would hide the global constant `pi`: subsequent uses inside the function would see the new identifier, whereas uses elsewhere in the program would continue to see the original one. - Nothing the main program does to its variable `Area` affects the local variable `Area` defined by `CalculateArea`. Global can’t see local. Nor can `CalculateArea` change the main program’s global variable `Area`. But wait… can’t local see global? Of course. But in this case, `CalculateArea` has defined a local variable with the same identifier as a global variable. From `CalculateArea`’s perspective, the global variable `Area` is now hidden because `CalculateArea` used the identifier `Area` to define its own local variable `Area`. The global `Area` is hidden by the local `Area`.

The rules are not there solely to impose order. In most languages (including C, C++, Java, Ada and Pascal) a function’s arguments and local variables literally do not exist unless the function has been called and is running. A function’s arguments and local variables are set up on the system stack (which is explained in [Chapter 4](#07_9781119183938-ch04.xhtml)) by the code that calls the function. When the function returns, those arguments and local variables are removed from the stack and no longer exist. Languages like Python still use the idea of scope, even though functions are handled in an entirely different way “under the skin”. Scope is a subtle business, and as with almost everything else in programming, the details vary widely from language to language. Worse, there are occasional language implementations that permit certain tricks allowing code to violate the rules of scope. This is always a bad idea.

Scope will come up again in the next section of this chapter.

## Object-Oriented Programming

Up to this point, we’ve drawn a hard distinction between code and the data that code operates upon. For the first three decades of digital computing, tools and development methodologies largely reflected this separation. A programmer would define a collection of functions to perform the operations required of the program, and a collection of concrete data structures (arrays, structs or records and so on) to contain the program’s state. For large applications, the choice of functions and structures is typically informed by a _domain modelling process_ during the design phase; this aims to capture the relevant real-world entities (perhaps vehicles and people for a government vehicle licensing application), constraints (every vehicle has a single owner) and operations (transferring ownership of a vehicle, applying for a driving licence) in the domain where the program will be used.

In the 1970s, computer science researchers at a number of institutions began to experiment with a new conceptual model for programming, which became known as _object-oriented programming_ (OOP). OOP attempts to reduce the semantic gap between the design and implementation phases of the development process by providing facilities at the language level to describe entities and the operations that can be performed on them. A new species of data structure was born—the _object_—which expands on the notion of a struct or record (see the section in this chapter entitled “[Types and Type Definitions](#08_9781119183938-ch05.xhtml#c05-sec-0038)”) by also incorporating the functions that act on its internal data.

The jargon changed, as jargon often does when new concepts appear. Programmers define _classes_ of object, which often correspond closely to the entities identified during domain modelling; in the case of our vehicle licensing example the programmer might define a class `Car` and another class `Person`. As the program runs, individual objects will be created in memory, each of which is an _instance_ of some class; we might have millions of instances of class `Car`, of which one represents my car, and millions of instances of class `Person`, one of which represents me. A class definition describes the data elements (variously called fields, attributes or properties), which each instance of that class will possess, and a function (generally called a method) for each operation that can be performed on an instance. An instance of `Car` might have a string field `license_plate`, and a field `owner` that refers to the instance of `Person` that corresponds to the car’s current owner, and a method `change_owner` to change the current owner. Figure 5-21 provides a summary of this terminology.

![[FIGURE 5-21:](#08_9781119183938-ch05.xhtml#rc05-fig-0021) Classes and objects](./media/images/9781119183938-fg0521.png)

Don’t get the terms class and object mixed up. A class is a type definition; it exists in your source code. An object is an instance of a class, and is a real data item in memory at runtime, allocated and initialised according to the specifications of its class and the particulars of the language you’re using.

In most languages, new objects are initialised by a special constructor method defined in the class definition. When an object is no longer needed, it may be explicitly destroyed (in languages like C++), or removed by automatic garbage collection (in languages that offer it, like Java). Any cleanup required is handled by a special destructor or finaliser method. In most cases, objects are referred to via references, which are effectively pointers to the location in memory where the object’s data is stored; when a new object is created, and the constructor has been executed, a reference is returned that can be used to access the object’s fields and call its methods.

The syntax for defining classes, for creating objects and for accessing their fields and records varies widely among languages. Let’s take a look how a simple version of `Car` might be defined and used, first in C++:

```
	class Car
	{
	  Person *owner;
	  char *plate;
	` 

	  Car(Person *owner, const char *plate)
	  {
	    this->owner = owner;
	    this->plate = strdup(plate);
	  }
	` 

	  ~Car()
	  {
	    free(this->plate);
	  }
	` 

	  void set_owner(Person *owner)
	  {
	    this->owner = owner;
	  }
	};
	` 

	Car *my_car = new Car(me, “RN04 KDK”);
	` 

	printf("%s\n", my_car->plate);
	my_car->set_owner(you);
and now in Python:



	class Car:
	  def __init__(self, owner, plate):
	    self.owner = owner
	    self.plate = plate
	  def set_owner(self, owner):
	    self.owner = owner
	` 

	my_car = Car(me, "RN04 KDK")
	` 

	print my_car.plate
	my_car.set_owner(you)
```

Most object-oriented languages are share three basic language features:

- **Encapsulation:** Classes define both the data elements (fields) that will be associated with each instance and the code (methods) that operate on them. - **Inheritance:** A class may be a _subclass_ of another class, meaning that it inherits the fields and methods of its _superclass_, to which it adds its own. - **Polymorphism:** An instance of a subclass may be used in a context where an instance of a superclass is expected.

The next sections look at each of these features in a little more detail.

### Encapsulation

The binding together of data with the code that manipulates it is called _encapsulation_. But what is it good for? After all, even in a language that lacks object-oriented (OO) features, nothing stops us from declaring a struct or record type, and writing a function that takes a reference to an instance of that type and performs operations on its elements.

The key distinction is that encapsulation usually implies a mechanism for _data hiding_, which is when the programmer has full control over which fields or methods are visible from outside the object. You can allow code from other parts of the program to “reach in” and directly read or write a field, or call a method, or you can declare the field _private_, which means it can only be accessed by the object’s methods. The methods then act as a sort of controlled interface to an object’s data. In C++ we might write:

```
	class MyClass
	{
	private:
	  int my_attribute;
	` 

	public:
	  int get_attribute();
	  void set_attribute(int new_value);
	};
	` 

	MyClass *c = new MyClass();
	` 

	// these lines will give compile-time errors
	` 

	int a = c->my_attribute;
	` 

	c->my_attribute = 42;
	` 

	// use the accessor methods instead
	` 

	int a = c->get_attribute();
	` 

	c->set_attribute(42);
```

The `my_attribute` field is declared private (using the access qualifier `private`), and so is only accessible to the `get_attribute()` and `set_attribute()` methods. The compiler can detect and reject attempts to access `my_attribute` directly.

A brief example may help to explain the importance of data hiding. Suppose you want to create a class that models a child’s piggy bank. A piggy bank contains coins of various denominations. The coins have a total value, but it might be interesting to record which denominations are present in the bank, and how many of each are there. The different coins are referred to by an enumerated type `CoinConstant`, with elements like `FivePence`, `TwentyPence` and `OnePound`. The interface to the object’s data will consist of methods to add a coin, remove a coin, report the number of coins of a given denomination and report the total value of all coins. In C++, the skeleton of our class might look like this:

```
	class PiggyBank
	{
	  // some internal state here
	` 

	public:
	  void add_coin(CoinConstant c) { … }
	  void remove_coin(CoinConstant c) { … }
	  int how_many_of(CoinConstant c) { … }
	  int total_value(){ … }
	};
```

The four methods represent the only access that the outside world has to the coin bank object’s data. The outside world cannot see the data’s internal representation at all.

There are a number of obvious ways to implement the `piggy bank` class. You could define a private counter field for each coin denomination. Or you could look around and see if there are any predefined library data types that would work as well or better. Most programming languages offer predefined data types called _collections_ that include arrays, lists and so on. A _bag_ is a collection data type that can tell you whether a particular value is present (in a way similar to the set data type) and also how many times that value is present in the bag. One bag collection inside your object would do almost the entire job of modelling the coin bank.

Whether you define the data yourself or use a “canned” data type instead doesn’t matter. The point is that the internal representation of the data remains hidden. If the data inside the coin bank object could be accessed directly from outside the object, outside code could make assumptions about the structure of the data, or change data in ways that have unintended consequences. By limiting data access to a small number of methods, access is controlled completely by the object itself, and you can change the internal representation of the data at any time without fear of breaking outside code that depends on the object’s internals.

Taken together, the definitions of a class’s methods (and any public data items, if they exist) are called the class’s _interface_.

### Inheritance

If encapsulation were the sole advantage of OOP, it would still be well worthwhile. OOP has other significant tricks up its sleeve, however, and the next one up for discussion is called inheritance.

Most languages allow new types to be defined in terms of existing types. This is routine and done in various ways, for example an array of real numbers, a set of characters or a struct containing members of several other types. A struct, in fact, may include another struct as one of its members.

This comes close to what _inheritance_ is: a class is defined as a child or subclass of an existing class. The child class inherits everything defined in its parent or superclass: all fields and methods defined in the parent class are available in the child class. The child class may add its own fields and methods that did not exist in its parent class; this extends the parent class but does not change the behaviour inherited from the parent class. Inheritance allows that too: a child class may redefine fields and methods belonging to a parent class. We say that the child class _overrides_ inherited elements.

Figure 5-22 illustrates how inheritance works. The base class `Shape` is used to model two-dimensional shapes as might be drawn in a flowcharting program. There’s not much in `Shape`: a constructor, a destructor and the fields `x,y` and `line_width`, which define where a shape is located on the screen and how bold a line the shape will use. A child class `Circle` is later defined as inheriting from `Shape`. The `Circle` class gets everything in `Shape` and adds a new property, `Radius`. It also defines a new method, `Redraw`, and defines its own constructor and destructor.

![[FIGURE 5-22:](#08_9781119183938-ch05.xhtml#rc05-fig-0022) How inheritance works](./media/images/9781119183938-fg0522.png)

Now, why do it this way? The key to understanding inheritance is to think of classes in a hierarchy that moves from an abstract base class at the top to specific child classes at the bottom. An ellipse is a specific kind of shape. A polygon is another kind of shape. If you’re writing a flowcharting program, you would probably define an `Ellipse` child class and a `Polygon` child class below `Shape`. Drawing a rectangle is different from drawing a pentagon, so under `Polygon` you would then create child classes like `Rectangle`, `Pentagon`, `Hexagon` and so on. Such a hierarchy is shown in Figure 5-23.

![[FIGURE 5-23:](#08_9781119183938-ch05.xhtml#rc05-fig-0023) A class hierarchy](./media/images/9781119183938-fg0523.png)

A circle is a special case of an ellipse, and a square is a special case of a rectangle. This is why `Circle` is a child class of `Ellipse`, and `Square` is a child class of `Rectangle`. Classes are generally created as belonging to this kind of hierarchy, with an abstract base class providing the methods and fields that all child classes have. Child classes add specificity, either by defining new methods and fields, or by overriding those that they inherit.

You may already be experienced in this kind of thinking. Consider text styles in a word processor or desktop publishing program. A generic paragraph style might specify the font and the type size and nothing else. You can then define more specific paragraph styles that add first line indents, space before and after, margin insets, bullets and numbering and so on. This is key: _the generic paragraph style contains only those style items that all paragraphs have_. This provides a default font and type size for all paragraphs—and also allows you to change the font in all paragraph styles by changing it only once in the basic paragraph style. Because the more specific paragraph styles are, in a sense, child classes of the basic paragraph style, they inherit the font and type size and can override it to whatever they need for the specific types of paragraph that they are.

If you have some grounding in OOP, it may occur to you that the example shown in [Figure 5-22](#08_9781119183938-ch05.xhtml#c05-fig-0022) isn’t optimal. You’re right, it isn’t—but to explain why, we first have to explain the third leg in OOP’s three-legged stool: polymorphism.

### Polymorphism

Key to the idea of object-oriented programming is that objects know what to do. If we want to draw a shape object, we call its `Redraw()` method. The object knows what sort of shape it is, and its `Redraw()` method allows it to redraw itself on the screen according to its class. The redrawing itself is done in class-specific ways, but the method name is the same for all shapes.

It sounds odd at first, but in OOP, you don’t always have to know the precise class of an object in order to call one of its methods. This feature goes by the heavy-duty word _polymorphism_, from the Greek for “many shapes”. Because objects know what to do, you simply have to tell them to go do it. You don’t have to tell them how.

A good metaphor for polymorphism is the humble farmer. There are many kinds of farmer who grow many different kinds of crops. However, all farmers have certain tasks in common: they prepare the ground, plant, tend and harvest. Each of these tasks is done in a different way for different crops; harvesting tomatoes is nothing like harvesting wheat. Tomato farmers know how to harvest tomatoes, and wheat farmers know how to harvest wheat. If a government weather office predicts that an early killer frost is coming later in the week, it would be enough to call or text all the farmers in the frost area with a simple message: “Harvest your crops now”. The weather office people don’t need to tell the farmers how to do their harvesting. The farmers know how. Telling them to start harvesting is enough.

In the programming world, polymorphism acts on classes in a hierarchy. If the base class in the hierarchy defines a method, then all classes that descend from it have that method. Each class may override the method with class specifics, but all classes in the hierarchy respond to a call to that particular method.

How does this work in practice? Let’s go back to our shapes example, and the scenario illustrated in Figure 5-24. A number of shape objects have been created, and all have been added to a collection. (We described the idea of collections earlier in this chapter in the section entitled “[Types and Type Definitions](#08_9781119183938-ch05.xhtml#c05-sec-0038)”.) Here, the collection is defined as a list of class `Shape`. Inside, the list is really a list of pointers to objects of class `Shape`. We can step through the list and perform an operation on each object in the list. In this case, for each object in the list, we call `Redraw()`. It works because all classes descending from class `Shape` contain everything that `Shape` contains. If class `Shape` contains the `Redraw()` method, so do all of its descendants.

![[FIGURE 5-24:](#08_9781119183938-ch05.xhtml#rc05-fig-0024) How polymorphism works](./media/images/9781119183938-fg0524.png)

This is why the example as originally configured in [Figure 5-22](#08_9781119183938-ch05.xhtml#c05-fig-0022) isn’t ideal. The `Redraw()` method wasn’t present in class `Shape` because `Shape` is so generic that there’s nothing to draw. However, if we intend to use polymorphism to call a method, that method must be present throughout the hierarchy. The proper place for the `Redraw()` method is in the hierarchy’s base class `Shape`, from which all other shape classes descend. This is true even if the `Redraw()` method is empty. A class like `Shape` that is not intended to be instantiated is called an _abstract class_. The whole purpose of an abstract class is to ensure that particular methods are defined in all classes that descend from the abstract class.

Polymorphism comes free in dynamically typed languages like Python and Smalltalk, because an association between an identifier and an object may be changed at any time, and every object carries with it type information that can be used to resolve which version of the method to call. In C++, however, the type of an identifier is determined at compile time, which can cause problems. Consider the following code:

```
	class Rectangle
	{
	  void name()
	  {
	    printf("Rectangle!\n");
	  }
	};
	` 

	class Square : public Rectangle
	{
	  void name()
	  {
	    printf("Square!\n");
	  }
	};
	` 

	Rectangle *r = new Rectangle();
	r->name();             // prints "Rectangle"
	` 

	Square *s = new Square();
	s->name();             // prints "Square!"
	` 

	Rectangle *r = new Square();
	r->name();             // prints "Rectangle!" even though r
	                       // points to an instance of Square
```

This defines a class `Rectangle`, with a method `name()` that prints “`Rectangle!”`, and a subclass `Square`, which overrides `name()` to print “`Square!”`. We instantiate a `Rectangle`, and call its `name()` method, which prints “`Rectangle!”`, as expected. Next we instantiate a `Square`, and call its `name()` method, which prints “`Square!”`, again as expected. The third example is more perplexing at first glance. We instantiate a `Square`, but store the pointer in an identifier that has type `Rectangle *` (pointer to `Rectangle`); this is semantically legal, as `Square` is a child of `Rectangle`, so every `Square` is also a `Rectangle`. However, when we call `name()`, the program prints “`Rectangle!”` rather than “`Square!”`. The reason for this is that the compiler decides which version of the `name()` method to call based on the type of the pointer `r`, rather than on the type of the object it points to.

The fix for statically typed languages is called _dynamic dispatch_, which looks at the object itself to determine the appropriate method body to invoke. A common mechanism for implementing dynamic dispatch is to have each object carry around a pointer to its class’s virtual method table, which points to the appropriate implementation of each method. In C++ methods must be explicitly tagged as `virtual` to be included in the virtual method table and thus be available for polymorphic calls; methods that are not flagged as virtual are subject to static dispatch.

### OOP Wrapup

OOP is both a programming technology and a way of thinking about structuring code and data. The basic idea is that data should be defined along with the code that manipulates it. A data type defining code and data together is a class. An object is an instance of a class; that is, a data item created in memory according to its class definition. Three basic principles define OOP:

- **Encapsulation:** Combines code and data into classes, and allows the programmer to control access to a class’s code and data through the use of access qualifiers and class functions called methods which have privileged access to fields. - **Inheritance:** Allows us to define a class as an extension of another class. Everything the parent class defines is inherited by the child class. This allows related classes to be combined into a hierarchy of classes moving from a generic parent at the root to specific descendant classes at the leaves. - **Polymorphism:** Allows related classes in a hierarchy to respond to method calls in cases where the caller does not know the precise type of the object on which is it calling the method. Metaphorically, the caller tells an object, “Do X: you know how”, and relies on dynamic dispatch to ensure that the correct implementation is called.

The details of how OOP is implemented vary significantly by language, and especially by whether a language is statically typed (C++, Object Pascal) or dynamically typed (Python, Smalltalk) but many of the principles are the same.

## A Tour of the GNU Compiler Collection Toolset

If you want to try native code programming on the Raspberry Pi, the easiest way involves a set of compilers and tools that predates Linux itself. Linux is written in C (with a very small amount of assembly language) and the GNU Compiler Collection (GCC) is the toolset used to build Linux from its source code files. The GCC is preinstalled in Raspbian Linux. This section takes you on a quick tour of the GCC toolset, with a test program in C.

### gcc as Both Compiler and Builder

The `gcc` is more than a set of compilers and utilities. The `gcc` program itself (always written in lowercase) is nominally the C compiler of the collection. However, in addition to being a compiler it’s also a sort of build supervisor. When you launch `gcc` to build a C program, `gcc` in turn launches several other tools present in the collection to complete the build. The `gcc` build process includes these four steps:

- **Preprocessing:** Expands macros and include files. To accomplish this step, `gcc` launches a preprocessor utility called `cpp`. - **Compiling:** Translates a preprocessed C file into its intermediate code, which for `gcc` is assembly language source code. The `gcc` program does the compilation itself. - **Assembly:** Translates the assembly language source code into native object code. The `gcc` program launches the GNU assembler, `as`, to perform this step. - **Linking:** Converts and binds together one or more object code files into a single native code executable file. The `gcc` program launches the GNU linker, `ld`, to perform this step.

All four of these steps may be accomplished by a single invocation of the `gcc` program. To see how it works, let’s build the classic “Hello, World!” program in C, using the `gcc`.

To begin, open the Raspbian file manager and create a work folder somewhere under the `pi` folder. It doesn’t matter what the folder is called; `tests` will work fine. Next, open a text editor window and enter the following short program:

```
	#include <stdio.h>
	` 

	int main (void)
	{
	printf ("Hello, world!\n");
	return 0;
	}
```

Save the C source code to a file named `hello.c` in your work folder. Navigate to your work folder with the file manager to be sure the file was saved. Then press the F4 key to open your work folder in a terminal window. (If F4 doesn’t launch a terminal window in the editing environment you’re using, you will have to launch one manually.) Enter the following command at the terminal command line:

```
	gcc hello.c -o hello
```

This command turns `gcc` loose on your source code file, and uses the `–o` option to direct it to generate an executable file named `hello`. (In general, Linux executable files don’t have file extensions.) Assuming you entered the source code correctly, `gcc` will do its work and return to the command-line prompt. In your work directory will now be the files `hello.c` and `hello`.

To run the executable, enter this command:

```
	./hello
```

The message will appear in the terminal window:

```
	Hello, world!
```

Now, let’s do it again, one piece at a time. Erase the executable file `hello`, and then execute this command in the terminal window:

```
cpp hello.c –o hello.i
```

The program `cpp` is the preprocessor utility. The `–o` command tells it to create an output file named `hello.i`. You’ll see the file appear in the file manager window. You can open `hello.i` in a text editor, but unless you’ve had some experience in C, it won’t make much sense to you. Basically, your test program is at the end, and the bulk of the rest consists of external function headers (in place of that `#include` preprocessor directive at the top of the original source) that allow your program to call functions in the standard C library.

The next step is to compile the preprocessed source code to intermediate code. Enter this command:

```
	gcc –S hello.i
```

Compilation is something that `gcc` itself does. The output in this case is `hello.s`, which is the program compiled into assembly language source code. The `–S` (uppercase) command tells `gcc` to create assembly source code and then stop. You can open `hello.s` in a text editor to see the assembly source code, and it’s an interesting exercise to see if you can follow the logic. Even if you’re writing in pure C on the Raspberry Pi, studying ARM assembly language may come in handy if you ever have to debug a peculiar problem. If you’re feeling really ambitious, or intend to pursue assembly language systematically, try invoking `gcc` with the options `–O1`, `-O2`, and `–O3` and then examining the code in the generated .s files. These three options (which use the letter “O” and not the digit “0,” by the way) instruct the compiler to apply increasingly sophisticated levels of optimization to the generated code.

That said, there’s a caution here: don’t try to learn how to write assembly language by using the assembly language source output of `gcc` as a model. A `.s` file produced by `gcc` contains all kinds of things that are necessary to generate machine code from a program originally written in C. Writing assembly language is a separate discipline, and you should learn it by reading books on assembly language.

If you’re not convinced, take a look at `hello.s` in a text editor. Then compare it to the “Hello, World!” program as written from scratch in assembly language:

```
	.data
	` 

	message:
	.asciz "\nHello, World!\n"
	` 

	.text
	` 

	.global main
	` 

	main:
	push {lr}@    Save return address on stack
	ldr r0, message_address    @ Load message address into R0
	bl puts  @ Call puts() function in clib
	pop {pc} @ Return by popping return address into PC
	` 

	message_address: .word message
	` 

	.global puts
```

In C work, it’s best simply to let assembly language be an intermediate language.

The third step is to assemble `hello.s` to an object-code file. Enter this command:

```
	as –o hello.o hello.s
```

This time, we’re using `as`, the GNU assembler. It will produce the object code file `hello.o`. Object code files contain binary machine instructions, and you can’t open them in a text editor to examine them in any useful way.

You can’t execute them either. That takes one last step, which is linking `hello.o` with a fair number of other things in the C runtime library. Unfortunately, linking a C program manually by invoking the linker `ld` at the command line is a very complicated business, and it is where `gcc`’s skills as a build manager really come in handy. Instead of having you type in hundreds of characters, we’ll run `gcc` again in _verbose mode_, during which it will display every command it issues to `cpp`, `as`, and `ld`. Enter this command:

```
	gcc –v hello.c –o hello
```

The `–v` command puts `gcc` in verbose mode. As you’ll see, while your terminal screen fills up and scrolls, it’s verbose with a vengeance. There’s a lesson here too: building a program is often complex even when the program itself is trivially simple. Unless you have a very good reason not to, let `gcc` do the heavily lifting on C projects.

### Using Linux Make

The `gcc` compiler is actually very good at managing the complexity of the build process, but it has limits. Once you go beyond simple test programs like “Hello, World!” you should study Linux `make`. In general terms, a _make utility_ is a software mechanism that coordinates the compilation and linking of multiple source code files into a single executable file. Make utilities pay special attention to two things:

- **Dependencies:** What source code files depend on what other files to provide functions, data definitions, constants and so on. - **Timestamps:** When a source code file was last changed, and when object code files and executables were last built.

When we say that File X _depends_ on File Y, we mean that we need File Y to build File X. Furthermore, a change in File Y requires that File X be rebuilt, otherwise File X may make assumptions about code or data defined in File Y that are no longer true. That can cause several kinds of error. For example, if a variable called `Distance` is defined in File Y as an integer, code in File X will use integer maths to manipulate the `Distance` variable. If we change `Distance` to a floating-point number in File Y, the integer maths code in File X may no longer work correctly. We then have to modify and rebuild File X to match the changes we made earlier in File Y.

Files may depend upon files that in turn depend upon other files. This is called a _dependency chain_. We saw this on our quick tour of `gcc`: an executable file depends upon one or more object files, which in turn depend upon one or more source files (see Figure 5-25).

![[FIGURE 5-25:](#08_9781119183938-ch05.xhtml#rc05-fig-0025) Dependency chains](./media/images/9781119183938-fg0525.png)

In [Figure 5-25](#08_9781119183938-ch05.xhtml#c05-fig-0025), a dependency chain begins at any block and follows the arrows to the executable file. All object files depend on their source files. The Library A object file depends on the Library A source file, and so on. Application Modules 2 and 3 both make calls into Library A, so both depend on Library A. Neither depends on Library B. Application Module 1 makes calls only into Library B, so it depends on Library B but not Library A. All chains end at the executable. This means that the executable file depends on everything.

The brute-force way to avoid problems when building the application executable is to rebuild _everything_ whenever _anything_ changes anywhere in the chart. That may work for simple projects, but once there are eight or 10 source code files, lots of time will be wasted rebuilding code that doesn’t depend on anything that has changed since the last build.

The make utility automates the build process. It uses file timestamps to determine what has to be rebuilt and what doesn’t. If an object file is newer than its source file, it means that any changes made to the source file are already reflected in the object file. Once edits are made to the source code, the source file will be newer than the object file. The make utility then invokes whatever tools are necessary to rebuild the object file.

The same is true of object code files that make use of code or data in other object code files. In [Figure 5-25](#08_9781119183938-ch05.xhtml#c05-fig-0025), the App 1 object code file calls functions in Library B. So when the Library B source code changes, Library B has to be rebuilt. However, because Application Module 1 calls functions in Library B, any changes to Library B will require that Application Module 1 be rebuilt as well. Because the application executable depends on everything, it must be newer than everything. Once some part of a dependency chain becomes newer than the application, the whole chain starting at the newer file must be rebuilt.

How does the make utility know what depends on what? It needs a road map, and on Linux operating systems, the road map is called a _makefile_. The makefile is a simple text file that describes dependencies among files, and how files are to be rebuilt. Its default name is `makefile`. If you define a project folder and all project files are present in that folder, you can use the default name. Once you have a makefile that describes your project, you can kick off a build by simply executing the program `make` in a terminal window. Even if there’s only a single source code file in your project, it’s less keyboarding to simply type `make` than, for example, `gcc hello.c -o hello.`

In its simplest form (as you’ll encounter it while learning a new language or programming generally) a makefile is a sequence of rules. Each rule has two parts:

- A line defining a target file and one or more component files. The target file depends on the component files. - A line immediately beneath it specifying the command used to build the target from its components. In Linux `make`, this second line must be indented from the left margin by a single tab character. The tab character helps `make` easily determine which line in the rule is which.

For our simple “Hello, World!” project in C, the makefile would contain only one rule:

```
	hello: hello.c
	  gcc hello.c –o hello
```

Type the rule into a text editor and save it as `makefile`, with no file extension. Then type `make`. If your executable file is older than your source file, `hello.c`, `make` will rebuild your executable by running `gcc` as shown in the second line of the rule.

As explained earlier, `gcc` hides some of the complexity of a build by automatically executing the preprocessor, assembler and linker as needed. If you’re not using compilers in `gcc`, you may have to spell out the steps separately in your makefile. Here’s an example makefile that invokes a non-`gcc` assembler and the `gcc` linker separately to create an executable:

```
	hellosyscall: hellosyscall.o
	  ld –o hellosyscall.o hellosyscall
	hellosyscall.o: hellosyscall.asm
	  nasm –f elf –g –F stabs hellosyscall.asm
```

Rules generally begin with the executable file and work back from there. The preceding makefile begins with the rule defining the dependency of the executable file on its object file, and how the executable is created with the linker `ld`. The second rule defines the dependency of the object file `hellosyscall.o` on `hellosyscall.asm`, and how the object file is built from the source file with a non-`gcc` assembler called `nasm`.

If your project has libraries or multiple modules with separate source code files, those rules would be included after rules building the executable. As a rule of thumb: the file that depends on everything (generally the executable) has the first rule in the makefile. The file or files that depend upon nothing but their own source would be last. Look back at [Figure 5-25](#08_9781119183938-ch05.xhtml#c05-fig-0025) and trace out its dependency chains if this isn’t clear to you.
