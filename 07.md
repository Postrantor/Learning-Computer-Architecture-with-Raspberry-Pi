# Flip-Flops: Where Bits Live

A _flip-flop_ is an electronic circuit that stores a logical state, conventionally described as either 1 or 0. Once set to a particular state by a digital signal on an input (typically a voltage change from 0 volts to 5 volts or 5 to 0) the flip-flop will maintain that state until another input signal changes it. Because a flip-flop can store one of two logical states, it is sometimes described as _bistable_. There are several different types of flip-flop, but the one most used in computer logic is the _D-type_, where D stands for “data”. The 1 and 0 states stored in flip-flops may be used to express computer data, hence the name.

A D-type flip-flop takes a snapshot of the D input every time it sees a low-to-high transition on its clock input (a rising clock edge), and presents it on the Q output until the next clock edge arrives. You can build complex systems by combining D-type flip-flops that store state with a combinatorial logic circuit to compute the next state from the current state and (optionally) external inputs.

Figure 4-4 presents a simple example. Assuming you’ve built a piece of combinatorial logic to add 1 to a four-digit binary number, you can implement a counter that increments a four-digit value stored in four flip-flops every time the clock ticks. The maximum clock speed is determined by the longest path through the cloud of combinatorial logic: you need to respond to a change in the values in the flip-flops and get the new value ready before the next clock edge comes along.

![[FIGURE 4-4:](#07_9781119183938-ch04.xhtml#rc04-fig-0004) A counter built from four flip-flops](./media/images/9781119183938-fg0404.png)

Another useful example is a shift register, shown in Figure 4-5. A shift register hands bits down the chain of flip-flops, advancing by one position every clock edge.

![[FIGURE 4-5:](#07_9781119183938-ch04.xhtml#rc04-fig-0005) A shift register built from four flip-flops](./media/images/9781119183938-fg0405.png)

Everything you see in this chapter is an elaboration of these fundamental principles: clouds of combinatorial logic and D-type edge-triggered flip-flops that store a digital state.

## Inside the CPU

As explained briefly in [Chapter 2](#05_9781119183938-ch02.xhtml), a computer program is just a long series of _very_ small steps. Each of these very small steps is called a _machine instruction_, and it is an “atomic” unit of action that cannot be divided further outside of the CPU. Each family of CPUs has its own unique roster of machine instructions. They may do similar things, but, in general, the machine instructions from one family of CPUs will _not_ execute on another family of CPUs. The definition of a CPU’s machine instructions and what they do is called its _instruction set architecture_ (ISA).

An instruction is represented in memory by a binary value some number of bytes long. (On many 32-bit CPUs like the ARM11 in the original Raspberry Pi, this number is four 8-bit bytes.) Within this binary number are encoded the identity of the instruction (called the _operation code_, or _opcode_), and one or more operands, which are values or addresses associated with the instruction. A binary machine instruction is loaded from memory into the CPU, where the CPU decodes it (takes it apart to determine what must be done) and then executes it, during which the actual work of the instruction is accomplished. When an instruction has been dispatched for execution it’s said to have been issued, and after it is completely executed it’s said to be retired*.* The next instruction in the program is loaded into the CPU for execution. (In modern CPUs, the process is more complicated than that, as we explain later in this chapter.)

From a height, program execution by the CPU works like this:

1. Fetch the first instruction in the program. 2. Decode the instruction. 3. Execute the instruction. 4. Fetch the next instruction. 5. Decode the instruction. 6. Execute the instruction.

…and so on, for as many instructions as are in the program. The _program counter_ is a pointer inside the CPU that contains the memory address of the currently executing instruction.

Machine instructions do things like

- Add, subtract, multiply or divide - Perform logical operations like AND, OR, XOR and NOT on binary values - Shift a multi-bit binary value to the left or right - Copy data from one location to another - Compare values against constants or other values - Perform CPU control functions

The values on which the machines operate may come from external memory or from one of a comparatively small number of registers inside the CPU itself. A _register_ is a storage location that can hold multiple bits at once; typically 16, 32 or 64, depending on the CPU. Results from machine instruction operations may be stored to memory or to registers.

In modern CPUs, separate subsystems execute different groups of machine instructions:

- **Arithmetic logic unit (ALU):** Handles simple integer maths and logical operations
- **Floating point unit (FPU):** Handles floating point maths
- **Single-instruction, multiple data (SIMD) unit:** Handles vector maths that performs operations on multiple data values at once. This type of maths is essential in audio and video applications.

A modern high-performance CPU may have multiple copies of each unit to support parallel execution of instructions, as we explain a little later.

### Branching and Flags

As useful as executing a linear sequence of instructions may be, the real magic of computing lies in the ability of a program to change its course of execution depending on the results of its work. This is done using _branch instructions_, which have the power to skip forward or backward in the sequence of machine instructions that make up a program. Some branch instructions—called unconditional branch instructions—tell the CPU to “just go” and load the next instruction from a memory address included in the branch instruction.

A conditional branch instruction combines a test of some sort with a branch. These tests generally involve a group of single-bit binary values called _flags_ that are stored somewhere in the CPU, generally in a group called the flags register or status word. When certain machine instructions execute, they set (change to binary 1) or clear (change to binary 0) one or more flags. For example, all CPUs have instructions that compare the values of two registers. If the values are equal, a flag (generally called the zero flag) is set to 1. If the values are not equal, the flag is cleared to zero. The flag is called the “zero” flag because of the way comparisons work. To compare two registers, the CPU subtracts one of them from the other. If the result of the subtraction is zero, they are equal, and the zero flag is set. If the result of the subtraction is anything but zero, the two registers are not equal, and the zero flag is cleared.

A machine instruction is just a binary number. Although it is possible to program directly in machine code, for convenience programmers generally use an assembler to convert assembly language directly into machine instructions. Instructions in assembly language are represented by a short string called a mnemonic, and the various operands are written in human-readable form. The assembly language representation of a conditional branch machine instruction might look like this:

```
	BEQ [address]
```

What the instruction does is to branch if equal (that is, if the zero flag is set) to the machine instruction stored at the specified memory address; if the zero flag is clear, execution continues to the next instruction in memory.

There may be a dozen or more flags in a CPU’s architecture. Some flags reflect equality or the fact that a register’s value has become zero. Some indicate whether an arithmetic carry has occurred. Some indicate whether a register has been set to a positive or negative value. Some indicate error conditions, like numeric overflow or an attempt to divide by zero. Some reflect the current state of the CPU’s internal machinery. For each flag there are one or more conditional branch instructions that check the value of the flag and branch accordingly.

In addition to supporting conditional branch instructions, the ARM CPUs used by the Raspberry Pi has a more general conditional execution feature in its instruction set that is described in some detail later on.

### The System Stack

There are a fair number of data structures catalogued and described by computer scientists including arrays, queues, lists, stacks, sets, rings and bags, among others. A few are used so often that some CPUs have hardwired support for them in their machine instructions. The most important of these is the stack.

A _stack_ is a last-in-first-out (LIFO) data storage mechanism essential to the operation of most modern CPUs, including the Raspberry Pi’s ARM11. The key characteristic of stack operation is that data items are removed from the stack in the reverse order of how they were stored.

A metaphor captures this well. If you’ve ever eaten in a school cafeteria, you may have seen a common mechanism for storing plates and saucers: a spring-loaded platform within a metal cylinder, adjusted to balance the weight of whatever plates it contains. When you place a plate in the cylinder the platform moves down just enough to make room for the next plate. When you need a plate, you simply take one from the top of the cylinder. With its load lightened, the platform rises just enough to bring the next plate to the top of the cylinder.

The key to the plate storage cylinder is that the first plate placed in the cylinder is all the way at the bottom. The last plate placed in the cylinder is at the top. The last plate stored is the first one taken out of storage—thus, “last in, first out”.

In a computer system, a stack is an area of memory set aside for LIFO data storage and managed by machine instructions designed to implement the stack data structure. Figure 4-6 shows a simple stack.

![[FIGURE 4-6:](#07_9781119183938-ch04.xhtml#rc04-fig-0006) How a stack works](./media/images/9781119183938-fg0406.png)

The stack begins at a location in memory specified by a _base pointer_. (A _pointer_ is simply a memory address.) After it’s loaded with an address, the value of the base pointer doesn’t change. A second pointer, called the _stack pointer_, indicates the memory location to be accessed next. It’s sometimes called the “top of the stack”. In [Figure 4-6](#07_9781119183938-ch04.xhtml#c04-fig-0006), the items at the top of the stack are shaded.

To add an item to a stack, the stack pointer is first incremented so that it points to the next available memory location in the stack. The data item is then written to that location. Informally, this is called pushing an item onto the stack.

To remove an item from the stack, the item at the top of the stack is first copied to a register or some other place in memory, and then the stack pointer is decremented so that it points to what had previously been the top item on the stack. This process is called popping an item from the stack. If you follow the four stack snapshots in [Figure 4-6](#07_9781119183938-ch04.xhtml#c04-fig-0006), you can see how the stack grows or shrinks as items are pushed onto it and popped from it. The last item pushed onto the stack is the first item popped from it—remember, last in, first out.

There are some variations on how stacks are implemented in any given architecture. An _ascending stack_, as just described, grows upwards in memory with each push by incrementing the stack pointer to the next higher memory location. A _descending stack_ grows downwards in memory with each push by decrementing the stack pointer to the next lower memory location. The ARM CPU stack can be configured to work either way, though by convention ARM stacks are descending. Some architectures assume that the stack pointer points to the first free memory location on the stack, whereas others assume that the stack pointer points to the last item pushed onto the stack. If the stack is empty, the stack pointer always points to the first available stack location. Again, ARM processors can be configured either way, but, by default, ARM stacks assume that the stack pointer points to the last item pushed.

Stacks are used for temporary storage of both data items (often register values) and memory addresses during subroutine calls. A _subroutine_ is a sequence of actions in a program that is executed as a group and given a name. Any time the subroutine’s actions need to be executed, some other part of the program can _call_ it, meaning transfer execution to the subroutine until the subroutine’s work is finished. Then the subroutine returns execution to the part of the program that called it. In programming languages like C and Python, subroutines are called _functions_. We’ll have much more to say about subroutines and their role in programming in [Chapter 5](#08_9781119183938-ch05.xhtml).

Many computer architectures provide a dedicated instruction for calling a subroutine, which automatically pushes the program counter value to the stack before branching to the start address of the subroutine. When the subroutine is finished, the saved program counter (referred to as the return address) may be popped back into the program counter by another dedicated instruction, and the program continues on its way. If the subroutine wants to use a CPU register (which is likely already in use by whoever called the subroutine), it can push the existing value to the stack itself, and pop it back before returning.

> [!NOTE]
> that although the ARM CPUs can choose to save subroutine return addresses on the stack manually, there is a faster way that doesn’t impose the time penalty of accessing system memory. As you see a little later in this chapter, return addresses are first stored in the link register (LR), allowing some leaf functions (those functions that do not call any functions in turn) to avoid stack accesses altogether.

Stacks are useful in that they can manage nested subroutine calls (subroutine calls made from within subroutines). Each time a new nested subroutine call is made, another layer of data and return addresses is added to the stack. Assuming that the stack has room, dozens or even hundreds of nested calls may be made. If the stack becomes full and no longer has room for additional values, an attempt to push anything on the stack causes a stack overflow. If there is no protection in place, for example from a memory management unit, data stored in memory areas adjacent to the stack are then overwritten, and program malfunctions occur.

### System Clocks and Execution Time

As described earlier in the “[Digital Logic Primer](#07_9781119183938-ch04.xhtml#c04-sec-0005)” section, everything that goes on inside a sequential circuit like a CPU is synchronised to a pulse generator called the _clock_. Each pulse from the clock triggers a _clock cycle_, during which the CPU does some specific work. In very old CPUs, a single machine instruction might take anywhere from 4 clock cycles to 40 clock cycles to complete execution. Different instructions took different times, and some (like multiplication and division instructions) took a lot more time than others.

Why did different instructions take more time? In the early decades of computing, machine instructions were implemented within the CPU silicon as sequences of _microinstructions_, which are very simple mini-steps from which more complex instructions may be built. (Microinstructions are not accessible from outside the CPU.) Microinstructions conserved space on the CPU chip by allowing a large number of machine instructions to be implemented by combining a far smaller number of microinstructions. The digital logic that implements instructions is thus shared across many instructions, reducing the total transistor count required. The list of microinstructions required to perform each instruction is called _microcode_.

Executing machine instructions implemented as microcode adds significant time to instruction execution. Whenever possible, CPU designers hardwire instructions; that is, they implement each instruction directly with transistor logic dedicated to that single instruction. This takes more transistor budget and more room on the chip than microcode, but it produces _much_ faster instructions. As more transistors could be fitted on a single chip, more and more instructions were hardwired and fewer relied on microcode. Even so, until fairly recently, the use of microcode forced some instructions to take more clock cycles to complete than others. Figure 4-7 shows how this worked on early computers that had slow instructions due to microcode.

![[FIGURE 4-7:](#07_9781119183938-ch04.xhtml#rc04-fig-0007) Machine instructions and clock cycles](./media/images/9781119183938-fg0407.png)

Higher transistor budgets allow more hardwired instructions. At some point, there are enough transistors on a chip to hardwire even complicated operations like multiplication and division. When all machine instructions are hardwired, all instructions execute in almost the same amount of time. The Holy Grail in CPU architecture has always been to execute all machine instructions in a single clock cycle. By 2000 or so that goal had mostly been achieved, and the chart of machine instructions versus clock cycles changed to something like Figure 4-8.

![[FIGURE 4-8:](#07_9781119183938-ch04.xhtml#rc04-fig-0008) Single-cycle machine instructions](./media/images/9781119183938-fg0408.png)

[Figure 4-8](#07_9781119183938-ch04.xhtml#c04-fig-0008) might make you think that instruction execution speed had hit a wall, and the only thing that could be done to get more instructions executed per second would be to increase the clock speed. You’d be wrong.

### Pipelining

There’s a misunderstanding about CPU operation and clock speeds: the CPU does not operate as quickly as the clock speed demands. The clock speed can only be as fast as the CPU allows. The CPU needs a certain amount of time to do what it does.

If you look closely at how a CPU executes a single machine instruction, you see that it happens in a number of relatively distinct stages:

1. Fetch the instruction from memory.
2. Decode the instruction.
3. Execute the instruction.
4. Write back any changes made by the instruction to registers or memory.

When a machine instruction is executed in a single clock cycle, all four stages happen in one wave of transistor activity. This wave propagates through the CPU from the logic that deals with fetching and decoding instructions through the execution stage to the write-back logic. It’s tough to make that wave proceed more quickly, and the maximum clock speed will be determined by the time taken to get a signal down the longest path through all that logic.

However, because the four stages occur in a specific order, you can treat each stage as a separate action. If you can engineer the logic that executes machine instructions such that all four stages take roughly the same amount of time, an interesting possibility opens up: you can overlap them. See Figure 4-9.

![[FIGURE 4-9:](#07_9781119183938-ch04.xhtml#rc04-fig-0009) Overlapping instruction execution](./media/images/9781119183938-fg0409.png)

In [Figure 4-9](#07_9781119183938-ch04.xhtml#c04-fig-0009), each stage of instruction execution takes one clock cycle. This means that the clock can be made faster, because executing an instruction now takes four ticks of the clock rather than one. This sounds like a step back in performance, even if the clock rate doubles. In fact, it sounds at first like a paradox: it takes four clock cycles to complete any single instruction, but one instruction is issued and another retires (that is, finishes its work) every clock cycle. The net result is that you still have instructions executing in a single, much faster clock cycle.

To get a sense of this, consider the sort of conveyor-belt pizza ovens that you see baking pizzas behind some pizza counters. The chef drops a raw pizza on the conveyor belt at the opening of the oven. Ten minutes later, the pizza emerges from the oven fully cooked and ready to sell. It takes 10 minutes to bake a pizza. However, there can be five pizzas making their way through the oven at any given time, and assuming that the chef keeps dropping raw pizzas on the belt, a finished pizza will emerge from the oven every two minutes. The first pizza takes 10 minutes. But once the oven is full, a pizza is finished every two minutes.

Overlapping the execution of machine instructions in this way is called _pipelining_. First implemented in supercomputers during the 1980s, pipelining is now the norm in virtually all CPUs, even Microchip Technology’s low-cost PIC (Programmable Intelligent Computer) microcontrollers. Pipelining is second only to memory caching as a contributor to recent CPU performance improvements.

### Pipelining in Detail

To get a feel for what pipelining involves, take a look at a simple hypothetical non-pipelined processor, as shown in Figure 4-10. Flip-flops hold the current state of the processor (the current program counter (PC) and registers), and a cloud of logic calculates the next state ready to be fed back into the D inputs of the flip-flops in time for the next clock edge. You can roughly divide this cloud into three parts: Instruction Fetch (IF), Decode (DC), and Execute (EX). In the IF part is some logic that works out the next program counter (PC) value—there are no branches in the hypothetical processor example. The registers aren’t needed until the EX part. At the start of each cycle the outputs of some of the flip-flops change, and during the cycle a wave of activity propagates from left to right through the logic cloud. The maximum clock speed is determined by the time taken to traverse the longest path through the cloud’s logic. During the latter parts of the cycle, the left-hand bits of the cloud have reached a steady state, and are just supplying the results to the still-changing logic in the right-hand part. Wouldn’t it be nice to take a snapshot of that steady state and let the left-hand bits get on with something else, such as fetching the next instruction? A pipelined processor inserts pipeline latches (again, flip-flops) into the cloud to do precisely that.

![[FIGURE 4-10:](#07_9781119183938-ch04.xhtml#rc04-fig-0010) A simple non-pipelined processor](./media/images/9781119183938-fg0410.png)

Figure 4-11 shows a processor with pipeline latches. In the illustration, we split the logic cloud into three subclouds. The IF cloud just needs to get the instruction from memory and figure out the next PC value in time for the first set of pipeline latches to record the result. It can then get on with fetching the next instruction during the next cycle, while the DC cloud logic decodes the previous instruction using the pipeline latch data as its input. The register read/write is all done during the EX part because we weren’t using the registers until the EX part of the original cloud, and we want to be able to write a value to the register file during one cycle and use it in the next cycle.

![[FIGURE 4-11:](#07_9781119183938-ch04.xhtml#rc04-fig-0011) Adding latches to create a pipeline](./media/images/9781119183938-fg0411.png)

The speed of the CPU is again determined by the time required to traverse the longest path in any part of the cloud, but because we chopped up the cloud into three parts, what was once the longest path is bound to be quicker than in the non-pipelined processor shown in [Figure 4-10](#07_9781119183938-ch04.xhtml#c04-fig-0010).

Looking at this, you might imagine that the EX stage is a bit “full”. All the interesting stuff, in particularly the arithmetic logic unit (ALU), lives there. If so, you’d be right: in a simple pipeline like this, the EX stage tends to contain the longest path, and thus constrains the pipeline. The next logical step, which you see in the ARM11 in the next section, is to subdivide the EX cloud into multiple smaller stages. This in turn requires you to cope with the issues that arise when the register file is read from and written to in different pipeline stages.

### Deeper Pipelines and Pipeline Hazards

How much overlap you can create in a given CPU depends primarily on how many stages a CPU’s instruction execution can be broken down to. Early on, 3- and 4-stage pipelines were state of the art. As you will see later, the ARM11 CPU inside the original Raspberry Pi has an 8-stage instruction pipeline, and many of the current Intel processors have pipelines with 20 stages or more. A challenge for CPU designers pondering longer pipelines is that the different stages of instruction execution don’t all take the same amount of time: because it takes one clock cycle to perform each stage, the length of the clock cycle governing CPU operation is the time required to complete the slowest pipeline stage.

Moving instructions through the pipeline at a continuous, uniform rate is crucial. Certain things can disrupt the smooth flow of instructions through a CPU pipeline. These are called pipeline hazards, and they can lead to delays in the pipeline. The delays are called pipeline stalls. There are three general categories of pipeline hazard:

- **Control hazards:** Caused by conditional branch instructions - **Data hazards:** Caused by data dependency between instructions - **Structural hazards:** Caused by resource conflicts

It’s easy to see how a conditional branch could disrupt a pipeline. If the first instruction shown in the pipeline in [Figure 4-9](#07_9781119183938-ch04.xhtml#c04-fig-0009) is a conditional branch instruction, and if (as is generally the case) the logic that resolves whether a branch is taken is located in the EX stage, you could end up branching away from sequential instructions that are already in the pipeline and have been fetched and decoded. Those instructions would no longer be in the path of program execution. So to preserve the illusion that you’re executing instructions one at a time they would need to be discarded and the pipeline would need to be refilled with instructions starting at the branch target address. Thinking back to the pizza-oven metaphor, if the order-taker submits an incorrect order to the chef, one or more pizzas already on their way through the oven may need to be discarded, and replacements placed on the belt. This leads to a pause before new, valid pizzas start to emerge from the oven—not to mention a loss of overall throughput.

One historical approach to control hazards is to abandon the illusion that you’re executing instructions one at a time and to say that our branches are delayed: sequential instructions that have entered the pipeline at the time when the branch is resolved always execute, regardless of whether the branch is taken. It is then up to the assembly-language programmer or high-level language compiler to find useful work to fill these branch delay slots.

This behaviour is uncommon, however. Most architectures attempt to mitigate the impact of pipeline hazards through two interrelated mechanisms: _branch prediction_ and _speculative execution_. Here, the CPU’s execution logic attempts to predict which of two possible branch destinations will be taken. The prediction is based on a cumulative history of branches taken in that part of the code. The CPU fetches instructions from the more likely destination before the actual result of the branch is known, and starts executing them speculatively. Recovering from an incorrect prediction involves killing the speculatively executed instructions before they reach a stage of the pipeline that can affect the outside world, generally by replacing them with _bubbles_ (no-op instructions, which do nothing). Speculative execution amounts to the CPU doing some guessing, and bad guesses are expensive. They incur a delay roughly proportional to the depth of the pipeline, which needs time to refill. A delay of 20 cycles is not unusual in a modern high-performance processor, so branch predictor improvements have become a major determinant of CPU performance.

Data dependence is more subtle. Suppose the result value from one instruction is needed as an operand by the next instruction in the pipeline. The second instruction may require the value before the first instruction has finished generating it. If you don’t stop the second instruction from proceeding through the pipeline it would end up using a value that is garbage or a leftover from some earlier calculation. This doesn’t happen in the simple pipelined processor described earlier, because reading the registers, computing the result and writing it back all occur in the EX stage. The registers are entirely consistent once the next instruction arrives at the EX stage. It’s only once you start to break the over-full EX stage apart (as almost all modern processors, including the ARM, do) that you need to worry.

Resource conflicts happen when two instructions in the pipeline need to access some CPU resource at the same time. For example, if two instructions in different pipeline stages need to access external memory via the cache system at the same time, one of these instructions must take priority over the other. A trivial example of this can occur between the IF stage reading instructions, and some other pipeline stage (the EX stage in our simple example) reading or writing data. This particular conflict may be partially resolved by splitting the unified level 1 cache into two separate caches: one for data and one for machine instructions. This is called a _modified Harvard architecture_, after Harvard’s early experimental computers that stored and accessed machine instructions and data separately. The ARM11 CPUs incorporate a modified Harvard architecture.

Detecting and resolving data dependence and resource conflict hazards takes still more transistors on the silicon to solve. The general approach is for the instruction decode logic to identify when a hazard is about to occur in the pipeline; the hardware that performs this check is referred to as an _interlock_. If a fetched instruction represents a hazard of any kind, a bubble is inserted into the pipeline ahead of the problematic instruction. This generates a delay that allows earlier instructions to finish what they’re doing before they conflict with instructions coming up the pipe.

### The ARM11 Pipeline

The pipeline in the ARM11 CPU is divided into eight stages, as shown in Figure 4-12. The pipeline isn’t quite as simple as the one shown in [Figure 4-9](#07_9781119183938-ch04.xhtml#c04-fig-0009). In addition to the pipeline being divided into eight different stages, there are three possible paths through the pipeline. Which path the execution takes depends on what type of instruction is executing.

![[FIGURE 4-12:](#07_9781119183938-ch04.xhtml#rc04-fig-0012) The ARM11 pipeline](./media/images/9781119183938-fg0412.png)

The first four stages are identical, regardless of the instruction. However, when the instruction is issued, the decode logic chooses one of the three possible paths. Each category of instructions has its own pipeline path:

- **Integer execution path:** For most instructions that execute integer operations - **Multiply-accumulate path:** For integer multiply instructions - **Load/store path:** For load and store instructions

The stages shown in the figure and their abbreviations are:

- **FE1:** The first fetch stage; the address for the instruction is requested and the instruction is received. - **FE2:** Branch prediction is done in this stage. - **Decode:** The instruction is decoded. - **Issue:** The registers are read and the instruction is issued. - **Shift:** Any required shift operations are done in this stage. - **ALU:** Any required integer operations are done in the ALU in this stage. - **Saturate:** Integer results are saturated; that is, forced to fall within integer range. - **MAC1:** The first stage for execution of multiply instructions. - **MAC2:** The second stage for execution of multiply instructions. - **MAC3:** The third stage for execution of multiply instructions. - **WBex:** Whatever register data was changed by the instruction is written back to the registers. WBex is the last stage on both the integer execution path and the multiply-accumulate path. - **Address:** Used to generate addresses used by the instruction to access memory. - **DC1:** The first stage during which the address is processed by the data cache logic. - **DC2:** The second stage during which the address is processed by the data cache logic. - **WBls:** The final stage in the load/store path writes back any changes made to memory locations.

Making things yet more complex is the fact that the integer execution path and the multiply-accumulate path are handled by the integer execution unit, and the load/store path is handled by the separate load/store unit. An _execution unit_ is a CPU subsystem that handles the “work” of an instruction—that is, integer maths or logical operations, memory access and so on. If a floating point coprocessor is present in the core, the coprocessor’s own pipeline, not shown here, handles execution once the instruction is issued. (We’ll explain coprocessors in more detail later on, in the section “[Coprocessors](#07_9781119183938-ch04.xhtml#c04-sec-0043).”)

### Superscalar Execution

As it turns out, still more performance can be wrung from the pipelining idea. A mechanism called _superscalar execution_ appeared towards the end of the 1980s. A superscalar architecture has an instruction pipeline like the one described in the previous section, as nearly all CPUs do today. However, a superscalar CPU issues more than one instruction for execution at the same time. Once issued, the instructions execute simultaneously. With superscalar CPUs, the execution of instructions goes beyond overlapping, to true parallelism. A superscalar pipeline is shown in Figure 4-13.

![[FIGURE 4-13:](#07_9781119183938-ch04.xhtml#rc04-fig-0013) Superscalar execution](./media/images/9781119183938-fg0413.png)

In a simple case like this, a superscalar CPU fetches two instructions from memory and examines them to determine whether they can be run in parallel. If so, the CPU parcels out execution of both instructions to dual execution units. The execution units are _not_ complete processor cores. They handle the work of the instruction only and specialise in integer maths and logic, floating point maths and vector maths. The CPU strives to keep all the execution units busy as much of the time as possible.

The basic mechanism is the same as with pipelining: the CPU checks for data dependencies in the instruction stream, such as whether an instruction provides a data value to the instruction that follows it. If such a dependency exists, the two instructions cannot be issued at once, and a pipeline stall occurs. For example, if one instruction adds a value to Register 4, and the next instruction in sequence multiplies the contents of Register 4 by still another value, the instructions cannot be issued together to run in parallel because the second instruction depends on data calculated by the first.

As with pipelining, the compiler that generates program code has the power to look for data dependencies and rearrange instructions so that two consecutive instructions do not depend on one another in ways that would trigger an interlock; that is, a situation where one instruction gets ahead of another on which it relies for data. These optimisations have become less important lately, because recent superscalar CPUs allow _out-of-order_ execution. Such CPUs have the ability to dynamically reorder the incoming instruction stream to maximise the amount of achievable parallelism and minimize data dependencies that cause interlocks.

Superscalar execution, and particularly out-of-order execution, is expensive in terms of transistor logic. In addition to the burden of providing duplicate execution units, the logic to implement dependency checks becomes increasingly complex. In theory it is possible for a CPU to issue more than four instructions at once, but at around this point designers generally reach a point of diminishing returns.

The ARM11 microarchitecture does not support superscalar execution. Superscalar capability was introduced into the ARM family with the “Cortex A” family of processors, some of which are capable of issuing four instructions at once. (More on Cortex later in this chapter.)

### More Parallelism with SIMD

Superscalar execution is difficult to implement but easy to describe: multiple instructions are issued at the same time, and they execute in parallel. Modern CPUs support another type of parallelism: instructions that operate on multiple data items at once. As a class, these are called single-instruction, multiple data (SIMD) instructions. Most computer architectures have their own SIMD instructions, which are generally not identical to or even compatible with those of other architectures.

SIMD is best explained by an example. Ordinary addition instructions in a 32-bit microarchitecture like ARM11 add one 32-bit value to another 32-bit value in a single operation. Other instructions perform subtraction in the same way. Certain common tasks in computing require that a great many additions (or other arithmetic operations) be performed as quickly as possible. Adjusting colour on a display is one such challenge. If you have a 1600-×-1200 pixel display, you have to process almost two million pixels. Each pixel, furthermore, requires three or four additions or subtractions to adjust colour. That’s a lot of maths, even if it’s simple and repetitive maths.

With traditional machine instructions, the only way to do all those additions and subtractions is one at a time (see Figure 4-14). Adjusting the whole group of pixels requires a program loop that takes one pass to process each value. (We’ll describe program loops in more detail in [Chapter 5](#08_9781119183938-ch05.xhtml).) Such a loop requires one branch per value, as well as an instruction to load the value and another to write the changed value back.

![[FIGURE 4-14:](#07_9781119183938-ch04.xhtml#rc04-fig-0014) Processing one value at a time](./media/images/9781119183938-fg0414.png)

There are tricks to minimise the number of branches required in such a loop, but tricks save only so much, and they come at the cost of additional instructions and additional memory. If you have to process two million pixels, it all adds up, and not in a good way.

SIMD instructions are designed to do the same work on more than one data value at a time. Whereas regular instructions operate on scalars (single values), we say that SIMD instructions operate on _vectors_. A vector is simply a one-dimensional array of data values arranged such that a given architecture’s SIMD instructions can act on them. Vectors are typically from two to 16 data values in length, with a width (the number of bits in each value) varying from architecture to architecture.

In many computer architectures, a single SIMD instruction performs four operations (addition, subtraction multiplication and division) at once, in parallel. In some computer architectures it may be more than four operations, but the principle is the same: a vector of four values is loaded from memory into registers. A SIMD instruction performs an operation on all four values in the vector simultaneously. Then the entire vector is written back to memory. Figure 4-15 illustrates this.

![[FIGURE 4-15:](#07_9781119183938-ch04.xhtml#rc04-fig-0015) How SIMD instructions work](./media/images/9781119183938-fg0415.png)

What would have taken four separate additions or subtractions is now accomplished with only one, saving three clock cycles. Better yet, in most architectures there are associated SIMD instructions that load and save four memory values at once.

Why build SIMD machines instead of increasing the superscalar issue width of the processor and allowing the programmer to stick with instructions that operate on scalars? The key benefit of SIMD is that the cost, in terms of time and energy, of fetching and decoding a SIMD instruction is shared across several computations. Because the programmer explicitly declares that the computations are independent by using a SIMD instruction, there is no need for expensive interlock logic to detect and work around dependencies that now cannot occur.

It’s not immediately obvious to beginners what SIMD instructions are used for but, as it turns out, the mathematics of sound and graphics (especially 3D graphics and video) requires a lot of repetitive maths on long sequences of values. A SIMD instruction can perform mathematical operations on long sequences of values at once. SIMD instructions can radically improve the performance of code that handles tasks such as encoding and decoding sound and video and managing 3D graphics.

The ARM11 core in the original Raspberry Pi supports SIMD instruction execution in a limited way: a 32-bit data word is loaded as always, but the SIMD instructions treat each of the 4 bytes within the word as a separate value. This obviously limits the size of the values that may be processed using SIMD, though a great deal in graphics and audio processing can be done with 8-bit quantities.

In the newer ARM Cortex CPUs, there is a coprocessor called NEON, which provides SIMD instructions that operate on multiple quantities stored in special 128-bit registers. This allows throughput over twice that of the SIMD instructions in the ARMv6 instruction set. You can read more on NEON a little later, in connection with ARM Cortex.

### Endianness

The first mass-market microprocessors were 8-bit units, which operated on data 8 bits (1 byte) at a time. They also read from and wrote to system memory 1 byte at a time. Later CPUs raised this to 16 bits and then 32 bits, with many architectures now reading and writing 64 bits at a time. Accessing multiple bytes from memory in a single read or write raises a non-obvious question: how are those multiple bytes ordered in memory? If a 4-byte or 8-byte quantity is read from memory, how does the CPU interpret those bytes?

This issue is called _endianness_, so named because of a bit of sly satire in Jonathan Swift’s novel _Gulliver’s Travels_, where the Lilliputians argue bitterly about whether to crack a soft-boiled egg on the wide (“big”) or narrow (“small”) end. It’s an important issue in computer architectures, if not in eggs. During this discussion, refer to Figure 4-16.

![[FIGURE 4-16:](#07_9781119183938-ch04.xhtml#rc04-fig-0016) Big-endian vs. little-endian architectures](./media/images/9781119183938-fg0416.png)

[Figure 4-16](#07_9781119183938-ch04.xhtml#c04-fig-0016) shows a short run of computer memory. Each location has an address and stores 1 byte of data. Address and data values are given in hexadecimal form. A modern 32-bit CPU like the ARM11 core reads or writes 4 bytes during every memory access. If those 4 bytes represent a 32-bit number, you need to know the order in which the 4 bytes appear in the number. In a columnar notation (see [Chapter 2](#05_9781119183938-ch02.xhtml) for a recap) the least significant column of a number is by convention shown on the right, and the most significant column is shown on the left. “Most significant” here means “highest value”. The rightmost column in 32-bit binary notation has the value of 2<sup>0</sup>, or 1. The leftmost has a value of 2<sup>31</sup>, or 2,147,483,648. (Refer to [Table 3-1](#06_9781119183938-ch03.xhtml#c03-tbl-0001) in [Chapter 3](#06_9781119183938-ch03.xhtml).) Order matters!

In a little-endian architecture, the least significant byte of a multi-byte value is stored at the lowest address of the four in memory. The most significant byte is stored at the highest address of the four. In [Figure 4-16](#07_9781119183938-ch04.xhtml#c04-fig-0016), the data at address 0x10000 is 0xE7. In a little-endian system, the value 0xE7 is interpreted as the least significant byte. In a big-endian system, the value 0xE7 would be the most significant byte. This changes the value of the 32-bit number radically: in a little-endian system, the hexadecimal value 0x00 11 04 E7 is 1,115,367 in decimal. In a big-endian system, the hex number changes to 0xE7 04 11 00, which in decimal form is 3,875,803,392.

Although abstruse technical issues favour little-endian architectures over big-endian ones, for the most part little-endian architecture has been a convention. Most recent microprocessor architectures, including Intel’s x86, have been little endian. (Motorola’s 6800 and 68000 and Sun Microsystems’ SPARC are notable exceptions.) Mainframe architectures like IBM’s venerable System 360 are often big endian.

By default, the ARM11 core is little endian. However, ARM architectures since ARMv3 offer an interesting feature: the endianness may be configured as either little or big as needed. This is called _bi-endianness_. Because computer networks are by convention big endian, allowing a CPU to interpret network data as big endian yields performance improvements, because the bytes of a value do not need to be re-ordered by the CPU.

The other place endianness matters crucially is in data files. Applications that operate on byte-resolution binary data in memory need to know whether a CPU has written that data to disk in big-endian or little-endian chunks. If a data file is moved to a system using a different endianness, the CPU may load the data in a different order, and an application that accesses the file may not be able to interpret its data correctly.

## Rethinking the CPU: CISC vs. RISC

Around 1980, a new concept, which came to be called _reduced instruction set computing_ (RISC), emerged from labs at IBM’s Thomas J. Watson Research Center, the University of California at Berkeley and Stanford University. The results of these research programs would eventually be developed into the popular POWER (Performance Optimization with Enhanced RISC), SPARC (Scalable Processor Architecture) and MIPS (Microprocessor without Interlocked Pipeline Stages) architectures, respectively, and they embodied a radically different vision of how CPUs should be designed compared to the state of the art at the time. The term _complex instruction set computing_ (CISC) was coined retroactively to refer to these prior architectures. The battle between RISC and CISC architectures has been one of the defining features of the computer industry over the last three decades.

By the mid-1970s, the design of high-performance CPUs for use in minicomputers and mainframes had come to focus on two key goals: increasing code density and bridging the semantic gap with the high-level programming languages of the day. Both of these goals led designers to pack more and more functionality into individual machine instructions. A look at instruction sets from the dawn of computing shows some wild and peculiar examples: one first-generation CPU had an instruction that triggered a camera aimed at an early video display; another had an instruction that raised the protective lid from the attached system printer. And remember, these weren’t library routines or utility programs. These were genuine, wired-into-the-CPU machine instructions.

The requirement for increased code density was driven by the high cost and low relative speed of memory. As explained in [Chapter 3](#06_9781119183938-ch03.xhtml), for most of the history of computing, system memory was horribly expensive. When memory was expensive, memory systems were necessarily small. The total physical address space of the DEC PDP-8 minicomputer was only 4,096 bytes. Back when the PDP-8 was designed, this was all the memory that a typical purchaser could afford. Larger programs could be run, but only after operating systems began to implement virtual memory (see [Chapter 3](#06_9781119183938-ch03.xhtml)).

Under these conditions, there was obviously an advantage in keeping programs physically short. Complex, semantically rich instructions help to reduce instruction count: a snap-the-camera machine instruction requiring 2 bytes in memory could take the place of a snap-the-camera subroutine that might require 50 or 100 bytes in memory. By the mid- to late-1970s, the availability of high-capacity DRAM chips had reduced the imperative to pursue code density at all costs. (As an aside, it was inexpensive memory, as much as inexpensive CPUs, that made the first personal computers possible: a $100 CPU chip won’t help you much if memory costs $5,000 per kilobyte.)

The term “semantic gap” refers to the difference between the behaviours expressible in high-level languages (nested loops, function calls, multidimensional array indexing) and those provided by the underlying hardware (conditional and unconditional unstructured branches, the ability to load and store from addresses in memory). Microcoding allowed designers to create instructions that directly implemented high-level features at the machine language level, closing the gap. A compiler, or a careful low-level programmer, could achieve significant performance gains by using these instructions, but in practice most compilers chose to ignore them for reasons of simplicity and portability between architectures. A rough 80/20 rule was observed, in which 20% of instructions were used 80% of the time, and many were not used at all. Tantalisingly, the “reduced instruction set” used by compilers bore a close resemblance to the microinstructions provided inside the CPU.

The earliest experimental RISC machines exploited this insight by providing only a very small instruction set comprising very simple instructions; they can be thought of as CPUs that simply exposed their microinstructions to the outside world. It takes more RISC instructions to implement a program, but program performance was excellent in comparison with CISC architectures; because RISC instructions ran very quickly, their simple execution made it easier to apply techniques like pipelining, and compilers hadn’t been using the more complex instructions anyway.

One distinguishing feature of RISC CPUs has always been that all or nearly all of their instructions are implemented in hardwired logic. Indeed, today microcode has been banished from the internals of even the main surviving CISC architecture—Intel x86. Since the Netburst microarchitecture was introduced in 2000, Intel processors have operated internally on RISC-like micro-ops, with legacy CISC instructions translated to independently issued micro-ops at the very front of the pipeline.

At the same time RISC processors have added instruction-set features in search of incremental performance and, ironically, code density, to the point that the once-sharp distinction between RISC and CISC has become thoroughly blurred. Much of the original motivation for simplifying instruction sets was motivated by a desire to repurpose limited transistor budgets toward new performance features, such as cache and greatly expanded register sets. As transistor budgets exploded during the 1990s, instruction set expansion became possible again. Today, many RISC architectures (including ARM) have roughly the same number of instructions as their CISC counterparts.

### RISC’s Legacy

Despite the blurring of the distinction between RISC and CISC, it is still possible to identify some key characteristics that the RISC movement brought to the CPU architecture table:

- Expanded register files - Load/store architecture - Orthogonal machine instructions - Separate caches for instructions and data

There is a fifth RISC characteristic that not everyone understands: RISC was a fresh start. With almost 40 years of experience to draw on, computer scientists reimagined CPU architecture from scratch. Assumptions based on the limitations of 20-year-old technology were cast aside. Requirements to support “legacy” code vanished. Intel’s current x86 architecture still reflects decisions made to allow easy conversion of programs for 1974’s 8080 chip to 1980’s 8086. RISC architectures had no such legacy to support.

Let’s take a closer look at these characteristics.

### Expanded Register Files

Taken as a group, a CPU’s registers are called its _register file_ or _register set_. Machine registers are “expensive” in terms of transistor budgets. Early CPUs had very few, and those they had were small. The 8080 had seven 8-bit registers that could be used in ordinary programming. The popular Motorola 6800 and MOS Technology 6502 had only three each. By contrast, the first ARM CPUs had 13 32-bit general-purpose registers, and the later POWER RISC processors had 32.

Registers are the fastest data storage locations in the entire computer. Reading data from memory takes much more time than processing data in registers. With enough registers to hold operands and intermediate results, a program can “stay out of memory” (and thus stay inside the far faster machinery of the CPU) as much as possible. This increases performance by avoiding round trips to memory (or at least to cache), and helps modern out-of-order superscalar processors to identify opportunities for instruction-level parallelism.

### Load/Store Architecture

In most CISC architectures, machine instructions can act directly on data stored in system memory. This was done because CISC architectures are old and generally “register-starved”. A typical CISC ADD instruction can add the contents of a register or an immediate value to a data word in memory:

```
	ADD [memory address], 8
```

This instruction adds the literal value 8 to the memory location at the address given in the first operand. Instructions like this are slow because they require two memory accesses for a simple addition: one to fetch the original value from memory, and another to write the new value back. In a real-world program, such an addition would be part of a longer sequence of actions. If these calculations could all be done within registers, memory would be accessed much less often. Alas, when all the registers are busy, there’s no alternative.

With access to a larger register file, RISC architectures generally remove memory-access powers from most instructions so that they act only on registers. Accessing memory becomes the speciality of a small cadre of machine instructions that do nothing else.

Designing a CPU this way results in a _load/store architecture_. Values are loaded from memory into registers by specialised load instructions, worked on within the registers and then written from the registers back to memory by specialised store instructions. The goal (as with almost everything in modern computer architectures) is to access memory as little as possible and to simplify the internal working of the pipeline.

### Orthogonal Machine Instructions

Most CISC instructions have deep historical roots. As computer architectures evolved across the 1950s and 1960s, new instructions were added to instruction sets in response to new needs. This tended to make CISC instruction sets hodgepodges of multi-byte instructions of several lengths. They were not designed as a group; instead they “just grew”.

The second problem with such ad-hoc instruction sets is that many instructions are special cases, in terms of how they access memory or registers. Early CPUs, for example, had one register called an _accumulator_, which held values acted upon by arithmetic and logical instructions. (The name comes from the fact that some very early computers and electromechanical tabulators accumulated intermediate results in a designated register.) Many early instructions had forms that treated the accumulator as a special case among registers.

Special cases make decoding and executing instructions more involved and time-consuming than they would be otherwise. So when computer scientists began designing new RISC instructions sets from scratch, they did away with special cases and made all instructions the same length. For 32-bit RISC architectures (including the original Raspberry Pi’s ARM11 CPU) this length is virtually always one 32-bit word.

An instruction set designed such that instructions are all the same length and CPU resources are treated without special cases is said to be _orthogonal_. The internal structure of machine instructions is also standardised to simplify instruction decoding, as we’ll explain later on.

### Separate Caches for Instructions and Data

As explained in [Chapter 2](#05_9781119183938-ch02.xhtml), the earliest computers, like Harvard University’s 1944 Mark I machine, stored machine instructions and data in entirely separate memory systems. John von Neumann pointed out that machine instructions are not physically different from data, and both should reside in a single memory system.

The computer scientists who created the early RISC CPUs backed away from von Neumann’s principle a little. They demonstrated that although code and data should be stored in a single memory system, there were performance advantages in having a separate instruction cache and data cache. The StrongARM microarchitecture was the first implementation of the ARM ISA to have separate code and data caches. The contribution of cache to CPU performance is shown by the fact that out of the 2.5 million transistors on the StrongARM silicon die, the designers chose to devote 60% to the two caches. The ARM11 microarchitecture also uses this “modified Harvard architecture” and has separate caches.

The reasons for the improved performance lie in the notion of locality, as explained in [Chapter 3](#06_9781119183938-ch03.xhtml). Machine instructions are generally stored in a separate area of memory from program data. More significantly, instructions in memory are usually accessed in sequence as a program is executed. Data are arranged as blocks of memory words that may be accessed in any order as the program’s needs require. Data access may not be truly random, but it’s rarely sequential. Separate code and data caches allow the use of different replacement policies and potentially cache line sizes (see [Chapter 3](#06_9781119183938-ch03.xhtml)) tailored to the access patterns of each cache.

Not all RISC architectures are the same, of course. Across RISC’s 35-year history, many things have been tried. It’s a measure of the success of RISC design principles that most modern CISC architectures incorporate many RISC characteristics, including the dominant CISC architecture, Intel’s x86.

The rest of this chapter focuses on a particular family of RISC CPUs: the ARM processors from ARM Holdings PLC, especially the ARM11 processor and the ARM CORTEX processors that followed it.

## ARMs from Little Acorns Grow

In early 1981, the British Broadcasting Corporation (BBC) began working on a project to foster computer skills among its audience, especially young people. The Computer Literacy Project required a solid and reasonably inexpensive mass-market computer to serve as a basis for the program. The project put out specs and asked for bids. The only design that met their specifications was the Proton from Acorn Computers, which was based, like the Raspberry Pi Foundation, in Cambridge. The Proton was based on the same 6502 microprocessor used in the popular Apple II machine. After its adoption by the BBC, the Proton became known as the BBC Microcomputer and more than 1.5 million were sold.

Once the IBM PC legitimised personal computers for business use, Acorn decided to create a higher-end unit to sell to the office market. It evaluated all the major microprocessors of the time, including the 8086 and the 68000, and found them unsuitable for various reasons. In 1983, Acorn began an ambitious project to design its own microprocessor for use in its high-end systems.

A team led by Acorn engineers Sophie Wilson and Steve Furber drew on research that came out of the Berkeley RISC Project. First silicon for the Acorn RISC Machine (ARM) CPU came back in mid-1985. ARM1 was a prototype that was never produced commercially. Production chips appeared in 1986, as the ARM2. ARM2 microprocessors first served as coprocessors in the 6502-based BBC Microcomputer to increase machine performance, particularly in areas like graphics and computer-aided design (CAD).

The first complete ARM-based microcomputer was released in 1987, as the Acorn Archimedes. The Archimedes included something new for Acorn: Arthur, an operating system with a fully graphical user interface. Arthur was later developed into RISC OS, which still exists.

---

> [!NOTE]

RISC OS is available as a free download for the Raspberry Pi. You can learn more about RISC OS (and obtain the release for the Raspberry Pi) at [`https://www.riscosopen.org/wiki/documentation/show/Welcome to RISC OS Pi`](https://www.riscosopen.org/wiki/documentation/show/Welcome).

Development of the ARM CPUs was spun off to a separate company in 1990, at which time the ARM acronym changed to Advanced RISC Machine. Advanced RISC Machines became ARM Holdings in 1998.

### Microarchitectures, Cores and Families

The nomenclature ARM uses for its products can be confusing at times. The instruction set architecture (ISA) of the ARM processors has a version number. A separate version number is given to the ARM microarchitecture. The term _microarchitecture_ refers to the way that a CPU designer implements an instruction set architecture in silicon. Think of it this way: the ISA defines the behaviour of a CPU, and the microarchitecture defines its structure.

ARM processors are grouped in families, each with its own microarchitecture version number. The first ARM ISA version was ARMv1, used only in the prototype ARM1 processor. The ARMv2 ISA was implemented in the ARM2 and ARM3 families of CPUs. ARMv3 was implemented in the ARM6 and ARM7 families, and so on. The original Raspberry Pi’s CPU belongs to the ARM11 family, which implements the ARMv6 instruction set. Processors within an ARM family generally differ in small ways that reflect emphases rather than significant architectural differences. The ARM11 microarchitecture applies to all four cores in the ARM11 family.

You’ll often hear ARM CPUs referred to as “cores”. The word _core_ is not a precise technical term in the computer industry. Most of the time it indicates any large independent component that may exist in a single-chip design containing multiple cores. In the ARM universe, a “core” is more specifically a CPU that may be incorporated into a custom device that includes non-CPU logic like USB and network ports, graphics processors, mass storage controllers, timers, bus controllers and so on. Such a device is called a _system-on-a-chip_ (SoC).

### Selling Licenses Rather Than Chips

The ARM-specific definition of “core” will start to make a little more sense once you understand the radical difference between the business models used by ARM Holdings and Intel. Intel designs and manufactures finished chips, each one in its own plastic or ceramic integrated circuit package, ready to be plugged or soldered into a computer circuit board. ARM Holdings, by contrast, is purely a design firm. Its engineers design CPU cores and other computer logic, and then license the designs to other firms. Firms that license ARM designs may customise them or integrate them with in-house logic to create a finished SoC design. They then take the design to a firm called a _chip foundry_ that manufactures integrated circuits for them.

As long as the computer industry was dominated by mature and mostly identical laptop and desktop PC designs, Intel’s business model predominated. However, after smartphones and tablet computers entered the mass market, customisation became crucial not only to differentiate products but also to evolve them. Innovation in ARM-powered devices extends all the way down to the CPU silicon. Most licensees use finished and certified ARM cores, but ARM has also licensed its ISA to a number of architecture licensees who then create their own custom cores representing a non-ARM microarchitecture. The earliest example of this is the StrongARM core, which was designed by Digital Equipment Corporation in the 1990s and later sold to Intel as XScale. StrongARM/XScale implements the ARMv4 ISA in a novel microarchitecture; it was the first CPU in the ARM line to incorporate separate instruction and data caches. More recent architecture licensees include Apple, with their Swift cores, and Qualcomm, with their Scorpion and later Krait cores.

The Raspberry Pi computers all use SoCs designed by Broadcom. The first generation boards contain a single ARM11 core. The second and third generation boards each contain four Cortex family cores. At this point we’ll turn to a more detailed description of the ARM11 microarchitecture. Later in this chapter, we will explore the Raspberry Pi’s SoC device and how SoCs are designed.

## ARM11

The ARM11 microarchitecture, announced in 2002, was the first, and so far the only, ARM family to implement the ARMv6 ISA. It’s a 32-bit microarchitecture, meaning that all machine instructions are 32 bits wide and that memory is accessed in 32-bit words. Some ARM machine instructions are designed to operate on smaller operands, of which there are two types: 16-bit halfwords and 8-bit bytes.

### The ARM Instruction Set

The ARMv6 ISA includes three separate instruction sets: ARM, Jazelle, and Thumb. Of these, the ARM instruction set is the most frequently used.

#### ARM

You’ll see an occasional ARM machine instruction in this chapter (and elsewhere in this book, including a complete program in [Chapter 5](#08_9781119183938-ch05.xhtml)) so it would be good to take a quick look at how machine instructions are built. Let’s look at a few examples.

---

> [!NOTE]

We say “built” advisedly, because ARM machine instructions allow various options to make them work in different ways, as needed.

The easiest machine instructions to understand are those that perform arithmetic operations on data. Remember from our earlier discussion that RISC machine instructions don’t access memory directly. All work to be done on data is done with data stored in registers. Consider the `ADD`</code> instruction, which adds the contents of two registers and places the sum in a third register. The general assembly-language form of an `ADD` instruction looks like this:

```
	ADD{<condition code>} {S} <Rd>, <Rn>, <Rm>
```

Instructions are summarised this way in most ARM instruction references. The notation works like this:

- Anything enclosed by curly brackets (`{}`) is optional. Anything not inside curly brackets is required. - Anything within angle brackets (`< >`) is a placeholder for a symbol or a value. - `Rd` means destination register. When an instruction has a destination register operand, the destination operand is the first after the mnemonic. `Rn` and `Rm` name the source register operands. The `m` and `n` don’t stand for anything specific.

Nearly all ARM instructions may be executed conditionally. (We cover this in some detail later in this chapter.) The optional `<condition code>` specifies 1 of 15 conditions that must be met before the instruction’s action takes place. If the condition code is not met, the instruction works its way through the pipeline but does not take any other action. If no condition code is specified, the default is “always”, meaning unconditional execution.

The optional `S` suffix directs the `ADD` instruction to modify the condition flags based on the result of the addition; these flags then control any subsequent conditionally executed instructions. Without the `S` suffix, a machine instruction does its work without changing the values of the flags. This means that a series of instructions can perform their work conditionally, based on an initial operation that sets the flags.

The following instruction handles adding the contents of register 1 (R1) to register 2 (R2) and placing the sum in register 5 (R5):

```
	ADD R5, R1, R2
```

To build the instruction such that it only executes if the `Zero` flag is set, you’d add the condition code `EQ` to the mnemonic:

```
	ADDEQ R5, R1, R2
```

Subtraction works in much the same way. An instruction to subtract R3 from R4 and place the difference in R2 would look like this, assuming the programmer wants the subtraction to set the flags:

```
	SUBS R2, R4, R3
```

Not all instructions take three operands. The `MOV` instruction copies a value stored in one register to another, or places a literal value into a register:

```
	MOV R5, R3MOV R5, #42
```

The first instruction copies whatever is in R3 into R5. The second stores the literal value 42 into R5.

Although it’s no longer generally available, the _ARM Architecture Reference Manual_ is very useful as an introduction to the several ARM instruction sets. (You can sometimes find available downloads by performing a Google search on the title.) Writing short assembly language programs and then inspecting their execution in a debugger is a good way to see what various instructions do. The GNU Compiler Collection, which is included with the Raspbian operating system, has a very good assembler. [Chapter 5](#08_9781119183938-ch05.xhtml) explains how to assemble and run short assembly language test programs.

#### Jazelle

The Jazelle instruction set allows an ARM11 core to execute Java bytecodes directly, without software interpretation. ([Chapter 5](#08_9781119183938-ch05.xhtml) explains bytecode languages like Java and Python.) ARM Holdings deprecated Jazelle in 2011, which means that the company will not be evolving the technology any further and recommends that it is not used in new projects.

---

> [!NOTE]

Computer manufacturers sometimes _deprecate_ a feature or a product line once they feel it has reached the end of its useful life. This does not mean that they disable it but rather that they advise strongly against its future use. Many manufacturers add that a deprecated product or feature may well be withdrawn at some time in the future, or that support for it will be eliminated in various ways. Deprecated features and products should not be used in new designs for those reasons.

#### Thumb

The Thumb instruction set is a 16-bit implementation of the 32-bit ARM instruction set. Thumb instructions are 16 bits wide instead of 32 bits wide. This allows greater _code density_, meaning that more instructions (and thus more functionality) may be stored in a given quantity of memory. Some low-end devices have limited memory, and they access that memory 16 bits at a time over a 16-bit system bus. Thumb instructions are designed to make more efficient use of such a bus. Thumb instructions still process 32-bit quantities in registers. Not all registers are fully available to Thumb instructions, and certain other hardware resources are available in only limited ways.

The Thumb instruction set is interesting for another reason: after Thumb instructions are fetched from memory or cache, they’re expanded to ordinary ARMv6 instructions by dedicated logic inside the CPU. After they enter the instruction pipeline, they’re no longer Thumb instructions at all. Thumb instructions are thus a sort of shorthand that allows more instructions to fit in a given amount of memory. The Thumb instruction set is generally used in programming _embedded systems_, which are devices that incorporate microprocessors and software to do their work but are not general-purpose computers themselves. The line is not sharp: the Raspberry Pi is often used for embedded systems, even though it has enough memory and CPU power to function as a conventional desktop computer.

---

> [!NOTE]

When the ARM11 core is executing Thumb instructions, it’s said to be in the Thumb state. Similarly, the core is in the Jazelle state while executing Jazelle instructions. In virtually all cases, the Raspberry Pi operates in the ARM state, using the full 32-bit ARM instruction set.

Don’t confuse the processor _state_ with the processor _mode_. Read on.

### Processor Modes

Early desktop operating systems did little or nothing to prevent applications from misbehaving. CP/M-80 systems, in fact, had so little memory that much of CP/M-80 basically removed itself from memory after launching an application and then reloaded itself when the application terminated. PC-DOS remained in memory, but Windows was a user interface running over PC-DOS rather than an operating system until Windows NT was first released in 1993. CP/M-80 and PC-DOS are more correctly considered system monitors than operating systems.

---

> [!NOTE]

A _monitor_ is system software that loads and runs applications but does little in terms of managing system resources.

Part of the problem was a shortage of memory, but a greater part was that the CPU chips at the time had no ability to protect system software from application software. In 1985, Intel’s 386 CPUs were the first Intel chips to offer a practical _protected mode_, which provides the operating system kernel with privileged access to system resources denied to applications (which run in real or User mode) and was a prerequisite for implementing a true operating system on Intel processors. All modern CPUs intended for use in general-purpose computers contain logic to manage system resources and prevent applications from interfering with the operating system and other applications.

ARM11 processors provide several different modes to support operating system management of user apps and the computer’s hardware. These are summarised in Table 4-1. All but User mode are considered _privileged_ modes, meaning that they have full access to system resources. Supervisor mode is specifically for use by operating system kernels and other protected code connected with operating systems. System mode is basically User mode with full privileges and access to all the hardware. It is not used much, except in low-end embedded work; it’s considered obsolete.

<figure> <figcaption>

[Table 4-1](#07_9781119183938-ch04.xhtml#rc04-tbl-0001) ARM11 Processor Modes

</figcaption>

**Mode** **Abbreviation** **Mode bits** **Description** ---------------- ------------------ --------------- -------------------------------------------------------------------------------------------- User usr 10000 For user application execution Supervisor svc 10011 For the operating system kernel System Sys 11111 Now obsolete Secure monitor mon 10110 Used in TrustZone applications FIQ fiq 10001 For “fast interrupt” servicing IRQ irq 10010 For general-purpose interrupt servicing Abort abt 10111 For virtual memory and other memory management Undefined und 11011 For software emulation of undefined machine instructions, as in coprocessors or newer ISAs

</figure>

The FIQ, IRQ, Abort and Undefined modes support interrupts and exceptions. _Interrupts_ are signals from hardware devices outside the CPU indicating that the device requires attention. _Exceptions_ are anomalous events within the CPU that require special handling by the CPU, generally in cooperation with the operating system. These include virtual memory page faults and arithmetic errors like divide-by-zero. We mention these again in connection with registers.

The System Monitor mode is used with an ARMv6 feature called TrustZone, which creates isolated memory regions called _worlds_ and manages data transfers between them. TrustZone is used primarily in content digital rights management (DRM) to prevent programs from “sniffing” decrypted content in memory and writing it out to storage. TrustZone is not implemented in all ARM11 processors, and requires special changes to behaviour of the system data bus used in SoC designs. TrustZone is not available in the BCM2835 SoC in the Raspberry Pi.

ARM’s Supervisor mode is the mode used by the operating system kernel. The kernel and the memory it runs in are often called _kernel space_. When an ARM system is reset, the CPU is placed in Supervisor mode and the kernel begins executing. In Unix/Linux jargon, _userland_ is the memory and software environment where user applications run. Some operating systems place noncritical device drivers in userland, along with software libraries that provide an interface to the OS and certain hardware resources.

Most of the differences between the several processor modes have to do with the use of the ARM register file. Let’s take a closer look at the ARM family’s register riches.

### Modes and Registers

One of the fundamental decisions behind RISC CPU design is to put as many registers as is practical within the CPU. The more registers a CPU has, the less often it has to access instruction operands in memory or save intermediate results out to memory. The more that a CPU can execute its instructions without accessing memory, the faster that execution will be.

Compared to almost any non-RISC ISA, ARMv6 has a lot of registers. All are 32 bits in size. There are 40 registers in all: 33 general-purpose registers plus 7 status registers. Not all of these registers are available at all times in all modes. Furthermore, some of the registers have special functions that place limits on how they may be used.

Untangling ARM register usage requires a chart indicating which registers are available in which modes. Refer to Figure 4-17 during the following discussion.

![[FIGURE 4-17:](#07_9781119183938-ch04.xhtml#rc04-fig-0017) The ARM11 register file](./media/images/9781119183938-fg0417.png)

Of the 16 ARM general-purpose registers, only the first 13 are truly general purpose. Registers R13, R14 and R15 play special roles in program execution. R15 acts as the program counter (PC), which always contains the address of the next instruction to be executed. Unlike some other processor architectures, the ARM program counter may be freely read and written to even in User mode. Simply writing a new address to R15 effectively implements an unconditional branch, but doing so is considered bad programming practice. Hard-coding addresses in software makes it impossible for operating systems to decide where in memory to load and run the code, and such code is very likely to malfunction.

R14 is called the _link register_ (LR). The LR is used to execute fast subroutine calls using one of a group of instructions called Branch with Link. When a `BL` or `BLX` instruction is executed, the CPU stores the return address in the LR and then branches to the subroutine address. When the subroutine finishes executing, the return address stored in LR is copied back to the program counter. The program then continues on its main line of execution, having “ducked out” to execute the subroutine.

R13 is by convention used as the stack pointer (SP). The ARM SP works the way SPs work in nearly all CPU architectures. (Refer to [Figure 4-6](#07_9781119183938-ch04.xhtml#c04-fig-0006) and the associated text earlier in this chapter if you don’t understand how stacks work.) Most ARM instructions allow you to use R13 as a general-purpose register, but ARM Holdings has deprecated this use, and for a very good reason: nearly all operating systems make intensive use of the stack, and without extreme care, using SP as a general-purpose register can cause crashes.

#### Banked Registers

[Figure 4-17](#07_9781119183938-ch04.xhtml#c04-fig-0017) suggests that there are a lot more ARM registers than there actually are. Read the figure carefully: each column represents a processor mode, and beneath the mode is a list of registers available while the CPU is operating in that mode. All modes can access registers R0 to R7, and it’s the _same_ R0 to R7 irrespective of mode. There is not a separate group of registers from R0 to R7 for each mode.

After that it becomes complicated. In Fast Interrupt mode, registers R8 to R14 are private and have their own mode-specific names: R8*fiq, R9_fiq, and so on. Machine instructions that specify one of the R8 to R14 registers while the CPU is in Fast Interrupt mode access registers in Fast Interrupt mode’s private bank. Registers R8_fiq to R14_fiq are \_banked registers*. There’s more information about Fast Interrupt mode later in this chapter.

In [Figure 4-17](#07_9781119183938-ch04.xhtml#c04-fig-0017), all shaded registers are banked registers. Fast Interrupt mode has a lot of them; the other modes have either two or, in the case of User and System modes, none at all.

> [!NOTE]
> that the description of processor modes and registers in [Figure 4-17](#07_9781119183938-ch04.xhtml#c04-fig-0017) applies only to ARMv6 and earlier ISAs.

#### The Current Program Status Registers

Most ARM registers are general-purpose, or almost general-purpose. One register is definitely not: the current program status register (CPSR) is a single 32-bit value divided into bits and groups of bits. Each bit or group stores some information about what the CPU is doing (or has recently done) at any particular instant.

Figure 4-18 shows what’s inside the CPSR. Explaining all of it in detail is beyond the scope of this book, and in any case much of it is mainly of use to compilers and assemblers who build executable programs. (Read more on this in [Chapter 5](#08_9781119183938-ch05.xhtml).) The shaded areas represent bits that are undefined and reserved for use in newer ARM microarchitectures.

![[FIGURE 4-18:](#07_9781119183938-ch04.xhtml#rc04-fig-0018) Inside the current program status register](./media/images/9781119183938-fg0418.png)

The part of the CPSR that sees the most use is the group of five bits called the _condition flags_. Each of the five bits in the group may be tested by conditional branch instructions. The `N`, `Z`, `C`, and `V` bits may also be tested by a conditional execution mechanism that can be used to “turn an instruction off” if one or more of the condition flags match the corresponding flags inside the instruction itself. (More on this in the section entitled “[Conditional Instruction Execution](#07_9781119183938-ch04.xhtml#c04-sec-0042)”.)

- **`N` (Negative) flag:** Set when the result of a calculation is considered negative. - **`Z` (Zero) flag:** Set when the result operand of an instruction is 0. Because of the way that comparisons are calculated, the `Z` flag is also set when two compared operands are equal. - **`C` (Carry) flag:** Set when an addition generates a carry or when a subtraction generates a borrow. C is also changed by shift instructions (which shuffle the bits in a 32-bit value left or right) to the value (1 or 0) of the last bit shifted out of the operand. - **`V` (Overflow) flag:** Set when a signed overflow occurs in the destination operand. - **`Q` (Saturation) flag:** Used with saturated integer arithmetic instructions to indicate that the result of a saturated addition or subtraction was corrected to place it within the range of the destination operand. Saturated arithmetic is frequently used by digital signal processing (DSP) algorithms and is outside the scope of this book.

With the exception of the `Q` flag, the ARM processor condition flags work very much as condition flags do in other architectures, including Intel’s.

The `T` and `J` bits select which of the three ARMv6 instruction sets is active. If the `T` bit is set, the CPU is in the Thumb state. If the `J` bit is set, the CPU is in the Jazelle state. If neither is set, the CPU is in the ARM state.

The CPU mode bits indicate which mode the CPU is currently using. The binary values for each mode are included in [Table 4-1](#07_9781119183938-ch04.xhtml#c04-tbl-0001).

Four bits are used as flags indicating a greater than or equal to (`GE`) result after the execution of certain SIMD instructions.

The `E` bit specifies the “endianness” of current CPU operations. When set to 1, it indicates little-endian operation. When cleared to 0, it indicates big-endian operations. The `E` bit must be set by two machine instructions specifically for that purpose, `SETEND LE` and `SETEND BE.`

The `A` bit allows system software to discriminate between a virtual memory page fault and an actual external memory error.

The `I` bit and `F` bit are interrupt masks. More on this in the next section.

#### Interrupts, Exceptions, Registers and the Vector Table

Understanding banked registers requires an understanding of the nature of interrupts and exceptions. These are events that require CPU attention, irrespective of what the CPU is doing when the event occurs. When a virtual memory page fault occurs, the CPU _must_ handle it to continue running. When the CPU encounters a machine instruction that it doesn’t understand, it must “switch gears” for a moment and figure out what to do next. When one of the computer’s peripherals has data ready or needs data, the CPU must service the request, often within a short time frame if correct operation is to be assured.

In every case, when an event happens, the CPU responds by running a special block of code known as a _handler_. Handlers are not part of user applications. They’re typically installed and configured by the operating system. There are several different classes of interrupt and exception, each with its own processor mode and banked registers. When an interrupt or exception occurs, the CPU immediately changes the processor mode, stores the current program counter in the new mode’s banked version of the link register and the CPSR in the new mode’s saved program status register (SPSR), and sets the program counter to one of a small number of addresses within the vector table; the mode and address chosen depends on the type of event that has occurred. The vector table is eight 32-bit words in length and resides either at the very bottom or nearly at the very top of the address map. Each entry is generally a single 32-bit unconditional branch instruction that directs the CPU to the appropriate handler elsewhere in memory (see Figure 4-19).

![[FIGURE 4-19:](#07_9781119183938-ch04.xhtml#rc04-fig-0019) The ARM exception vector table](./media/images/9781119183938-fg0419.png)

You can now see the value of the banked registers. Interrupts and exceptions can happen at any time, and the CPU must have room to store the bare minimum amount of state required to resume the user-mode program where it left off. It can’t rely on being able to store the program counter in the user-mode LR, as it would normally do with a branch with link instruction. What if the interrupted program has just made a function call and needs the value in LR to know where to return to? It can’t even rely on being able to push values to the user-mode stack. What if the stack is nearly full, or the program is using R13 as a general-purpose register? The sudden appearance of banked copies of LR (R14) and SP (R13) provides room to store the return address, and a pointer (generally pre-initialised by the operating system) to a stack that is guaranteed to be valid and have enough space.

The branch from the vector table takes execution into the appropriate handler, where the code does what it has to do to satisfy the exception. Typically the handler will first save some registers to the (known valid) stack to free up some registers with which to work. Once the handler is complete, it must explicitly restore these registers from the stack, and the CPSR from the copy in the SPSR, before returning to User mode and resuming execution at the address stored in the mode’s banked copy of LR.

### Fast Interrupts

There are two separate types of interrupt, which we’ll call regular (IRQ; from Interrupt Request) and fast (FIQ; from Fast Interrupt Request), corresponding to two physical signals entering the ARM11 from outside SoC, and two entries in the vector table. Fast interrupts have two useful properties that help to minimise interrupt servicing latency compared to regular interrupts.

The FIQ vector table entry is located at the end of the table. Although it’s perfectly permissible to insert a branch instruction to a handler in this table entry, as we must do for the IRQ entry and the various exceptions, it is more common to simply append the handler to the table, with the first instruction inside the table itself, so that the flow of control passes smoothly into the handler with no possibility of pipeline stalls.

While all other processor modes have only banked copies of SP (R13) and LR (R14), FIQ mode also has banked copies of R8 to R12. FIQ handlers therefore have five dedicated scratch registers that they can use without corrupting the registers of the interrupted program or incurring the time penalty of pushing registers to the stack.

Response to FIQ events is faster and more deterministic than the IRQ case because we minimise memory access. Indeed, if the exception handler code is present in cache (see [Chapter 3](#06_9781119183938-ch03.xhtml)), the exception begins and ends without accessing system memory at all. Under Linux on Raspberry Pi, we use FIQ to service high-frequency interrupts from the USB core, and IRQ to service all other system peripherals.

### Software Interrupts

One further type of event deserves mention at this point. Unlike all the other interrupts and exceptions, a software interrupt (SWI) doesn’t interrupt what the CPU is doing at some unplanned moment. Instead, it can be seen as a kind of subroutine call that’s used to enter supervisor mode in a carefully managed way, generally for the purpose of communicating with the operating system kernel. The SWI doesn’t include the address of the subroutine in the call; instead, software interrupts are numbered, and the number of the software interrupt is included as an operand to the software interrupt machine instruction, which would be written like this in ARM assembly language:

```
	SWI 0x21
```

When an SWI instruction is executed, the CPU executes the branch instruction stored at address 0x0000 0008 in the vector table (refer to [Figure 4-19](#07_9781119183938-ch04.xhtml#c04-fig-0019)). This branch takes execution to the SWI handler. The interrupt number included as the operand to the SWI instruction is generally used by the exception handler to select yet another branch, to the block of code that handles the specific software interrupt given in the operand. There may be dozens or more software interrupts, each with its own number and each with a subhandler corresponding to that number.

The value of SWIs is that they allow user programs to make managed calls into the operating system. As mentioned in [Chapter 8](#11_9781119183938-ch08.xhtml), the operating system kernel comprises code for accessing peripherals, providing a virtual machine abstraction to individual processes and guaranteeing security properties, including isolation between processes. The limitations on what applications can do when in User mode, particularly with respect to configuring the MMU (see [Chapter 3](#06_9781119183938-ch03.xhtml)), underpin the notion of process isolation. An SWI is the only way to switch from User to Supervisor mode; forcing this transition to happen via the vector table prevents applications from running arbitrary code in a privileged mode.

### Interrupt Priority

So what happens when a second interrupt or exception occurs while an earlier one is still being handled? Handlers are special in a number of ways but they’re still code, and they take time to run. Having an exception occur while an exception handler is running is not only possible but likely. Sorting this out is done in two general ways:

- Both kinds of interrupt (IRQ and FIQ) may be disabled independently while an exception handler is executing. This is done with two disable bits in the CPSR: F and I. Setting F to 1 disables fast interrupts. Setting I to 1 disables conventional interrupts. Interrupts may be disabled within all or part of an exception handler. - Each of the various classes of exception has a priority (see Table 4-2). Priorities work like this: a handler for an interrupt or exception of a given priority may be interrupted by one of higher priority, but not by one of lower priority. For example, the handler for the Reset exception is of Priority 0 and may not be interrupted by anything. An IRQ handler may be interrupted by an FIQ exception, but not vice versa.

<figure> <figcaption>

[Table 4-2](#07_9781119183938-ch04.xhtml#rc04-tbl-0002) ARM11 Interrupt Priorities

</figcaption>

**Exception** **Priority** ------------------------------ -------------- Reset 1 Data abort 2 Fast interrupt (FIQ) 3 Conventional interrupt (IRQ) 4 Prefetch abort 5 Software interrupt 6 Undefined instruction 6

</figure>

When an interrupt handler begins executing, all interrupts of the same priority are automatically disabled. Thus an IRQ handler cannot be interrupted by another IRQ exception unless the IRQ handler has the intelligence to sort out simultaneous interrupts and re-enables IRQ exceptions.

Interrupts may not be disabled by software running in User mode, because this would undermine the operating system’s ability to schedule other processes. Software interrupts may be issued by userland programs, but because software interrupts have the lowest priority, all other kinds of exceptions may occur during a software interrupt handler, unless interrupts are specifically disabled.

The software interrupt exception has the same priority as the undefined instruction exception because the two cannot occur together. All software interrupts are generated by the SWI instruction, which is present in all ARM processors and is thus always defined.

### Conditional Instruction Execution

In most instruction set architectures, conditional branch instructions are used to alter the flow of program execution. The ARM CPUs have conditional branch instructions, but they also offer something that in many cases is even better: conditional instruction execution. All 32-bit ARM instructions have a 4-bit field inside them expressing condition codes. The ARM architecture provides 15 condition codes, for conditions like equal, not equal, greater than, less than, overflow, and so on. (Four bits are capable of expressing 16 conditions codes, but one of the values is reserved and not used.) The condition code field is evaluated while the instruction is being decoded by the CPU.

The codes correspond to various combinations of the four condition flags maintained in the CPSR: `N`, `Z`, `C`, and `V`. If conditional execution is enabled for an instruction then that instruction executes _only_ if its condition code agrees with the current state of the condition flags.

> [!NOTE]
> that this is _not_ a bit-by-bit comparison of the condition codes to the four CPSR flags. Each four-bit binary value has an assigned meaning—for example:

- %0000 means that the instruction executes if the `Z` flag is set. - %0001 means that the instruction executes if the `Z` flag is cleared. - %1000 means the instruction executes if the `C` flag is set and the `Z` flag is cleared. - %1100 means that the instruction executes if the `Z` flag is cleared, and the `N` flag is equal to the V flag.

One of the codes, %1110, means that the flags will be ignored and that the instruction will always execute. (Recall that the “%” prefix means that the value shown is in binary notation.)

Condition codes are built into machine instructions by the assembler or compiler that creates an executable program. For assembly language, condition codes are specified with a two-character suffix appended to the mnemonic indicating the condition or conditions under which the instruction will execute. For example:

<figure>

------------------------------------------------------ --------------------------------------------------------------------- `MOV R0, #4` No suffix; always executes `MOVEQ R0, #4` Executes if `Z=1` (Equal) `MOVNE R0, #4` Executes if `Z=0` (Not Equal) `MOVMI R0, #4` Executes if `N=1` (Negative) ------------------------------------------------------ ---------------------------------------------------------------------

</figure>

All of these instructions copy the value 4 into R0. The first form lacks a suffix, and so it executes unconditionally; that is, always. The second form executes only if the `Z` flag in CPSR is set to 1, indicating that an earlier comparison (or other operation) generated a result of 0; for comparisons, a result of 0 indicates that the two values compared were equal. The third form executes only if an earlier operation cleared the `Z` flag to 0; for comparisons, this means that the values compared were not equal. The fourth form executes only if the `N` flag is set to 1, meaning that a comparison or other operation generated a negative value. There are 15 possible condition codes, including a code meaning “execute always”.

Why is conditional execution such a useful feature? Figure 4-20 shows two ways of doing the same thing in ARM assembly language. The algorithm is a simple IF/THEN construct: If R0 = R4, then execute the code in Block A; otherwise, execute the code in Block B. What the code in Block A and Block B actually does is not important for the example, and the instruction boxes in those blocks have been deliberately left blank.

![[FIGURE 4-20:](#07_9781119183938-ch04.xhtml#rc04-fig-0020) ARM conditional execution](./media/images/9781119183938-fg0420.png)

The first machine instruction is a comparison that checks to see if two registers (R0 and R4) are equal. The `CMP` (compare) instruction does that. If the two registers are found to be equal, `CMP` sets the `Z` flag to 1. If they are not equal, `CMP` sets the Z flag to 0.

The traditional way of coding this, in ARM or any other architecture, is on the right. After `CMP`, a conditional branch instruction tests the `Z` flag for inequality using the `NE` (Not Equal) suffix. If the two registers are not equal, execution branches to a location labelled BlockB. If the two registers are equal, the conditional branch lets execution continue into Block A. At the end of Block A, an unconditional branch takes execution to whatever code lies after the IF/THEN construct. Block B begins at the label BlockB, and continues to the end of the IF/THEN construct.

The sequence of instructions on the left does the very same thing. This time, however, all of the instructions are subject to conditional execution. The instructions in Block A have been set to execute only if the `Z` flag is 1 (condition code set to %0000). The instructions in Block B have been set to execute only if the `Z` flag is 0 (condition code set to %0001). The other flags are not involved in this example. In terms of which blocks execute, you can see that it’s either/or: if Block A is executed, Block B will not be, and vice versa. No branches are required.

Conditional execution makes two instructions unnecessary: the BNE conditional branch, and the B unconditional branch. That’s valuable all by itself. The real win, however, is that mispredicted branches can disrupt the instruction pipeline and slow down execution. Anything that can be done to avoid branches will speed up execution.

It’s important to remember that instructions are not “skipped” when their condition codes are not met. They still move through the pipeline and consume one clock cycle. However, they do no work and change nothing. The benefit of conditional execution derives from the avoidance of branches over small blocks of code, which can cost much more time than reading (but not executing) the block. There is a block size threshold (which varies between microarchitectures) above which the branch implementation of an IF/THEN construct is preferred. This threshold is not large, and in most microarchitectures it’s as little as three or four instructions.

## Coprocessors

There’s nothing new about coprocessors, and they are not specific to the ARM architecture. Understanding how they operate in an ARM11 context does require an understanding of CPU exceptions, so this is a good point to take them up.

A _coprocessor_ is a separate, specialised execution unit that usually has an instruction set of its own, distinct from that of the CPU. It generally has additional registers to support its machine instructions. Early in microprocessor history, coprocessors were separate chips, connected to the CPU through an external bus. One of the earliest and best-known coprocessors was Intel’s 1980-era 8087, which lived in a separate 40-pin Dual Inline Package (DIP) socket and could be installed by a careful end user. The 8087 provided floating point maths instructions to the integer-only 8086 and 8088. It implemented 60 new instructions and several numeric concepts previously unavailable in microcomputers, like denormals to express underflow values, and the not-a-number (NaN) value to hold the results of undefined operations like divide-by-zero or values outside the domain of real numbers, like imaginary numbers.
